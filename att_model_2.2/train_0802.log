[1,3]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]
[1,2]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]
[1,4]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]
[1,1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]
[1,0]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]
[1,3]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
[1,3]<stderr>:Instructions for updating:
[1,3]<stderr>:Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
[1,2]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
[1,2]<stderr>:Instructions for updating:
[1,2]<stderr>:Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
[1,4]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
[1,4]<stderr>:Instructions for updating:
[1,4]<stderr>:Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
[1,1]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
[1,1]<stderr>:Instructions for updating:
[1,1]<stderr>:Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
[1,0]<stderr>:Instructions for updating:
[1,0]<stderr>:Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
[1,3]<stderr>:2024-08-02 10:29:52.238532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37988 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:50:00.0, compute capability: 8.0
[1,1]<stderr>:2024-08-02 10:29:52.242982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37988 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0
[1,2]<stderr>:2024-08-02 10:29:52.250072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37988 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4a:00.0, compute capability: 8.0
[1,0]<stderr>:2024-08-02 10:29:52.255632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37988 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
[1,3]<stdout>:============ load project: cht ================
[1,1]<stdout>:============ load project: cht ================
[1,2]<stdout>:============ load project: cht ================
[1,0]<stdout>:============ load project: cht ================
[1,4]<stderr>:2024-08-02 10:29:52.446251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36650 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:93:00.0, compute capability: 8.0
[1,4]<stdout>:============ load project: cht ================
[1,0]<stdout>:add 1500 chinese traditional chars
[1,0]<stdout>:----------------------------------------
[1,2]<stdout>:add 1500 chinese traditional chars
[1,2]<stdout>:----------------------------------------
[1,1]<stdout>:add 1500 chinese traditional chars
[1,1]<stdout>:----------------------------------------
[1,3]<stdout>:add 1500 chinese traditional chars
[1,3]<stdout>:----------------------------------------
[1,4]<stdout>:add 1500 chinese traditional chars
[1,4]<stdout>:----------------------------------------
[1,1]<stdout>:/mnt/server_data2/data/seq_chemical/tfrecords_20240712_latex/train*
[1,1]<stdout>:<TensorSliceDataset shapes: (), types: tf.string>
[1,1]<stdout>:================== total count: 445 ==============
[1,4]<stdout>:/mnt/server_data2/data/seq_chemical/tfrecords_20240712_latex/train*
[1,4]<stdout>:<TensorSliceDataset shapes: (), types: tf.string>
[1,4]<stdout>:================== total count: 445 ==============
[1,0]<stdout>:/mnt/server_data2/data/seq_chemical/tfrecords_20240712_latex/train*
[1,2]<stdout>:/mnt/server_data2/data/seq_chemical/tfrecords_20240712_latex/train*
[1,3]<stdout>:/mnt/server_data2/data/seq_chemical/tfrecords_20240712_latex/train*
[1,0]<stdout>:<TensorSliceDataset shapes: (), types: tf.string>
[1,0]<stdout>:================== total count: 445 ==============
[1,3]<stdout>:<TensorSliceDataset shapes: (), types: tf.string>
[1,3]<stdout>:================== total count: 445 ==============
[1,2]<stdout>:<TensorSliceDataset shapes: (), types: tf.string>
[1,2]<stdout>:================== total count: 445 ==============
[1,1]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,1]<stderr>:Instructions for updating:
[1,1]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,1]<stderr>:    options available in V2.
[1,1]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,1]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,1]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,1]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,1]<stderr>:    being differentiable using a gradient tape.
[1,1]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,1]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,1]<stderr>:    stateful argument making all functions stateful.
[1,1]<stderr>:    
[1,1]<stderr>:W0802 10:30:00.755384 140596578711360 deprecation.py:341] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,1]<stderr>:Instructions for updating:
[1,1]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,1]<stderr>:    options available in V2.
[1,1]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,1]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,1]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,1]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,1]<stderr>:    being differentiable using a gradient tape.
[1,1]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,1]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,1]<stderr>:    stateful argument making all functions stateful.
[1,1]<stderr>:    
[1,1]<stdout>:loading from existing checkpoint /mnt/server_data2/data/seq_chemical/models_equ_latex0802/ckpt-1
[1,4]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,4]<stderr>:Instructions for updating:
[1,4]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,4]<stderr>:    options available in V2.
[1,4]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,4]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,4]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,4]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,4]<stderr>:    being differentiable using a gradient tape.
[1,4]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,4]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,4]<stderr>:    stateful argument making all functions stateful.
[1,4]<stderr>:    
[1,4]<stderr>:W0802 10:30:00.852149 140593389885248 deprecation.py:341] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,4]<stderr>:Instructions for updating:
[1,4]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,4]<stderr>:    options available in V2.
[1,4]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,4]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,4]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,4]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,4]<stderr>:    being differentiable using a gradient tape.
[1,4]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,4]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,4]<stderr>:    stateful argument making all functions stateful.
[1,4]<stderr>:    
[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,0]<stderr>:Instructions for updating:
[1,0]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,0]<stderr>:    options available in V2.
[1,0]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,0]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,0]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,0]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,0]<stderr>:    being differentiable using a gradient tape.
[1,0]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,0]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,0]<stderr>:    stateful argument making all functions stateful.
[1,0]<stderr>:    
[1,0]<stderr>:W0802 10:30:00.879223 140110411638592 deprecation.py:341] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,0]<stderr>:Instructions for updating:
[1,0]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,0]<stderr>:    options available in V2.
[1,0]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,0]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,0]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,0]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,0]<stderr>:    being differentiable using a gradient tape.
[1,0]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,0]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,0]<stderr>:    stateful argument making all functions stateful.
[1,0]<stderr>:    
[1,3]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,3]<stderr>:Instructions for updating:
[1,3]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,3]<stderr>:    options available in V2.
[1,3]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,3]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,3]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,3]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,3]<stderr>:    being differentiable using a gradient tape.
[1,3]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,3]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,3]<stderr>:    stateful argument making all functions stateful.
[1,3]<stderr>:    
[1,3]<stderr>:W0802 10:30:00.896932 140025579792192 deprecation.py:341] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,3]<stderr>:Instructions for updating:
[1,3]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,3]<stderr>:    options available in V2.
[1,3]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,3]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,3]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,3]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,3]<stderr>:    being differentiable using a gradient tape.
[1,3]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,3]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,3]<stderr>:    stateful argument making all functions stateful.
[1,3]<stderr>:    
[1,2]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,2]<stderr>:Instructions for updating:
[1,2]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,2]<stderr>:    options available in V2.
[1,2]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,2]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,2]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,2]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,2]<stderr>:    being differentiable using a gradient tape.
[1,2]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,2]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,2]<stderr>:    stateful argument making all functions stateful.
[1,2]<stderr>:    
[1,2]<stderr>:W0802 10:30:00.897676 139799121237824 deprecation.py:341] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:465: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,2]<stderr>:Instructions for updating:
[1,2]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,2]<stderr>:    options available in V2.
[1,2]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,2]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,2]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,2]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,2]<stderr>:    being differentiable using a gradient tape.
[1,2]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,2]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,2]<stderr>:    stateful argument making all functions stateful.
[1,2]<stderr>:    
[1,4]<stdout>:loading from existing checkpoint /mnt/server_data2/data/seq_chemical/models_equ_latex0802/ckpt-1
[1,0]<stdout>:loading from existing checkpoint /mnt/server_data2/data/seq_chemical/models_equ_latex0802/ckpt-1
[1,3]<stdout>:loading from existing checkpoint /mnt/server_data2/data/seq_chemical/models_equ_latex0802/ckpt-1
[1,2]<stdout>:loading from existing checkpoint /mnt/server_data2/data/seq_chemical/models_equ_latex0802/ckpt-1
[1,1]<stdout>:steps:  0
[1,1]<stdout>:loading from existing checkpoint done
[1,4]<stdout>:steps:  0
[1,4]<stdout>:loading from existing checkpoint done
[1,0]<stdout>:steps:  0
[1,0]<stdout>:loading from existing checkpoint done
[1,3]<stdout>:steps:  0
[1,3]<stdout>:loading from existing checkpoint done
[1,2]<stdout>:steps:  0
[1,2]<stdout>:loading from existing checkpoint done
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:14.279958 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:14.304974 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:14.308553 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:14.323650 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:14.701614 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:14.706721 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:14.711358 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:14.717737 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:14.731160 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:14.735557 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:14.739201 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:14.744054 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:14.801383 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:14.806241 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:14.813015 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:14.820964 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:14.897310 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:14.900829 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:14.904253 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:14.909622 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:30.343973 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:30.348733 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:30.372956 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0802 10:30:30.404715 140110411638592 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:30.575504 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:30.579976 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:30.627628 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0802 10:30:30.633248 139799121237824 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:30.693270 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:30.708513 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:30.714911 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,4]<stderr>:W0802 10:30:30.720593 140593389885248 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:30.725243 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:30.731289 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:30.736649 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0802 10:30:30.743523 140025579792192 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:30.881526 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:30.889517 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:30.897321 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0802 10:30:30.904353 140596578711360 image_ops_impl.py:2434] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:2024-08-02 10:30:44.596562: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302
[1,3]<stderr>:2024-08-02 10:30:44.624689: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302
[1,0]<stderr>:2024-08-02 10:30:44.653995: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302
[1,2]<stderr>:2024-08-02 10:30:44.679937: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302
[1,4]<stderr>:2024-08-02 10:30:46.913927: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302
[1,0]<stdout>:model_loss: 0.88108677 l2_loss: 0.0137035195 total_loss: 0.8947903  time: 46.442134857177734 step: 1 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.31289718 l2_loss: 0.013707632 total_loss: 0.3266048  time: 364.2089812755585 step: 101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.23011845 l2_loss: 0.013708496 total_loss: 0.24382696  time: 352.76343154907227 step: 201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.26391426 l2_loss: 0.013708856 total_loss: 0.27762312  time: 359.30215978622437 step: 301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.2214017 l2_loss: 0.013709188 total_loss: 0.2351109  time: 361.0160026550293 step: 401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.29703066 l2_loss: 0.01370921 total_loss: 0.31073987  time: 358.3695616722107 step: 501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.15735637 l2_loss: 0.013708988 total_loss: 0.17106536  time: 361.23070669174194 step: 601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.21748772 l2_loss: 0.0137086 total_loss: 0.23119633  time: 363.00014090538025 step: 701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.18585835 l2_loss: 0.013708081 total_loss: 0.19956644  time: 357.68022656440735 step: 801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.19653712 l2_loss: 0.0137075195 total_loss: 0.21024464  time: 361.4688436985016 step: 901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12763262 l2_loss: 0.013706946 total_loss: 0.14133957  time: 358.7385492324829 step: 1001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09428884 l2_loss: 0.013706253 total_loss: 0.10799509  time: 359.24507546424866 step: 1101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09593809 l2_loss: 0.01370552 total_loss: 0.10964361  time: 351.54810786247253 step: 1201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05430581 l2_loss: 0.013704721 total_loss: 0.06801053  time: 360.05011892318726 step: 1301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07129308 l2_loss: 0.013703808 total_loss: 0.08499689  time: 360.64820313453674 step: 1401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09675808 l2_loss: 0.013702878 total_loss: 0.11046096  time: 361.53480052948 step: 1501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10649488 l2_loss: 0.013701937 total_loss: 0.12019682  time: 361.2132935523987 step: 1601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0714086 l2_loss: 0.013701 total_loss: 0.0851096  time: 360.80318450927734 step: 1701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09581382 l2_loss: 0.013700041 total_loss: 0.10951386  time: 361.84995317459106 step: 1801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1036029 l2_loss: 0.013699241 total_loss: 0.11730214  time: 361.6390759944916 step: 1901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11123391 l2_loss: 0.013698426 total_loss: 0.12493234  time: 361.4666893482208 step: 2001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.08360221 l2_loss: 0.013697531 total_loss: 0.09729974  time: 382.22450017929077 step: 2101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10431704 l2_loss: 0.013696588 total_loss: 0.11801363  time: 356.3018298149109 step: 2201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14718056 l2_loss: 0.013695828 total_loss: 0.16087638  time: 360.8685564994812 step: 2301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.080454625 l2_loss: 0.013695067 total_loss: 0.094149694  time: 360.83880496025085 step: 2401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.064871326 l2_loss: 0.013694324 total_loss: 0.07856565  time: 357.1141142845154 step: 2501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0631692 l2_loss: 0.013693483 total_loss: 0.076862685  time: 364.7199466228485 step: 2601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06443968 l2_loss: 0.01369256 total_loss: 0.078132235  time: 365.2130968570709 step: 2701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.075816095 l2_loss: 0.013691496 total_loss: 0.089507595  time: 363.4458680152893 step: 2801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.091142 l2_loss: 0.013690509 total_loss: 0.10483251  time: 367.79829597473145 step: 2901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07804012 l2_loss: 0.013689539 total_loss: 0.09172966  time: 359.32957315444946 step: 3001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.060686357 l2_loss: 0.013688626 total_loss: 0.07437498  time: 359.7599456310272 step: 3101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14839394 l2_loss: 0.013687811 total_loss: 0.16208175  time: 364.6385455131531 step: 3201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13741033 l2_loss: 0.013686993 total_loss: 0.15109733  time: 357.47249245643616 step: 3301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1222345 l2_loss: 0.013686133 total_loss: 0.13592063  time: 360.31955790519714 step: 3401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.18538834 l2_loss: 0.013685243 total_loss: 0.19907358  time: 363.2568666934967 step: 3501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.069981866 l2_loss: 0.013684321 total_loss: 0.08366619  time: 359.4336128234863 step: 3601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08728468 l2_loss: 0.013683424 total_loss: 0.1009681  time: 353.4110140800476 step: 3701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12726744 l2_loss: 0.013682586 total_loss: 0.14095002  time: 356.9454753398895 step: 3801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10136626 l2_loss: 0.013681604 total_loss: 0.115047865  time: 361.29251074790955 step: 3901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1732099 l2_loss: 0.013680679 total_loss: 0.18689059  time: 356.5737724304199 step: 4001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.061238144 l2_loss: 0.01367967 total_loss: 0.074917816  time: 360.8088159561157 step: 4101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06574567 l2_loss: 0.013678628 total_loss: 0.07942429  time: 358.89699482917786 step: 4201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09572703 l2_loss: 0.013677622 total_loss: 0.109404646  time: 359.2064733505249 step: 4301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09725059 l2_loss: 0.013676967 total_loss: 0.11092755  time: 360.3506050109863 step: 4401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10028709 l2_loss: 0.013676023 total_loss: 0.11396311  time: 360.8224868774414 step: 4501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1545512 l2_loss: 0.013675061 total_loss: 0.16822626  time: 358.3172347545624 step: 4601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14835456 l2_loss: 0.013674113 total_loss: 0.16202867  time: 354.43532514572144 step: 4701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.079557374 l2_loss: 0.013673225 total_loss: 0.0932306  time: 358.35082507133484 step: 4801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.050387572 l2_loss: 0.013672262 total_loss: 0.06405984  time: 359.7392840385437 step: 4901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14624174 l2_loss: 0.013671302 total_loss: 0.15991305  time: 360.38551139831543 step: 5001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11972618 l2_loss: 0.013670365 total_loss: 0.13339655  time: 353.95451831817627 step: 5101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07656837 l2_loss: 0.013669387 total_loss: 0.09023776  time: 365.84022188186646 step: 5201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07778332 l2_loss: 0.013668347 total_loss: 0.09145166  time: 358.7288603782654 step: 5301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06490328 l2_loss: 0.0136672435 total_loss: 0.07857052  time: 357.5296883583069 step: 5401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.064114116 l2_loss: 0.013666237 total_loss: 0.07778035  time: 359.6391553878784 step: 5501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.064456776 l2_loss: 0.013665145 total_loss: 0.07812192  time: 356.23962688446045 step: 5601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05314139 l2_loss: 0.013663909 total_loss: 0.066805296  time: 359.7464077472687 step: 5701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.16171724 l2_loss: 0.013662594 total_loss: 0.17537983  time: 362.7954533100128 step: 5801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.24257143 l2_loss: 0.013661321 total_loss: 0.25623274  time: 359.7506184577942 step: 5901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.21168172 l2_loss: 0.013660183 total_loss: 0.2253419  time: 363.3779423236847 step: 6001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.2151747 l2_loss: 0.013659072 total_loss: 0.22883378  time: 376.7032890319824 step: 6101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.29028362 l2_loss: 0.013657998 total_loss: 0.3039416  time: 359.5982460975647 step: 6201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.22989565 l2_loss: 0.013657066 total_loss: 0.24355271  time: 359.40114665031433 step: 6301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.22873022 l2_loss: 0.013656039 total_loss: 0.24238625  time: 359.81890320777893 step: 6401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.18887855 l2_loss: 0.013655123 total_loss: 0.20253368  time: 359.5175287723541 step: 6501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07488533 l2_loss: 0.013654255 total_loss: 0.088539585  time: 358.0507836341858 step: 6601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07697061 l2_loss: 0.013653512 total_loss: 0.09062412  time: 363.91559171676636 step: 6701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10930059 l2_loss: 0.013652733 total_loss: 0.122953326  time: 359.7349374294281 step: 6801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06042419 l2_loss: 0.013651948 total_loss: 0.07407614  time: 362.0286457538605 step: 6901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10070615 l2_loss: 0.013650943 total_loss: 0.1143571  time: 357.33644795417786 step: 7001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08159433 l2_loss: 0.013649946 total_loss: 0.09524428  time: 356.8041498661041 step: 7101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.122795895 l2_loss: 0.013648907 total_loss: 0.1364448  time: 359.412237405777 step: 7201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05719282 l2_loss: 0.013647845 total_loss: 0.070840664  time: 357.43026304244995 step: 7301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.22150058 l2_loss: 0.013647018 total_loss: 0.2351476  time: 359.3244092464447 step: 7401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.20233154 l2_loss: 0.013646048 total_loss: 0.2159776  time: 365.1427516937256 step: 7501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.31168222 l2_loss: 0.013645143 total_loss: 0.32532737  time: 360.60588240623474 step: 7601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.38721737 l2_loss: 0.013644265 total_loss: 0.40086165  time: 367.2891526222229 step: 7701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.28611794 l2_loss: 0.013643524 total_loss: 0.29976147  time: 361.2189292907715 step: 7801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.23558077 l2_loss: 0.013642651 total_loss: 0.24922343  time: 361.34861278533936 step: 7901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1810124 l2_loss: 0.013641769 total_loss: 0.19465418  time: 360.532838344574 step: 8001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.15368672 l2_loss: 0.013640907 total_loss: 0.16732763  time: 363.4135711193085 step: 8101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07648471 l2_loss: 0.013639941 total_loss: 0.09012465  time: 368.74368834495544 step: 8201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.051912688 l2_loss: 0.013638879 total_loss: 0.065551564  time: 355.78546500205994 step: 8301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06530354 l2_loss: 0.0136378305 total_loss: 0.078941375  time: 358.8332550525665 step: 8401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08498577 l2_loss: 0.013636627 total_loss: 0.0986224  time: 356.55519676208496 step: 8501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04664782 l2_loss: 0.0136354 total_loss: 0.06028322  time: 362.29756021499634 step: 8601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.027081015 l2_loss: 0.013634106 total_loss: 0.04071512  time: 363.02297139167786 step: 8701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08219758 l2_loss: 0.013632791 total_loss: 0.095830366  time: 360.9058394432068 step: 8801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06515081 l2_loss: 0.013631568 total_loss: 0.07878238  time: 358.2138924598694 step: 8901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.059861984 l2_loss: 0.013630305 total_loss: 0.07349229  time: 359.50632309913635 step: 9001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07401014 l2_loss: 0.013629154 total_loss: 0.087639295  time: 359.56702733039856 step: 9101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04041691 l2_loss: 0.013628091 total_loss: 0.054045003  time: 362.8596444129944 step: 9201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.076778844 l2_loss: 0.013627031 total_loss: 0.090405874  time: 360.7037875652313 step: 9301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.066188365 l2_loss: 0.013625939 total_loss: 0.07981431  time: 360.2808167934418 step: 9401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.078521036 l2_loss: 0.013624942 total_loss: 0.09214598  time: 359.7353444099426 step: 9501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05646434 l2_loss: 0.013623919 total_loss: 0.07008826  time: 362.382203578949 step: 9601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045712147 l2_loss: 0.013623085 total_loss: 0.05933523  time: 359.7052426338196 step: 9701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.043915536 l2_loss: 0.013622048 total_loss: 0.057537585  time: 360.50798201560974 step: 9801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05244577 l2_loss: 0.013620934 total_loss: 0.066066705  time: 359.4021260738373 step: 9901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03216242 l2_loss: 0.0136197945 total_loss: 0.045782216  time: 361.31102180480957 step: 10001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.07347307 l2_loss: 0.013618566 total_loss: 0.08709164  time: 398.25058102607727 step: 10101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12939171 l2_loss: 0.013617469 total_loss: 0.14300919  time: 362.6146810054779 step: 10201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12479213 l2_loss: 0.013616488 total_loss: 0.13840862  time: 358.7730095386505 step: 10301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11041725 l2_loss: 0.013615418 total_loss: 0.12403266  time: 357.3706977367401 step: 10401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.098528795 l2_loss: 0.013614247 total_loss: 0.11214304  time: 358.44418597221375 step: 10501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.093870156 l2_loss: 0.013613152 total_loss: 0.107483305  time: 358.54907059669495 step: 10601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05248888 l2_loss: 0.013612102 total_loss: 0.06610098  time: 357.2561709880829 step: 10701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06676217 l2_loss: 0.013610965 total_loss: 0.08037314  time: 353.4913387298584 step: 10801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09265793 l2_loss: 0.0136098005 total_loss: 0.106267735  time: 362.49277210235596 step: 10901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13076137 l2_loss: 0.013608742 total_loss: 0.14437011  time: 359.6854181289673 step: 11001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09940999 l2_loss: 0.013607697 total_loss: 0.113017686  time: 356.7146759033203 step: 11101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.093708836 l2_loss: 0.013606817 total_loss: 0.10731565  time: 361.01643204689026 step: 11201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14413384 l2_loss: 0.013606017 total_loss: 0.15773985  time: 361.9994649887085 step: 11301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08433081 l2_loss: 0.013605283 total_loss: 0.097936094  time: 360.9249584674835 step: 11401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13940151 l2_loss: 0.013604559 total_loss: 0.15300608  time: 356.90200448036194 step: 11501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08161014 l2_loss: 0.013603734 total_loss: 0.095213875  time: 358.21978640556335 step: 11601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.087046534 l2_loss: 0.013602941 total_loss: 0.100649476  time: 357.42342352867126 step: 11701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04564182 l2_loss: 0.013602152 total_loss: 0.059243973  time: 358.09297490119934 step: 11801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.057562344 l2_loss: 0.013601265 total_loss: 0.07116361  time: 359.8808329105377 step: 11901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05412279 l2_loss: 0.013600486 total_loss: 0.067723274  time: 359.56330490112305 step: 12001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.023297688 l2_loss: 0.013599671 total_loss: 0.03689736  time: 365.76967573165894 step: 12101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.060490463 l2_loss: 0.013598817 total_loss: 0.07408928  time: 369.3870041370392 step: 12201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045352463 l2_loss: 0.013597824 total_loss: 0.058950286  time: 362.07904171943665 step: 12301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08433928 l2_loss: 0.013596867 total_loss: 0.09793615  time: 358.3443012237549 step: 12401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.087393194 l2_loss: 0.013595819 total_loss: 0.100989014  time: 362.37515592575073 step: 12501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.095413335 l2_loss: 0.0135948025 total_loss: 0.10900813  time: 357.92936301231384 step: 12601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08842517 l2_loss: 0.013593729 total_loss: 0.10201889  time: 356.71265029907227 step: 12701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08376569 l2_loss: 0.013592628 total_loss: 0.09735832  time: 357.91753673553467 step: 12801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.069753975 l2_loss: 0.013591504 total_loss: 0.08334548  time: 362.0526533126831 step: 12901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.093736745 l2_loss: 0.01359033 total_loss: 0.107327074  time: 355.6788258552551 step: 13001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07075718 l2_loss: 0.013589169 total_loss: 0.08434635  time: 359.6540551185608 step: 13101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.072195545 l2_loss: 0.0135880485 total_loss: 0.08578359  time: 360.2792353630066 step: 13201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08216309 l2_loss: 0.0135869775 total_loss: 0.09575006  time: 359.9095940589905 step: 13301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061654232 l2_loss: 0.013585934 total_loss: 0.075240165  time: 365.6600263118744 step: 13401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12425194 l2_loss: 0.013584933 total_loss: 0.13783687  time: 360.51614212989807 step: 13501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055965994 l2_loss: 0.013583982 total_loss: 0.06954998  time: 357.4825813770294 step: 13601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.034143128 l2_loss: 0.013582998 total_loss: 0.047726125  time: 359.6498005390167 step: 13701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11443646 l2_loss: 0.013581888 total_loss: 0.12801835  time: 355.6646981239319 step: 13801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08305832 l2_loss: 0.013580691 total_loss: 0.09663901  time: 361.82210779190063 step: 13901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.057685442 l2_loss: 0.013579596 total_loss: 0.07126504  time: 363.0724883079529 step: 14001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.05948432 l2_loss: 0.013578475 total_loss: 0.0730628  time: 361.2458291053772 step: 14101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.035901114 l2_loss: 0.013577292 total_loss: 0.049478404  time: 385.17328810691833 step: 14201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0935204 l2_loss: 0.013576073 total_loss: 0.10709648  time: 361.9110977649689 step: 14301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.122665174 l2_loss: 0.013574923 total_loss: 0.1362401  time: 360.10081362724304 step: 14401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10541216 l2_loss: 0.013573807 total_loss: 0.118985966  time: 354.7874085903168 step: 14501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09433126 l2_loss: 0.013572621 total_loss: 0.107903875  time: 358.8791573047638 step: 14601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14972055 l2_loss: 0.013571432 total_loss: 0.16329198  time: 362.1372537612915 step: 14701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13419786 l2_loss: 0.013570309 total_loss: 0.14776817  time: 360.75819158554077 step: 14801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.084714964 l2_loss: 0.013569216 total_loss: 0.09828418  time: 355.686425447464 step: 14901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.095963344 l2_loss: 0.013568186 total_loss: 0.10953153  time: 359.74891209602356 step: 15001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13782224 l2_loss: 0.013567014 total_loss: 0.15138926  time: 360.1715157032013 step: 15101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.108731516 l2_loss: 0.013566084 total_loss: 0.1222976  time: 356.8376359939575 step: 15201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11055263 l2_loss: 0.0135651985 total_loss: 0.12411783  time: 359.27958035469055 step: 15301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1076353 l2_loss: 0.013564236 total_loss: 0.12119953  time: 352.60369753837585 step: 15401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.059572678 l2_loss: 0.013563199 total_loss: 0.073135875  time: 357.04121589660645 step: 15501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07126115 l2_loss: 0.013562077 total_loss: 0.08482323  time: 360.0553255081177 step: 15601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09056348 l2_loss: 0.013560945 total_loss: 0.10412443  time: 362.03795289993286 step: 15701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061395418 l2_loss: 0.013559832 total_loss: 0.07495525  time: 361.5761094093323 step: 15801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.058569405 l2_loss: 0.013558565 total_loss: 0.07212797  time: 355.84144616127014 step: 15901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07911422 l2_loss: 0.01355724 total_loss: 0.09267146  time: 366.0184488296509 step: 16001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.062813245 l2_loss: 0.013556017 total_loss: 0.07636926  time: 358.66294622421265 step: 16101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10319239 l2_loss: 0.013554782 total_loss: 0.11674717  time: 381.3281764984131 step: 16201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045308977 l2_loss: 0.013553536 total_loss: 0.058862515  time: 355.96135926246643 step: 16301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.050009217 l2_loss: 0.013552334 total_loss: 0.06356155  time: 357.87439727783203 step: 16401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03492589 l2_loss: 0.013551036 total_loss: 0.048476927  time: 356.7797565460205 step: 16501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05312357 l2_loss: 0.013549754 total_loss: 0.06667332  time: 360.8886227607727 step: 16601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.041403633 l2_loss: 0.0135485 total_loss: 0.054952133  time: 356.4017515182495 step: 16701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.047152597 l2_loss: 0.013547121 total_loss: 0.060699716  time: 359.3587017059326 step: 16801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.056463655 l2_loss: 0.013545747 total_loss: 0.0700094  time: 354.58186626434326 step: 16901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08818176 l2_loss: 0.013544415 total_loss: 0.101726174  time: 357.3104944229126 step: 17001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06255317 l2_loss: 0.013543146 total_loss: 0.07609631  time: 356.6621654033661 step: 17101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07952561 l2_loss: 0.013541758 total_loss: 0.09306737  time: 362.2561628818512 step: 17201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08324952 l2_loss: 0.013540489 total_loss: 0.09679001  time: 362.3011484146118 step: 17301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11010721 l2_loss: 0.013539221 total_loss: 0.12364644  time: 359.6046528816223 step: 17401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10123454 l2_loss: 0.013538129 total_loss: 0.11477267  time: 358.1313199996948 step: 17501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1006008 l2_loss: 0.013537048 total_loss: 0.11413785  time: 359.019464969635 step: 17601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14588197 l2_loss: 0.013536021 total_loss: 0.15941799  time: 356.15669989585876 step: 17701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11629279 l2_loss: 0.013535015 total_loss: 0.1298278  time: 360.36699891090393 step: 17801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09968599 l2_loss: 0.01353401 total_loss: 0.11322  time: 361.6642565727234 step: 17901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09137785 l2_loss: 0.013532977 total_loss: 0.10491082  time: 359.34865522384644 step: 18001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.08711512 l2_loss: 0.013531939 total_loss: 0.100647055  time: 359.3346543312073 step: 18101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.097072005 l2_loss: 0.013530799 total_loss: 0.1106028  time: 367.5658552646637 step: 18201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08536327 l2_loss: 0.013529716 total_loss: 0.09889299  time: 359.7272102832794 step: 18301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05299539 l2_loss: 0.013528619 total_loss: 0.066524014  time: 357.34575152397156 step: 18401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08051356 l2_loss: 0.013527594 total_loss: 0.094041154  time: 360.1207363605499 step: 18501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10786279 l2_loss: 0.0135264825 total_loss: 0.12138928  time: 358.78171825408936 step: 18601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09790348 l2_loss: 0.013525372 total_loss: 0.11142886  time: 356.43971514701843 step: 18701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05527273 l2_loss: 0.013524395 total_loss: 0.06879713  time: 358.18673300743103 step: 18801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.074621834 l2_loss: 0.013523283 total_loss: 0.088145114  time: 359.8864412307739 step: 18901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.065928504 l2_loss: 0.013522317 total_loss: 0.07945082  time: 363.27915716171265 step: 19001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04574821 l2_loss: 0.01352135 total_loss: 0.059269562  time: 362.79033493995667 step: 19101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13814326 l2_loss: 0.01352038 total_loss: 0.15166363  time: 354.70688581466675 step: 19201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.15328667 l2_loss: 0.013519473 total_loss: 0.16680613  time: 359.6351284980774 step: 19301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13910253 l2_loss: 0.0135191325 total_loss: 0.15262167  time: 359.7540943622589 step: 19401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.15588199 l2_loss: 0.01351841 total_loss: 0.1694004  time: 355.26770877838135 step: 19501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.17022558 l2_loss: 0.013517727 total_loss: 0.1837433  time: 356.9441428184509 step: 19601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.17116979 l2_loss: 0.013516962 total_loss: 0.18468675  time: 357.77083468437195 step: 19701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12480967 l2_loss: 0.013516095 total_loss: 0.13832577  time: 360.23821353912354 step: 19801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07441674 l2_loss: 0.013515225 total_loss: 0.08793197  time: 362.6383738517761 step: 19901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06167838 l2_loss: 0.013514273 total_loss: 0.07519265  time: 359.13352823257446 step: 20001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.041089155 l2_loss: 0.013513205 total_loss: 0.05460236  time: 360.22834277153015 step: 20101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07644639 l2_loss: 0.013512105 total_loss: 0.0899585  time: 380.705757856369 step: 20201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0426018 l2_loss: 0.013510989 total_loss: 0.05611279  time: 357.97807693481445 step: 20301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.081236094 l2_loss: 0.013509813 total_loss: 0.094745904  time: 357.7937912940979 step: 20401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.048721023 l2_loss: 0.013508566 total_loss: 0.06222959  time: 359.6025187969208 step: 20501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.062302627 l2_loss: 0.0135073755 total_loss: 0.07581  time: 357.6016459465027 step: 20601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.041389912 l2_loss: 0.013506244 total_loss: 0.054896157  time: 356.92528915405273 step: 20701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.024147902 l2_loss: 0.01350511 total_loss: 0.03765301  time: 362.8652458190918 step: 20801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04552061 l2_loss: 0.0135039855 total_loss: 0.059024595  time: 358.8035705089569 step: 20901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.038039904 l2_loss: 0.013502893 total_loss: 0.051542796  time: 360.99146604537964 step: 21001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.057146203 l2_loss: 0.013501739 total_loss: 0.07064794  time: 355.19400668144226 step: 21101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0475568 l2_loss: 0.013500711 total_loss: 0.061057508  time: 362.48122572898865 step: 21201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.023707248 l2_loss: 0.013499753 total_loss: 0.037207  time: 358.89310812950134 step: 21301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03626338 l2_loss: 0.013498703 total_loss: 0.049762085  time: 356.15750217437744 step: 21401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.048564047 l2_loss: 0.013497749 total_loss: 0.062061794  time: 357.98881697654724 step: 21501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.041385345 l2_loss: 0.013496789 total_loss: 0.054882135  time: 357.717493057251 step: 21601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.065616764 l2_loss: 0.01349595 total_loss: 0.079112716  time: 359.61658215522766 step: 21701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05149005 l2_loss: 0.013495095 total_loss: 0.06498514  time: 357.97405099868774 step: 21801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06753572 l2_loss: 0.013494215 total_loss: 0.08102994  time: 357.45048904418945 step: 21901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06877027 l2_loss: 0.013493256 total_loss: 0.08226352  time: 356.5039086341858 step: 22001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.08402683 l2_loss: 0.013492404 total_loss: 0.097519234  time: 359.123907327652 step: 22101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.054326225 l2_loss: 0.013491479 total_loss: 0.0678177  time: 383.8994777202606 step: 22201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.116107814 l2_loss: 0.013490481 total_loss: 0.12959829  time: 355.88777923583984 step: 22301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03788616 l2_loss: 0.013489624 total_loss: 0.051375784  time: 360.7056484222412 step: 22401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.049848806 l2_loss: 0.013488713 total_loss: 0.06333752  time: 358.18379855155945 step: 22501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061581153 l2_loss: 0.013487819 total_loss: 0.07506897  time: 356.197518825531 step: 22601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05964633 l2_loss: 0.013486975 total_loss: 0.073133305  time: 361.2519609928131 step: 22701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07131787 l2_loss: 0.01348606 total_loss: 0.084803924  time: 359.6861710548401 step: 22801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.080821656 l2_loss: 0.013485058 total_loss: 0.094306715  time: 357.1625576019287 step: 22901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055830073 l2_loss: 0.013484006 total_loss: 0.06931408  time: 356.47582507133484 step: 23001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.056066696 l2_loss: 0.013482993 total_loss: 0.06954969  time: 360.8184289932251 step: 23101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09649245 l2_loss: 0.013481941 total_loss: 0.109974384  time: 360.7478153705597 step: 23201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09767498 l2_loss: 0.013480957 total_loss: 0.111155935  time: 358.3584237098694 step: 23301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09862454 l2_loss: 0.013479989 total_loss: 0.112104535  time: 358.84564995765686 step: 23401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.092230976 l2_loss: 0.013479028 total_loss: 0.10571  time: 360.9765763282776 step: 23501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.048534356 l2_loss: 0.013477942 total_loss: 0.0620123  time: 358.90622115135193 step: 23601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07634062 l2_loss: 0.01347679 total_loss: 0.08981741  time: 356.152619600296 step: 23701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.072964884 l2_loss: 0.013475641 total_loss: 0.086440526  time: 356.5471956729889 step: 23801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.043387033 l2_loss: 0.013474484 total_loss: 0.056861516  time: 357.2344937324524 step: 23901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.032895956 l2_loss: 0.013473313 total_loss: 0.04636927  time: 358.49734354019165 step: 24001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.056200888 l2_loss: 0.013472195 total_loss: 0.06967308  time: 362.8284161090851 step: 24101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0813744 l2_loss: 0.013470942 total_loss: 0.09484534  time: 358.4803566932678 step: 24201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.058277126 l2_loss: 0.013469612 total_loss: 0.07174674  time: 359.2485806941986 step: 24301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07945817 l2_loss: 0.01346831 total_loss: 0.09292648  time: 359.2579290866852 step: 24401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.050061606 l2_loss: 0.01346711 total_loss: 0.06352872  time: 358.99866342544556 step: 24501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.081221834 l2_loss: 0.013465941 total_loss: 0.094687775  time: 355.5088942050934 step: 24601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.038469657 l2_loss: 0.013464746 total_loss: 0.051934402  time: 360.0274600982666 step: 24701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07604861 l2_loss: 0.013463578 total_loss: 0.08951219  time: 360.4469711780548 step: 24801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09327246 l2_loss: 0.013462434 total_loss: 0.106734894  time: 353.33294653892517 step: 24901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07900893 l2_loss: 0.013461285 total_loss: 0.092470214  time: 359.94864869117737 step: 25001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.22564319 l2_loss: 0.013460119 total_loss: 0.2391033  time: 358.3656599521637 step: 25101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1982508 l2_loss: 0.01345901 total_loss: 0.21170981  time: 360.6031804084778 step: 25201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14337355 l2_loss: 0.013457889 total_loss: 0.15683144  time: 364.6285126209259 step: 25301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.15443933 l2_loss: 0.013456706 total_loss: 0.16789603  time: 360.4716160297394 step: 25401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.22979218 l2_loss: 0.013455535 total_loss: 0.24324772  time: 362.43627285957336 step: 25501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.15676285 l2_loss: 0.013454488 total_loss: 0.17021734  time: 356.58193922042847 step: 25601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10508645 l2_loss: 0.01345342 total_loss: 0.11853987  time: 360.0227143764496 step: 25701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.060174417 l2_loss: 0.013452466 total_loss: 0.07362688  time: 360.28407883644104 step: 25801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04365112 l2_loss: 0.0134514645 total_loss: 0.057102583  time: 361.26484632492065 step: 25901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06982569 l2_loss: 0.013450344 total_loss: 0.08327603  time: 363.2829909324646 step: 26001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.07672786 l2_loss: 0.013449203 total_loss: 0.09017706  time: 364.164674282074 step: 26101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12085607 l2_loss: 0.013448104 total_loss: 0.13430417  time: 366.4609980583191 step: 26201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07651681 l2_loss: 0.013446981 total_loss: 0.08996379  time: 391.49827575683594 step: 26301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.117366605 l2_loss: 0.013445821 total_loss: 0.13081242  time: 358.80693793296814 step: 26401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11711686 l2_loss: 0.0134447655 total_loss: 0.13056162  time: 357.09035086631775 step: 26501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12284997 l2_loss: 0.013443692 total_loss: 0.13629366  time: 358.1662492752075 step: 26601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.106621556 l2_loss: 0.013442571 total_loss: 0.120064124  time: 359.69529700279236 step: 26701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08873981 l2_loss: 0.013441423 total_loss: 0.10218123  time: 360.59677624702454 step: 26801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09458391 l2_loss: 0.01344031 total_loss: 0.108024225  time: 358.4505202770233 step: 26901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07901504 l2_loss: 0.013439177 total_loss: 0.09245422  time: 359.3627619743347 step: 27001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06691378 l2_loss: 0.013437871 total_loss: 0.08035165  time: 353.7265536785126 step: 27101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.057116482 l2_loss: 0.013436541 total_loss: 0.07055302  time: 360.3703043460846 step: 27201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.076348536 l2_loss: 0.013435135 total_loss: 0.08978367  time: 359.77949142456055 step: 27301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07696756 l2_loss: 0.013433693 total_loss: 0.090401255  time: 354.23318362236023 step: 27401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.024351541 l2_loss: 0.013432278 total_loss: 0.03778382  time: 355.6189777851105 step: 27501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0490382 l2_loss: 0.013430997 total_loss: 0.0624692  time: 356.6471629142761 step: 27601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.060042005 l2_loss: 0.013429764 total_loss: 0.07347177  time: 358.098192691803 step: 27701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.070311576 l2_loss: 0.013428662 total_loss: 0.083740234  time: 358.80166721343994 step: 27801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05550368 l2_loss: 0.013427386 total_loss: 0.068931065  time: 359.20443630218506 step: 27901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.047547624 l2_loss: 0.013426095 total_loss: 0.06097372  time: 360.9265718460083 step: 28001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.07231127 l2_loss: 0.013424895 total_loss: 0.08573616  time: 361.7301244735718 step: 28101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0428377 l2_loss: 0.013423681 total_loss: 0.056261383  time: 362.1768045425415 step: 28201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06386824 l2_loss: 0.013422632 total_loss: 0.07729087  time: 363.14197516441345 step: 28301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.078265406 l2_loss: 0.01342153 total_loss: 0.091686934  time: 361.1040472984314 step: 28401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10925168 l2_loss: 0.013420469 total_loss: 0.12267215  time: 360.2239372730255 step: 28501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04957503 l2_loss: 0.013419308 total_loss: 0.06299434  time: 357.7121217250824 step: 28601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04825446 l2_loss: 0.013418069 total_loss: 0.06167253  time: 359.3945622444153 step: 28701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.079045676 l2_loss: 0.013416871 total_loss: 0.09246255  time: 359.3885774612427 step: 28801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07643592 l2_loss: 0.0134157175 total_loss: 0.08985164  time: 361.3089163303375 step: 28901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10387015 l2_loss: 0.013414593 total_loss: 0.117284745  time: 356.70924615859985 step: 29001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08601709 l2_loss: 0.013413671 total_loss: 0.099430755  time: 360.05016803741455 step: 29101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09741376 l2_loss: 0.013412431 total_loss: 0.110826194  time: 360.10798740386963 step: 29201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061873674 l2_loss: 0.013411183 total_loss: 0.07528485  time: 360.9195840358734 step: 29301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07952904 l2_loss: 0.013410282 total_loss: 0.092939325  time: 358.7891745567322 step: 29401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.057822395 l2_loss: 0.0134091275 total_loss: 0.07123152  time: 360.3710217475891 step: 29501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.095647685 l2_loss: 0.013408095 total_loss: 0.10905578  time: 358.401495218277 step: 29601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1172679 l2_loss: 0.01340697 total_loss: 0.13067487  time: 362.8593816757202 step: 29701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04811587 l2_loss: 0.013405901 total_loss: 0.061521772  time: 363.02543473243713 step: 29801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.081065245 l2_loss: 0.013404782 total_loss: 0.094470024  time: 358.2396275997162 step: 29901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08207092 l2_loss: 0.0134037165 total_loss: 0.09547463  time: 356.8942177295685 step: 30001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.032271005 l2_loss: 0.013402612 total_loss: 0.045673616  time: 360.2537395954132 step: 30101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0815492 l2_loss: 0.01340172 total_loss: 0.094950914  time: 357.10335898399353 step: 30201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.039932735 l2_loss: 0.013400831 total_loss: 0.053333566  time: 383.1778371334076 step: 30301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06153712 l2_loss: 0.013399937 total_loss: 0.07493706  time: 356.57473254203796 step: 30401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08169779 l2_loss: 0.013399115 total_loss: 0.09509691  time: 354.72314381599426 step: 30501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07537896 l2_loss: 0.013398441 total_loss: 0.0887774  time: 354.647399187088 step: 30601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11410303 l2_loss: 0.013397726 total_loss: 0.12750076  time: 363.42138862609863 step: 30701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07287772 l2_loss: 0.013397023 total_loss: 0.08627474  time: 358.658570766449 step: 30801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07274472 l2_loss: 0.01339619 total_loss: 0.08614091  time: 358.1318111419678 step: 30901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06424737 l2_loss: 0.013395272 total_loss: 0.07764264  time: 357.9185791015625 step: 31001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06612684 l2_loss: 0.013394335 total_loss: 0.07952117  time: 360.90255880355835 step: 31101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.051522993 l2_loss: 0.013393352 total_loss: 0.06491634  time: 356.5600657463074 step: 31201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07270944 l2_loss: 0.013392367 total_loss: 0.08610181  time: 356.08302330970764 step: 31301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.024881547 l2_loss: 0.0133913895 total_loss: 0.038272936  time: 357.17160534858704 step: 31401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.042936873 l2_loss: 0.013390451 total_loss: 0.056327324  time: 358.5604319572449 step: 31501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.049078014 l2_loss: 0.01338955 total_loss: 0.062467564  time: 359.4141685962677 step: 31601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.048470896 l2_loss: 0.013388613 total_loss: 0.06185951  time: 359.3877098560333 step: 31701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06371167 l2_loss: 0.013387814 total_loss: 0.07709949  time: 359.8109242916107 step: 31801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12467276 l2_loss: 0.013387033 total_loss: 0.1380598  time: 358.30486583709717 step: 31901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14209016 l2_loss: 0.013386087 total_loss: 0.15547624  time: 360.60031032562256 step: 32001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.15822141 l2_loss: 0.013385178 total_loss: 0.17160659  time: 355.31711173057556 step: 32101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14847861 l2_loss: 0.013384329 total_loss: 0.16186294  time: 357.9311797618866 step: 32201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.16315486 l2_loss: 0.01338362 total_loss: 0.17653848  time: 391.4694356918335 step: 32301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.117288485 l2_loss: 0.013382831 total_loss: 0.13067132  time: 358.8142445087433 step: 32401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11110112 l2_loss: 0.013381927 total_loss: 0.12448305  time: 362.10778760910034 step: 32501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08512585 l2_loss: 0.013381091 total_loss: 0.09850694  time: 358.43207597732544 step: 32601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04957284 l2_loss: 0.013380125 total_loss: 0.062952965  time: 359.0264301300049 step: 32701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.067903794 l2_loss: 0.013379312 total_loss: 0.08128311  time: 360.1213183403015 step: 32801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046528887 l2_loss: 0.0133783845 total_loss: 0.059907272  time: 358.0250642299652 step: 32901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0975697 l2_loss: 0.013377508 total_loss: 0.11094721  time: 357.96111845970154 step: 33001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07884381 l2_loss: 0.013376844 total_loss: 0.09222066  time: 359.8527705669403 step: 33101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.067429006 l2_loss: 0.013376065 total_loss: 0.08080507  time: 358.73188161849976 step: 33201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.092448354 l2_loss: 0.013375228 total_loss: 0.105823584  time: 358.20918679237366 step: 33301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.035427585 l2_loss: 0.013374368 total_loss: 0.04880195  time: 358.3250288963318 step: 33401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05335532 l2_loss: 0.01337339 total_loss: 0.06672871  time: 357.42064929008484 step: 33501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03326196 l2_loss: 0.013372381 total_loss: 0.04663434  time: 362.2302780151367 step: 33601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.048273157 l2_loss: 0.013371299 total_loss: 0.061644457  time: 356.7385449409485 step: 33701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.033381213 l2_loss: 0.0133701 total_loss: 0.046751313  time: 360.4404180049896 step: 33801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.030335875 l2_loss: 0.013368841 total_loss: 0.04370472  time: 355.72235894203186 step: 33901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06390647 l2_loss: 0.0133675365 total_loss: 0.077274  time: 360.43855142593384 step: 34001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.067745134 l2_loss: 0.01336615 total_loss: 0.08111128  time: 363.12623500823975 step: 34101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.051720027 l2_loss: 0.013364714 total_loss: 0.06508474  time: 365.61674404144287 step: 34201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06476562 l2_loss: 0.013363301 total_loss: 0.07812892  time: 359.72369360923767 step: 34301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06397313 l2_loss: 0.013361969 total_loss: 0.0773351  time: 358.6200911998749 step: 34401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05650483 l2_loss: 0.0133606065 total_loss: 0.069865435  time: 358.55917620658875 step: 34501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.050636698 l2_loss: 0.013359321 total_loss: 0.06399602  time: 359.85276317596436 step: 34601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08812027 l2_loss: 0.013358169 total_loss: 0.101478435  time: 358.27255392074585 step: 34701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06280452 l2_loss: 0.013356845 total_loss: 0.07616136  time: 359.974796295166 step: 34801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053448416 l2_loss: 0.013355588 total_loss: 0.06680401  time: 363.06023240089417 step: 34901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10013286 l2_loss: 0.013354419 total_loss: 0.11348728  time: 362.64434361457825 step: 35001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11069675 l2_loss: 0.013353377 total_loss: 0.124050125  time: 362.3985846042633 step: 35101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.118891954 l2_loss: 0.013352328 total_loss: 0.13224429  time: 359.70705676078796 step: 35201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0794208 l2_loss: 0.013351382 total_loss: 0.09277218  time: 361.19910621643066 step: 35301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09485134 l2_loss: 0.013350389 total_loss: 0.10820173  time: 366.202614068985 step: 35401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.102990225 l2_loss: 0.013349615 total_loss: 0.11633984  time: 363.5841586589813 step: 35501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10179806 l2_loss: 0.013348664 total_loss: 0.11514672  time: 361.5455858707428 step: 35601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10766404 l2_loss: 0.0133477105 total_loss: 0.12101175  time: 360.1702663898468 step: 35701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10388412 l2_loss: 0.013346789 total_loss: 0.117230915  time: 360.3926930427551 step: 35801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04044934 l2_loss: 0.013345886 total_loss: 0.053795226  time: 363.65577840805054 step: 35901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.091248825 l2_loss: 0.0133450525 total_loss: 0.10459388  time: 358.4388906955719 step: 36001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.112450086 l2_loss: 0.013344103 total_loss: 0.12579419  time: 359.02555298805237 step: 36101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.115113296 l2_loss: 0.013343006 total_loss: 0.1284563  time: 356.5803599357605 step: 36201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10356568 l2_loss: 0.013341919 total_loss: 0.1169076  time: 368.4905321598053 step: 36301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13558738 l2_loss: 0.013340675 total_loss: 0.14892806  time: 362.0424189567566 step: 36401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09515823 l2_loss: 0.013339549 total_loss: 0.108497776  time: 359.6225583553314 step: 36501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10949602 l2_loss: 0.013338482 total_loss: 0.1228345  time: 362.2882707118988 step: 36601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.070751764 l2_loss: 0.013337414 total_loss: 0.084089175  time: 360.26348996162415 step: 36701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.036048867 l2_loss: 0.0133364685 total_loss: 0.049385335  time: 359.34066915512085 step: 36801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.026247658 l2_loss: 0.013335561 total_loss: 0.03958322  time: 362.3462438583374 step: 36901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03381673 l2_loss: 0.013334628 total_loss: 0.047151357  time: 358.97973823547363 step: 37001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06932732 l2_loss: 0.013333749 total_loss: 0.08266106  time: 356.3343777656555 step: 37101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06176532 l2_loss: 0.013332879 total_loss: 0.0750982  time: 359.3138597011566 step: 37201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.075458184 l2_loss: 0.013331945 total_loss: 0.088790126  time: 359.37857246398926 step: 37301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06844504 l2_loss: 0.013331021 total_loss: 0.08177606  time: 360.14096117019653 step: 37401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.063673824 l2_loss: 0.013329946 total_loss: 0.07700377  time: 358.31283497810364 step: 37501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.047422737 l2_loss: 0.013328763 total_loss: 0.060751498  time: 358.605761051178 step: 37601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.026843138 l2_loss: 0.013327627 total_loss: 0.040170766  time: 360.96014881134033 step: 37701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.028433656 l2_loss: 0.013326392 total_loss: 0.04176005  time: 353.90770292282104 step: 37801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.044002652 l2_loss: 0.013325114 total_loss: 0.057327766  time: 360.77720403671265 step: 37901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045625564 l2_loss: 0.013323813 total_loss: 0.058949377  time: 356.05671215057373 step: 38001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.047930233 l2_loss: 0.013322592 total_loss: 0.061252825  time: 360.14404916763306 step: 38101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046368565 l2_loss: 0.01332149 total_loss: 0.059690055  time: 360.44636368751526 step: 38201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08647046 l2_loss: 0.013320259 total_loss: 0.09979072  time: 361.81149530410767 step: 38301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.059751216 l2_loss: 0.013319088 total_loss: 0.0730703  time: 376.44891333580017 step: 38401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.052069888 l2_loss: 0.013317792 total_loss: 0.06538768  time: 354.73473739624023 step: 38501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08977417 l2_loss: 0.013316467 total_loss: 0.10309064  time: 356.62996912002563 step: 38601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12642802 l2_loss: 0.013315251 total_loss: 0.13974327  time: 358.7469301223755 step: 38701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1232492 l2_loss: 0.013314126 total_loss: 0.13656333  time: 360.70090889930725 step: 38801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12802741 l2_loss: 0.01331315 total_loss: 0.14134055  time: 359.4121105670929 step: 38901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.075038575 l2_loss: 0.013312171 total_loss: 0.08835074  time: 363.2956953048706 step: 39001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09085863 l2_loss: 0.013311172 total_loss: 0.1041698  time: 362.6548595428467 step: 39101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046538368 l2_loss: 0.013310191 total_loss: 0.05984856  time: 353.6463260650635 step: 39201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07190038 l2_loss: 0.013309118 total_loss: 0.085209504  time: 356.3419978618622 step: 39301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06864906 l2_loss: 0.01330816 total_loss: 0.08195722  time: 363.98522782325745 step: 39401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04632406 l2_loss: 0.013307211 total_loss: 0.05963127  time: 359.2555401325226 step: 39501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.047178894 l2_loss: 0.013306176 total_loss: 0.060485072  time: 360.3803565502167 step: 39601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055767193 l2_loss: 0.01330509 total_loss: 0.069072284  time: 356.8323736190796 step: 39701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.097317435 l2_loss: 0.013303939 total_loss: 0.11062138  time: 358.1927824020386 step: 39801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.20340386 l2_loss: 0.013302825 total_loss: 0.2167067  time: 363.1305146217346 step: 39901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13813192 l2_loss: 0.013301769 total_loss: 0.15143369  time: 355.73146414756775 step: 40001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.18506019 l2_loss: 0.013300815 total_loss: 0.19836101  time: 360.37148785591125 step: 40101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.20373681 l2_loss: 0.013299907 total_loss: 0.21703672  time: 360.1171293258667 step: 40201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0815776 l2_loss: 0.013298898 total_loss: 0.0948765  time: 359.19664478302 step: 40301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.16156882 l2_loss: 0.013297926 total_loss: 0.17486675  time: 360.3214919567108 step: 40401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11589626 l2_loss: 0.013296905 total_loss: 0.12919317  time: 360.51442694664 step: 40501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.039886992 l2_loss: 0.013295862 total_loss: 0.053182855  time: 359.6302742958069 step: 40601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10289517 l2_loss: 0.013294701 total_loss: 0.116189875  time: 358.5186800956726 step: 40701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05461195 l2_loss: 0.013293591 total_loss: 0.067905545  time: 358.1834535598755 step: 40801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06304955 l2_loss: 0.013292413 total_loss: 0.07634196  time: 364.80029916763306 step: 40901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.092170745 l2_loss: 0.013291254 total_loss: 0.105462  time: 358.57993483543396 step: 41001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.111397564 l2_loss: 0.013290127 total_loss: 0.124687694  time: 354.08688020706177 step: 41101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.119201034 l2_loss: 0.013288931 total_loss: 0.13248996  time: 356.28827834129333 step: 41201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046745956 l2_loss: 0.013287756 total_loss: 0.060033713  time: 357.619580745697 step: 41301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14604342 l2_loss: 0.01328654 total_loss: 0.15932997  time: 365.97246980667114 step: 41401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08105452 l2_loss: 0.013285281 total_loss: 0.0943398  time: 357.29374074935913 step: 41501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07083584 l2_loss: 0.0132840285 total_loss: 0.08411987  time: 358.93080019950867 step: 41601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05831789 l2_loss: 0.013282771 total_loss: 0.07160066  time: 362.62312030792236 step: 41701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.064580016 l2_loss: 0.013281486 total_loss: 0.0778615  time: 362.6696274280548 step: 41801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.047105916 l2_loss: 0.013280273 total_loss: 0.06038619  time: 352.4805450439453 step: 41901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07035507 l2_loss: 0.013279022 total_loss: 0.08363409  time: 360.5604190826416 step: 42001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.12943766 l2_loss: 0.013277748 total_loss: 0.14271541  time: 354.30625653266907 step: 42101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0808283 l2_loss: 0.013276447 total_loss: 0.09410475  time: 359.0141530036926 step: 42201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.120439574 l2_loss: 0.013275001 total_loss: 0.13371457  time: 358.1348190307617 step: 42301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07567646 l2_loss: 0.013273602 total_loss: 0.08895007  time: 390.1851165294647 step: 42401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06842218 l2_loss: 0.013272122 total_loss: 0.081694305  time: 353.554105758667 step: 42501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0739661 l2_loss: 0.013270695 total_loss: 0.08723679  time: 358.13892579078674 step: 42601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13413368 l2_loss: 0.013269291 total_loss: 0.14740297  time: 354.0342552661896 step: 42701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13054566 l2_loss: 0.013267981 total_loss: 0.14381364  time: 359.3002014160156 step: 42801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14607774 l2_loss: 0.013266667 total_loss: 0.1593444  time: 360.1677165031433 step: 42901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.078728475 l2_loss: 0.013265408 total_loss: 0.09199388  time: 365.3934907913208 step: 43001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08178469 l2_loss: 0.013264134 total_loss: 0.09504882  time: 360.50295639038086 step: 43101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07880375 l2_loss: 0.013262922 total_loss: 0.09206667  time: 363.0966901779175 step: 43201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07681482 l2_loss: 0.013261631 total_loss: 0.090076454  time: 356.8499286174774 step: 43301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0681991 l2_loss: 0.0132602705 total_loss: 0.081459366  time: 362.24737763404846 step: 43401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05134194 l2_loss: 0.013258952 total_loss: 0.06460089  time: 361.28850507736206 step: 43501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10992063 l2_loss: 0.013257897 total_loss: 0.12317853  time: 358.0198860168457 step: 43601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03379008 l2_loss: 0.013256901 total_loss: 0.04704698  time: 358.28614687919617 step: 43701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09112679 l2_loss: 0.013255995 total_loss: 0.10438278  time: 357.0949146747589 step: 43801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06443839 l2_loss: 0.013255209 total_loss: 0.0776936  time: 354.4860758781433 step: 43901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.070152044 l2_loss: 0.013254259 total_loss: 0.0834063  time: 352.2793552875519 step: 44001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.042752203 l2_loss: 0.013253183 total_loss: 0.056005385  time: 361.5671525001526 step: 44101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09993867 l2_loss: 0.013252081 total_loss: 0.11319075  time: 357.3744812011719 step: 44201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045779444 l2_loss: 0.0132509805 total_loss: 0.059030425  time: 360.99853801727295 step: 44301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10760178 l2_loss: 0.0132498 total_loss: 0.12085158  time: 353.73713755607605 step: 44401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05154952 l2_loss: 0.013248495 total_loss: 0.06479801  time: 359.9268250465393 step: 44501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.076234005 l2_loss: 0.013247354 total_loss: 0.08948136  time: 362.0537738800049 step: 44601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09842991 l2_loss: 0.01324636 total_loss: 0.11167627  time: 363.08418583869934 step: 44701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10816232 l2_loss: 0.013245366 total_loss: 0.12140769  time: 359.1904237270355 step: 44801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07217225 l2_loss: 0.013244426 total_loss: 0.085416675  time: 359.9805438518524 step: 44901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04153834 l2_loss: 0.013243444 total_loss: 0.054781783  time: 355.2225432395935 step: 45001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08652299 l2_loss: 0.013242459 total_loss: 0.09976545  time: 356.9149672985077 step: 45101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11326614 l2_loss: 0.01324136 total_loss: 0.1265075  time: 361.2436068058014 step: 45201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06208235 l2_loss: 0.013240219 total_loss: 0.07532257  time: 359.73414635658264 step: 45301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.065143496 l2_loss: 0.013239209 total_loss: 0.0783827  time: 356.0427141189575 step: 45401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06491295 l2_loss: 0.013238038 total_loss: 0.07815099  time: 359.4972472190857 step: 45501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11527292 l2_loss: 0.013236921 total_loss: 0.12850983  time: 359.5524549484253 step: 45601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06945152 l2_loss: 0.013235693 total_loss: 0.082687214  time: 361.32025241851807 step: 45701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1283447 l2_loss: 0.013234523 total_loss: 0.14157923  time: 358.3571093082428 step: 45801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061253015 l2_loss: 0.013233362 total_loss: 0.074486375  time: 358.9976558685303 step: 45901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07296036 l2_loss: 0.013232324 total_loss: 0.08619268  time: 359.49627685546875 step: 46001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.06546637 l2_loss: 0.013231134 total_loss: 0.0786975  time: 351.75884461402893 step: 46101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061695777 l2_loss: 0.013229837 total_loss: 0.07492562  time: 357.2730815410614 step: 46201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.079362154 l2_loss: 0.013228503 total_loss: 0.09259066  time: 352.5563189983368 step: 46301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053123627 l2_loss: 0.013227026 total_loss: 0.066350654  time: 361.09298825263977 step: 46401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.049318377 l2_loss: 0.013225717 total_loss: 0.06254409  time: 355.18924355506897 step: 46501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09291644 l2_loss: 0.013224366 total_loss: 0.1061408  time: 360.11771416664124 step: 46601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053395208 l2_loss: 0.013223245 total_loss: 0.06661845  time: 358.0304033756256 step: 46701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.034315787 l2_loss: 0.0132221 total_loss: 0.047537886  time: 355.86303424835205 step: 46801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07283307 l2_loss: 0.013221026 total_loss: 0.086054094  time: 364.67646861076355 step: 46901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.034870047 l2_loss: 0.013220006 total_loss: 0.04809005  time: 360.7522270679474 step: 47001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.083435595 l2_loss: 0.013218988 total_loss: 0.09665458  time: 355.89403653144836 step: 47101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07144896 l2_loss: 0.013217976 total_loss: 0.08466694  time: 363.5871675014496 step: 47201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05485105 l2_loss: 0.0132168345 total_loss: 0.068067886  time: 360.4226002693176 step: 47301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.062347934 l2_loss: 0.013215732 total_loss: 0.07556367  time: 359.75643968582153 step: 47401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.066841125 l2_loss: 0.013214626 total_loss: 0.08005575  time: 360.51046895980835 step: 47501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046983175 l2_loss: 0.013213553 total_loss: 0.060196728  time: 356.17126846313477 step: 47601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0643415 l2_loss: 0.0132123865 total_loss: 0.07755388  time: 356.92369198799133 step: 47701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.037581842 l2_loss: 0.013211724 total_loss: 0.050793566  time: 358.2045133113861 step: 47801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.030485181 l2_loss: 0.013210481 total_loss: 0.043695662  time: 360.2478356361389 step: 47901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10862552 l2_loss: 0.013209368 total_loss: 0.12183488  time: 362.1903781890869 step: 48001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.026392376 l2_loss: 0.013208346 total_loss: 0.039600722  time: 360.80503392219543 step: 48101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.028767092 l2_loss: 0.013207341 total_loss: 0.041974433  time: 360.27735710144043 step: 48201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055351138 l2_loss: 0.013206313 total_loss: 0.06855745  time: 356.1430296897888 step: 48301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.069215395 l2_loss: 0.013205263 total_loss: 0.082420655  time: 375.099556684494 step: 48401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.074057944 l2_loss: 0.0132042505 total_loss: 0.0872622  time: 359.04377841949463 step: 48501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09105487 l2_loss: 0.01320321 total_loss: 0.10425808  time: 355.15482997894287 step: 48601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10024223 l2_loss: 0.013202073 total_loss: 0.1134443  time: 353.4978563785553 step: 48701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.035748567 l2_loss: 0.013200937 total_loss: 0.048949502  time: 353.7615237236023 step: 48801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.028491963 l2_loss: 0.013199688 total_loss: 0.04169165  time: 357.6637170314789 step: 48901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12685384 l2_loss: 0.013198464 total_loss: 0.1400523  time: 359.2262976169586 step: 49001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.043240633 l2_loss: 0.013197334 total_loss: 0.056437965  time: 356.7439239025116 step: 49101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.059466567 l2_loss: 0.013196093 total_loss: 0.07266266  time: 360.5883672237396 step: 49201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.035546347 l2_loss: 0.013194778 total_loss: 0.048741125  time: 358.28294563293457 step: 49301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.024109423 l2_loss: 0.01319348 total_loss: 0.037302904  time: 359.9736247062683 step: 49401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.072524495 l2_loss: 0.013192204 total_loss: 0.0857167  time: 359.2832827568054 step: 49501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03618717 l2_loss: 0.013191008 total_loss: 0.049378175  time: 358.56972098350525 step: 49601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.088818334 l2_loss: 0.013189778 total_loss: 0.10200811  time: 361.01990485191345 step: 49701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06315065 l2_loss: 0.013188513 total_loss: 0.07633916  time: 362.36689352989197 step: 49801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06917756 l2_loss: 0.013187257 total_loss: 0.08236482  time: 359.59656167030334 step: 49901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08028588 l2_loss: 0.013186025 total_loss: 0.0934719  time: 359.9470202922821 step: 50001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.05361966 l2_loss: 0.013184828 total_loss: 0.06680449  time: 360.35214257240295 step: 50101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.054330274 l2_loss: 0.013183738 total_loss: 0.06751401  time: 360.6725289821625 step: 50201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07201749 l2_loss: 0.013182884 total_loss: 0.08520038  time: 359.27193689346313 step: 50301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09714939 l2_loss: 0.013182278 total_loss: 0.11033166  time: 360.18481492996216 step: 50401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04924717 l2_loss: 0.013181586 total_loss: 0.062428758  time: 361.0717394351959 step: 50501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08958107 l2_loss: 0.013180922 total_loss: 0.10276199  time: 361.5679614543915 step: 50601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08832714 l2_loss: 0.013180277 total_loss: 0.10150742  time: 354.45767283439636 step: 50701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.060675237 l2_loss: 0.013179556 total_loss: 0.07385479  time: 360.0849621295929 step: 50801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.043205146 l2_loss: 0.013178814 total_loss: 0.05638396  time: 359.5619547367096 step: 50901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.037957784 l2_loss: 0.01317796 total_loss: 0.051135745  time: 361.59833121299744 step: 51001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053843543 l2_loss: 0.013177093 total_loss: 0.06702064  time: 353.25618052482605 step: 51101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046678875 l2_loss: 0.013176092 total_loss: 0.059854966  time: 362.68337225914 step: 51201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045074664 l2_loss: 0.013175183 total_loss: 0.058249846  time: 360.00040197372437 step: 51301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08509913 l2_loss: 0.013174249 total_loss: 0.09827338  time: 358.48277735710144 step: 51401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05357324 l2_loss: 0.013173492 total_loss: 0.066746734  time: 351.5320727825165 step: 51501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053592823 l2_loss: 0.013172595 total_loss: 0.06676542  time: 356.6483645439148 step: 51601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05854056 l2_loss: 0.013171689 total_loss: 0.07171225  time: 360.04949259757996 step: 51701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.034883495 l2_loss: 0.013170779 total_loss: 0.048054274  time: 354.7331438064575 step: 51801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12007854 l2_loss: 0.013169853 total_loss: 0.13324839  time: 357.1556305885315 step: 51901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.1575908 l2_loss: 0.013168897 total_loss: 0.17075971  time: 363.6111669540405 step: 52001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.20484921 l2_loss: 0.013168083 total_loss: 0.2180173  time: 357.0862612724304 step: 52101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.19110152 l2_loss: 0.013167176 total_loss: 0.2042687  time: 361.1525399684906 step: 52201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.25682965 l2_loss: 0.013166229 total_loss: 0.26999587  time: 360.2175803184509 step: 52301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.21718685 l2_loss: 0.013165312 total_loss: 0.23035216  time: 355.24873185157776 step: 52401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.187552 l2_loss: 0.013164447 total_loss: 0.20071645  time: 376.5593011379242 step: 52501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.141992 l2_loss: 0.013163407 total_loss: 0.1551554  time: 353.38017535209656 step: 52601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09255121 l2_loss: 0.013162315 total_loss: 0.105713524  time: 354.0178859233856 step: 52701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09951044 l2_loss: 0.013161239 total_loss: 0.11267168  time: 357.57938957214355 step: 52801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.028492674 l2_loss: 0.013160238 total_loss: 0.04165291  time: 359.27369570732117 step: 52901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.044107683 l2_loss: 0.013159184 total_loss: 0.05726687  time: 357.09119725227356 step: 53001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06823789 l2_loss: 0.013158017 total_loss: 0.08139591  time: 361.3111460208893 step: 53101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.013326435 l2_loss: 0.013156772 total_loss: 0.026483208  time: 355.77615308761597 step: 53201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.034625225 l2_loss: 0.013155543 total_loss: 0.047780767  time: 357.78948068618774 step: 53301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04878112 l2_loss: 0.013154289 total_loss: 0.06193541  time: 357.86221623420715 step: 53401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.037987698 l2_loss: 0.013153019 total_loss: 0.05114072  time: 358.07562375068665 step: 53501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06422626 l2_loss: 0.013151901 total_loss: 0.07737816  time: 356.3105294704437 step: 53601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.072711974 l2_loss: 0.013150815 total_loss: 0.085862786  time: 357.47696113586426 step: 53701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053042255 l2_loss: 0.013149798 total_loss: 0.06619205  time: 359.63367462158203 step: 53801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07485938 l2_loss: 0.013148707 total_loss: 0.08800809  time: 355.0854985713959 step: 53901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.042263173 l2_loss: 0.013147592 total_loss: 0.055410765  time: 355.7844009399414 step: 54001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.03595951 l2_loss: 0.013146542 total_loss: 0.04910605  time: 356.5155107975006 step: 54101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03506284 l2_loss: 0.013145468 total_loss: 0.048208307  time: 357.1216812133789 step: 54201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0506495 l2_loss: 0.013144338 total_loss: 0.06379384  time: 355.7881031036377 step: 54301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05586424 l2_loss: 0.01314328 total_loss: 0.06900752  time: 361.08076429367065 step: 54401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.025454959 l2_loss: 0.01314216 total_loss: 0.038597118  time: 361.1426227092743 step: 54501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.046735782 l2_loss: 0.013141073 total_loss: 0.059876855  time: 357.9818949699402 step: 54601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.03678498 l2_loss: 0.013139943 total_loss: 0.049924925  time: 358.01335620880127 step: 54701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.121391185 l2_loss: 0.013138742 total_loss: 0.13452993  time: 359.81611227989197 step: 54801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.088965856 l2_loss: 0.0131374635 total_loss: 0.10210332  time: 354.2607789039612 step: 54901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0557452 l2_loss: 0.013136195 total_loss: 0.06888139  time: 360.24231147766113 step: 55001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.05518301 l2_loss: 0.013135023 total_loss: 0.06831803  time: 357.70975065231323 step: 55101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.054476336 l2_loss: 0.013133955 total_loss: 0.06761029  time: 361.6814544200897 step: 55201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09243257 l2_loss: 0.013132931 total_loss: 0.1055655  time: 361.4220175743103 step: 55301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.086209536 l2_loss: 0.013131854 total_loss: 0.09934139  time: 358.83814883232117 step: 55401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07526277 l2_loss: 0.013130917 total_loss: 0.08839369  time: 352.85904717445374 step: 55501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045212943 l2_loss: 0.013129931 total_loss: 0.058342874  time: 360.8494930267334 step: 55601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.13265444 l2_loss: 0.0131289 total_loss: 0.14578335  time: 350.6873037815094 step: 55701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06767504 l2_loss: 0.0131277405 total_loss: 0.08080278  time: 359.24472641944885 step: 55801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.093405016 l2_loss: 0.0131266145 total_loss: 0.10653163  time: 355.4339871406555 step: 55901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04967311 l2_loss: 0.013125532 total_loss: 0.06279864  time: 356.80965399742126 step: 56001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.070594095 l2_loss: 0.0131246485 total_loss: 0.08371875  time: 360.6612129211426 step: 56101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.054117512 l2_loss: 0.013123702 total_loss: 0.067241214  time: 357.7520925998688 step: 56201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11672598 l2_loss: 0.01312275 total_loss: 0.12984873  time: 355.45743894577026 step: 56301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.15702453 l2_loss: 0.013121847 total_loss: 0.17014638  time: 360.02647829055786 step: 56401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.118684575 l2_loss: 0.013120858 total_loss: 0.13180543  time: 355.985066652298 step: 56501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07893441 l2_loss: 0.013119813 total_loss: 0.09205422  time: 355.9891712665558 step: 56601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09663469 l2_loss: 0.01311882 total_loss: 0.10975351  time: 358.37756514549255 step: 56701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.10570166 l2_loss: 0.013117841 total_loss: 0.118819505  time: 357.6671187877655 step: 56801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08624767 l2_loss: 0.013116824 total_loss: 0.09936449  time: 360.94551134109497 step: 56901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07779398 l2_loss: 0.01311584 total_loss: 0.09090982  time: 355.7914652824402 step: 57001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09527061 l2_loss: 0.013114722 total_loss: 0.10838533  time: 355.82460618019104 step: 57101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09126449 l2_loss: 0.013113601 total_loss: 0.10437809  time: 361.0379192829132 step: 57201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08423328 l2_loss: 0.013112417 total_loss: 0.097345695  time: 354.87983655929565 step: 57301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.053856768 l2_loss: 0.013111254 total_loss: 0.066968024  time: 359.14918875694275 step: 57401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061139952 l2_loss: 0.013110142 total_loss: 0.074250095  time: 355.7845320701599 step: 57501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04532925 l2_loss: 0.013109036 total_loss: 0.058438286  time: 357.50199460983276 step: 57601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.114300825 l2_loss: 0.013108099 total_loss: 0.12740892  time: 359.3951292037964 step: 57701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.050331566 l2_loss: 0.013107376 total_loss: 0.063438945  time: 362.0408399105072 step: 57801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06964343 l2_loss: 0.013106628 total_loss: 0.08275006  time: 357.32178473472595 step: 57901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045980502 l2_loss: 0.013105781 total_loss: 0.05908628  time: 360.7872107028961 step: 58001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.116173856 l2_loss: 0.013104931 total_loss: 0.1292788  time: 358.56994247436523 step: 58101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055158082 l2_loss: 0.013103995 total_loss: 0.06826208  time: 354.24154829978943 step: 58201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.076783 l2_loss: 0.013103027 total_loss: 0.089886025  time: 360.57637667655945 step: 58301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08586111 l2_loss: 0.013102052 total_loss: 0.098963164  time: 355.55443048477173 step: 58401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09424261 l2_loss: 0.013100909 total_loss: 0.10734352  time: 383.1981871128082 step: 58501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06751143 l2_loss: 0.013099699 total_loss: 0.08061113  time: 358.5339603424072 step: 58601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061404012 l2_loss: 0.013098544 total_loss: 0.07450256  time: 358.13852429389954 step: 58701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.107146464 l2_loss: 0.013097429 total_loss: 0.12024389  time: 352.7566227912903 step: 58801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.121475406 l2_loss: 0.013096344 total_loss: 0.13457175  time: 356.35733437538147 step: 58901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.100077145 l2_loss: 0.013095257 total_loss: 0.113172404  time: 355.2865962982178 step: 59001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.08008652 l2_loss: 0.013094143 total_loss: 0.093180664  time: 353.50631070137024 step: 59101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09457173 l2_loss: 0.013092928 total_loss: 0.10766466  time: 359.0120041370392 step: 59201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12082578 l2_loss: 0.013091746 total_loss: 0.13391753  time: 358.48761773109436 step: 59301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.078250036 l2_loss: 0.013090478 total_loss: 0.09134051  time: 354.7862184047699 step: 59401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14584458 l2_loss: 0.013089236 total_loss: 0.15893382  time: 360.2681176662445 step: 59501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.0737075 l2_loss: 0.013088076 total_loss: 0.086795576  time: 361.8749535083771 step: 59601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055932116 l2_loss: 0.013086914 total_loss: 0.06901903  time: 357.78513741493225 step: 59701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06585611 l2_loss: 0.013085822 total_loss: 0.07894193  time: 356.7219994068146 step: 59801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07457629 l2_loss: 0.013084658 total_loss: 0.087660946  time: 360.26827001571655 step: 59901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11244223 l2_loss: 0.013083411 total_loss: 0.12552564  time: 357.1814241409302 step: 60001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.07296778 l2_loss: 0.013082113 total_loss: 0.0860499  time: 358.33564710617065 step: 60101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11489697 l2_loss: 0.013080875 total_loss: 0.12797785  time: 354.7718484401703 step: 60201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.058708277 l2_loss: 0.013079608 total_loss: 0.07178789  time: 358.46034717559814 step: 60301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09426952 l2_loss: 0.013078381 total_loss: 0.107347906  time: 361.86366176605225 step: 60401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.07413592 l2_loss: 0.013077294 total_loss: 0.08721322  time: 356.9290657043457 step: 60501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.061289407 l2_loss: 0.013076255 total_loss: 0.07436566  time: 354.9016869068146 step: 60601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.055440523 l2_loss: 0.013075131 total_loss: 0.06851565  time: 352.72052097320557 step: 60701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.076031156 l2_loss: 0.013074055 total_loss: 0.08910521  time: 354.98821210861206 step: 60801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.059074447 l2_loss: 0.013072964 total_loss: 0.072147414  time: 357.854496717453 step: 60901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09951362 l2_loss: 0.013071878 total_loss: 0.1125855  time: 352.07437086105347 step: 61001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.04314036 l2_loss: 0.013070666 total_loss: 0.056211025  time: 357.1464681625366 step: 61101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.057921067 l2_loss: 0.013069418 total_loss: 0.07099049  time: 359.5147795677185 step: 61201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.02295587 l2_loss: 0.013068249 total_loss: 0.03602412  time: 357.5300693511963 step: 61301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.045494188 l2_loss: 0.013067193 total_loss: 0.05856138  time: 357.19472765922546 step: 61401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09776206 l2_loss: 0.013066214 total_loss: 0.11082828  time: 358.5058059692383 step: 61501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09777502 l2_loss: 0.013065252 total_loss: 0.110840276  time: 357.56705594062805 step: 61601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14535375 l2_loss: 0.013064423 total_loss: 0.15841818  time: 361.9286458492279 step: 61701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14887081 l2_loss: 0.013063591 total_loss: 0.1619344  time: 358.4421741962433 step: 61801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.109755844 l2_loss: 0.013062881 total_loss: 0.12281872  time: 360.0435206890106 step: 61901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.16099451 l2_loss: 0.0130620515 total_loss: 0.17405656  time: 356.03060245513916 step: 62001 lr: 2.5e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.13471748 l2_loss: 0.013061154 total_loss: 0.14777863  time: 358.0519127845764 step: 62101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12398965 l2_loss: 0.013060168 total_loss: 0.13704982  time: 359.9121859073639 step: 62201 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.048371255 l2_loss: 0.013059222 total_loss: 0.061430477  time: 361.21415400505066 step: 62301 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09288699 l2_loss: 0.013058322 total_loss: 0.10594531  time: 361.05572748184204 step: 62401 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.059625898 l2_loss: 0.0130574135 total_loss: 0.07268331  time: 359.587792634964 step: 62501 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06212267 l2_loss: 0.01305644 total_loss: 0.07517911  time: 362.6925389766693 step: 62601 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.09901378 l2_loss: 0.013055482 total_loss: 0.112069264  time: 351.8112494945526 step: 62701 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.06781743 l2_loss: 0.013054602 total_loss: 0.08087203  time: 362.6670880317688 step: 62801 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.093755856 l2_loss: 0.013053684 total_loss: 0.10680954  time: 358.22429180145264 step: 62901 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.11504667 l2_loss: 0.013052719 total_loss: 0.1280994  time: 354.7380838394165 step: 63001 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.12139472 l2_loss: 0.013051724 total_loss: 0.13444644  time: 357.216349363327 step: 63101 lr: 2.5e-06
[1,0]<stdout>:model_loss: 0.14186731 l2_loss: 0.013050689 total_loss: 0.154918  time: 360.0634083747864 step: 63201 lr: 2.5e-06
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 0 on node dlm38 exited on signal 9 (Killed).
--------------------------------------------------------------------------
