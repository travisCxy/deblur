
2023-05-08 08:30:11.751706: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,3]<stderr>:2023-05-08 08:30:13.716239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,2]<stderr>:2023-05-08 08:30:13.722759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,1]<stderr>:2023-05-08 08:30:13.722844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2023-05-08 08:30:13.749718: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,1]<stderr>:2023-05-08 08:30:14.828793: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
[1,2]<stderr>:2023-05-08 08:30:14.828983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
[1,0]<stderr>:2023-05-08 08:30:14.829047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
[1,3]<stderr>:2023-05-08 08:30:14.829521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
[1,1]<stderr>:2023-05-08 08:30:15.358802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,1]<stderr>:pciBusID: 0000:0e:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,1]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:15.361141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 1 with properties: 
[1,1]<stderr>:pciBusID: 0000:13:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,1]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:15.363813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 2 with properties: 
[1,1]<stderr>:pciBusID: 0000:4a:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,1]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:15.366286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 3 with properties: 
[1,1]<stderr>:pciBusID: 0000:50:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,1]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:15.366318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,3]<stderr>:2023-05-08 08:30:15.369013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,3]<stderr>:pciBusID: 0000:0e:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,3]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:15.370238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,3]<stderr>:2023-05-08 08:30:15.370621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 1 with properties: 
[1,3]<stderr>:pciBusID: 0000:13:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,3]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,2]<stderr>:2023-05-08 08:30:15.371241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,2]<stderr>:pciBusID: 0000:0e:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,2]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:15.371685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,1]<stderr>:2023-05-08 08:30:15.372080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,3]<stderr>:2023-05-08 08:30:15.372855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 2 with properties: 
[1,3]<stderr>:pciBusID: 0000:4a:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,3]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,2]<stderr>:2023-05-08 08:30:15.373144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 1 with properties: 
[1,2]<stderr>:pciBusID: 0000:13:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,2]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,0]<stderr>:2023-05-08 08:30:15.374373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,0]<stderr>:pciBusID: 0000:0e:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,0]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,2]<stderr>:2023-05-08 08:30:15.374863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 2 with properties: 
[1,2]<stderr>:pciBusID: 0000:4a:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,2]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,3]<stderr>:2023-05-08 08:30:15.375061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 3 with properties: 
[1,3]<stderr>:pciBusID: 0000:50:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,3]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,3]<stderr>:2023-05-08 08:30:15.375092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,1]<stderr>:2023-05-08 08:30:15.375500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,0]<stderr>:2023-05-08 08:30:15.375776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 1 with properties: 
[1,0]<stderr>:pciBusID: 0000:13:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,0]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,2]<stderr>:2023-05-08 08:30:15.376193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 3 with properties: 
[1,2]<stderr>:pciBusID: 0000:50:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,2]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,2]<stderr>:2023-05-08 08:30:15.376216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,1]<stderr>:2023-05-08 08:30:15.376466: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,1]<stderr>:2023-05-08 08:30:15.376656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2023-05-08 08:30:15.376854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 2 with properties: 
[1,0]<stderr>:pciBusID: 0000:4a:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,0]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,0]<stderr>:2023-05-08 08:30:15.378027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 3 with properties: 
[1,0]<stderr>:pciBusID: 0000:50:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,0]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,0]<stderr>:2023-05-08 08:30:15.378050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,3]<stderr>:2023-05-08 08:30:15.378912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,2]<stderr>:2023-05-08 08:30:15.378988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,2]<stderr>:2023-05-08 08:30:15.379845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,2]<stderr>:2023-05-08 08:30:15.380053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,3]<stderr>:2023-05-08 08:30:15.380325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,3]<stderr>:2023-05-08 08:30:15.380641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,0]<stderr>:2023-05-08 08:30:15.381029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2023-05-08 08:30:15.382235: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,0]<stderr>:2023-05-08 08:30:15.382463: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,2]<stderr>:2023-05-08 08:30:15.382656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,2]<stderr>:2023-05-08 08:30:15.383258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,2]<stderr>:2023-05-08 08:30:15.383381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,3]<stderr>:2023-05-08 08:30:15.384770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,0]<stderr>:2023-05-08 08:30:15.385440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,1]<stderr>:2023-05-08 08:30:15.385794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 0, 1, 2, 3
[1,3]<stderr>:2023-05-08 08:30:15.385831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,1]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]
[1,3]<stderr>:2023-05-08 08:30:15.386047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2023-05-08 08:30:15.386186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,0]<stderr>:2023-05-08 08:30:15.386320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,2]<stderr>:2023-05-08 08:30:15.396745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 0, 1, 2, 3
[1,2]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]
[1,1]<stderr>:2023-05-08 08:30:15.398893: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2345575000 Hz
[1,3]<stderr>:2023-05-08 08:30:15.399471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 0, 1, 2, 3
[1,3]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]
[1,0]<stderr>:2023-05-08 08:30:15.400755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 0, 1, 2, 3
[1,0]<stdout>:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]
[1,1]<stderr>:2023-05-08 08:30:15.406158: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x579b930 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
[1,1]<stderr>:2023-05-08 08:30:15.406222: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[1,2]<stderr>:2023-05-08 08:30:15.411894: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2345575000 Hz
[1,3]<stderr>:2023-05-08 08:30:15.415105: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2345575000 Hz
[1,0]<stderr>:2023-05-08 08:30:15.415212: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2345575000 Hz
[1,2]<stderr>:2023-05-08 08:30:15.417802: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4e468e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
[1,2]<stderr>:2023-05-08 08:30:15.417857: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[1,3]<stderr>:2023-05-08 08:30:15.422245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ae19e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
[1,3]<stderr>:2023-05-08 08:30:15.422310: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[1,0]<stderr>:2023-05-08 08:30:15.424196: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a8e930 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
[1,0]<stderr>:2023-05-08 08:30:15.424243: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[1,1]<stderr>:2023-05-08 08:30:16.199521: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5808ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[1,1]<stderr>:2023-05-08 08:30:16.199819: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
[1,1]<stderr>:2023-05-08 08:30:16.202192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,1]<stderr>:pciBusID: 0000:13:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,1]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,1]<stderr>:2023-05-08 08:30:16.202272: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,1]<stderr>:2023-05-08 08:30:16.202370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,1]<stderr>:2023-05-08 08:30:16.202419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,1]<stderr>:2023-05-08 08:30:16.202461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,1]<stderr>:2023-05-08 08:30:16.202505: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,1]<stderr>:2023-05-08 08:30:16.202544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,1]<stderr>:2023-05-08 08:30:16.202582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,2]<stderr>:2023-05-08 08:30:16.204726: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4eb3ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[1,2]<stderr>:2023-05-08 08:30:16.204806: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
[1,1]<stderr>:2023-05-08 08:30:16.205435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 1
[1,1]<stderr>:2023-05-08 08:30:16.205509: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,2]<stderr>:2023-05-08 08:30:16.208627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,2]<stderr>:pciBusID: 0000:4a:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,2]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,2]<stderr>:2023-05-08 08:30:16.208670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,2]<stderr>:2023-05-08 08:30:16.208728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,2]<stderr>:2023-05-08 08:30:16.208750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,2]<stderr>:2023-05-08 08:30:16.208767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,2]<stderr>:2023-05-08 08:30:16.208785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,2]<stderr>:2023-05-08 08:30:16.208801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,2]<stderr>:2023-05-08 08:30:16.208824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2023-05-08 08:30:16.214444: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3f63ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[1,0]<stderr>:2023-05-08 08:30:16.214563: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
[1,2]<stderr>:2023-05-08 08:30:16.214878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 2
[1,2]<stderr>:2023-05-08 08:30:16.214918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2023-05-08 08:30:16.216970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,0]<stderr>:pciBusID: 0000:0e:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,0]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,0]<stderr>:2023-05-08 08:30:16.217050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2023-05-08 08:30:16.217127: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2023-05-08 08:30:16.217154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,0]<stderr>:2023-05-08 08:30:16.217175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,0]<stderr>:2023-05-08 08:30:16.217195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,0]<stderr>:2023-05-08 08:30:16.217215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,0]<stderr>:2023-05-08 08:30:16.217251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2023-05-08 08:30:16.219901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 0
[1,0]<stderr>:2023-05-08 08:30:16.219957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,3]<stderr>:2023-05-08 08:30:16.224171: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fb6ad0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[1,3]<stderr>:2023-05-08 08:30:16.224253: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0
[1,3]<stderr>:2023-05-08 08:30:16.227717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1742] Found device 0 with properties: 
[1,3]<stderr>:pciBusID: 0000:50:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0
[1,3]<stderr>:coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
[1,3]<stderr>:2023-05-08 08:30:16.227770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,3]<stderr>:2023-05-08 08:30:16.227826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,3]<stderr>:2023-05-08 08:30:16.227849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
[1,3]<stderr>:2023-05-08 08:30:16.227872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
[1,3]<stderr>:2023-05-08 08:30:16.227893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
[1,3]<stderr>:2023-05-08 08:30:16.227913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
[1,3]<stderr>:2023-05-08 08:30:16.227933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,3]<stderr>:2023-05-08 08:30:16.230974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Adding visible gpu devices: 3
[1,3]<stderr>:2023-05-08 08:30:16.231030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2023-05-08 08:30:16.548122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1283] Device interconnect StreamExecutor with strength 1 edge matrix:
[1,0]<stderr>:2023-05-08 08:30:16.548202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1289]      0 
[1,0]<stderr>:2023-05-08 08:30:16.548211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1302] 0:   N 
[1,0]<stderr>:2023-05-08 08:30:16.550732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1428] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20139 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0)
[1,0]<stdout>:============ load project: cht ================
[1,1]<stderr>:2023-05-08 08:30:16.583536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1283] Device interconnect StreamExecutor with strength 1 edge matrix:
[1,1]<stderr>:2023-05-08 08:30:16.583609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1289]      1 
[1,1]<stderr>:2023-05-08 08:30:16.583620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1302] 1:   N 
[1,1]<stderr>:2023-05-08 08:30:16.585711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1428] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 19995 MB memory) -> physical GPU (device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:13:00.0, compute capability: 8.0)
[1,1]<stdout>:============ load project: cht ================
[1,2]<stderr>:2023-05-08 08:30:16.637957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1283] Device interconnect StreamExecutor with strength 1 edge matrix:
[1,2]<stderr>:2023-05-08 08:30:16.638051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1289]      2 
[1,2]<stderr>:2023-05-08 08:30:16.638065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1302] 2:   N 
[1,2]<stderr>:2023-05-08 08:30:16.642980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1428] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 19995 MB memory) -> physical GPU (device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:4a:00.0, compute capability: 8.0)
[1,3]<stderr>:2023-05-08 08:30:16.666697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1283] Device interconnect StreamExecutor with strength 1 edge matrix:
[1,3]<stderr>:2023-05-08 08:30:16.666880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1289]      3 
[1,3]<stderr>:2023-05-08 08:30:16.666892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1302] 3:   N 
[1,2]<stdout>:============ load project: cht ================
[1,3]<stderr>:2023-05-08 08:30:16.669972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1428] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 20139 MB memory) -> physical GPU (device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:50:00.0, compute capability: 8.0)
[1,3]<stdout>:============ load project: cht ================
[1,0]<stdout>:add 1500 chinese traditional chars
[1,0]<stdout>:----------------------------------------
[1,1]<stdout>:add 1500 chinese traditional chars
[1,1]<stdout>:----------------------------------------
[1,2]<stdout>:add 1500 chinese traditional chars
[1,2]<stdout>:----------------------------------------
[1,3]<stdout>:add 1500 chinese traditional chars
[1,3]<stdout>:----------------------------------------
[1,2]<stderr>:2023-05-08 08:30:25.513775: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[1,2]<stdout>:================== total count: 3993 ==============
[1,1]<stderr>:2023-05-08 08:30:25.780716: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[1,0]<stderr>:2023-05-08 08:30:25.791083: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[1,1]<stdout>:================== total count: 3993 ==============
[1,0]<stdout>:================== total count: 3993 ==============
[1,2]<stderr>:WARNING:tensorflow:From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,2]<stderr>:Instructions for updating:
[1,2]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,2]<stderr>:    options available in V2.
[1,2]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,2]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,2]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,2]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,2]<stderr>:    being differentiable using a gradient tape.
[1,2]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,2]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,2]<stderr>:    stateful argument making all functions stateful.
[1,2]<stderr>:    
[1,2]<stderr>:W0508 08:30:26.055800 140682402551616 deprecation.py:317] From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,2]<stderr>:Instructions for updating:
[1,2]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,2]<stderr>:    options available in V2.
[1,2]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,2]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,2]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,2]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,2]<stderr>:    being differentiable using a gradient tape.
[1,2]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,2]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,2]<stderr>:    stateful argument making all functions stateful.
[1,2]<stderr>:    
[1,2]<stdout>:loading from existing checkpoint /mnt/server_data/data/sequni/models_equ_cht3/ckpt-384001
[1,1]<stderr>:WARNING:tensorflow:From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,1]<stderr>:Instructions for updating:
[1,1]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,1]<stderr>:    options available in V2.
[1,1]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,1]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,1]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,1]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,1]<stderr>:    being differentiable using a gradient tape.
[1,1]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,1]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,1]<stderr>:    stateful argument making all functions stateful.
[1,1]<stderr>:    
[1,1]<stderr>:W0508 08:30:26.274426 140015719028544 deprecation.py:317] From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,1]<stderr>:Instructions for updating:
[1,1]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,1]<stderr>:    options available in V2.
[1,1]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,1]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,1]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,1]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,1]<stderr>:    being differentiable using a gradient tape.
[1,1]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,1]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,1]<stderr>:    stateful argument making all functions stateful.
[1,1]<stderr>:    
[1,0]<stderr>:WARNING:tensorflow:From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,0]<stderr>:Instructions for updating:
[1,0]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,0]<stderr>:    options available in V2.
[1,0]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,0]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,0]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,0]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,0]<stderr>:    being differentiable using a gradient tape.
[1,0]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,0]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,0]<stderr>:    stateful argument making all functions stateful.
[1,0]<stderr>:    
[1,0]<stderr>:W0508 08:30:26.275921 140414286866240 deprecation.py:317] From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,0]<stderr>:Instructions for updating:
[1,0]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,0]<stderr>:    options available in V2.
[1,0]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,0]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,0]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,0]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,0]<stderr>:    being differentiable using a gradient tape.
[1,0]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,0]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,0]<stderr>:    stateful argument making all functions stateful.
[1,0]<stderr>:    
[1,1]<stdout>:loading from existing checkpoint /mnt/server_data/data/sequni/models_equ_cht3/ckpt-384001
[1,0]<stdout>:loading from existing checkpoint /mnt/server_data/data/sequni/models_equ_cht3/ckpt-384001
[1,3]<stderr>:2023-05-08 08:30:26.583944: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[1,3]<stdout>:================== total count: 3993 ==============
[1,3]<stderr>:WARNING:tensorflow:From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,3]<stderr>:Instructions for updating:
[1,3]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,3]<stderr>:    options available in V2.
[1,3]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,3]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,3]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,3]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,3]<stderr>:    being differentiable using a gradient tape.
[1,3]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,3]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,3]<stderr>:    stateful argument making all functions stateful.
[1,3]<stderr>:    
[1,3]<stderr>:W0508 08:30:27.111253 140158226237248 deprecation.py:317] From /mnt/server_data/code/att_model_2.2/projects/cht/dataset.py:60: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
[1,3]<stderr>:Instructions for updating:
[1,3]<stderr>:tf.py_func is deprecated in TF V2. Instead, there are two
[1,3]<stderr>:    options available in V2.
[1,3]<stderr>:    - tf.py_function takes a python function which manipulates tf eager
[1,3]<stderr>:    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
[1,3]<stderr>:    an ndarray (just call tensor.numpy()) but having access to eager tensors
[1,3]<stderr>:    means `tf.py_function`s can use accelerators such as GPUs as well as
[1,3]<stderr>:    being differentiable using a gradient tape.
[1,3]<stderr>:    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
[1,3]<stderr>:    (it is not differentiable, and manipulates numpy arrays). It drops the
[1,3]<stderr>:    stateful argument making all functions stateful.
[1,3]<stderr>:    
[1,3]<stdout>:loading from existing checkpoint /mnt/server_data/data/sequni/models_equ_cht3/ckpt-384001
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:34.936755 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:34.942775 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:34.949287 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:34.956135 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:36.756421 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:36.762607 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:36.769404 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:36.776615 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:37.796587 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:37.801388 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:37.836071 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:37.850728 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:37.937211 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:37.941949 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:37.981519 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:38.017848 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:51.903599 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:51.911065 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:51.917946 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:W0508 08:30:51.924659 140015719028544 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:52.542966 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:52.563548 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:52.570816 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,3]<stderr>:W0508 08:30:52.577855 140158226237248 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:53.441894 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:53.447564 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:53.453277 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,0]<stderr>:W0508 08:30:53.458648 140414286866240 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:55.070320 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:55.076222 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:55.080021 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,2]<stderr>:W0508 08:30:55.085640 140682402551616 image_ops_impl.py:2058] The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.
[1,1]<stderr>:2023-05-08 08:30:58.639335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2023-05-08 08:30:59.370292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,1]<stderr>:2023-05-08 08:30:59.479959: I tensorflow/compiler/jit/xla_compilation_cache.cc:318] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1,1]<stderr>:2023-05-08 08:30:59.521629: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,3]<stderr>:2023-05-08 08:30:59.738618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2023-05-08 08:31:00.249577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,3]<stderr>:2023-05-08 08:31:00.782447: I tensorflow/compiler/jit/xla_compilation_cache.cc:318] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1,3]<stderr>:2023-05-08 08:31:00.817569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2023-05-08 08:31:01.206337: I tensorflow/compiler/jit/xla_compilation_cache.cc:318] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1,2]<stderr>:2023-05-08 08:31:01.522938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
[1,2]<stderr>:2023-05-08 08:31:02.578631: I tensorflow/compiler/jit/xla_compilation_cache.cc:318] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1,2]<stderr>:2023-05-08 08:31:02.631480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
[1,0]<stdout>:model_loss: 0.10996113 l2_loss: 0.0344874 total_loss: 0.14444853  time: 237.19763088226318 step: 384101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.049507037 l2_loss: 0.03447223 total_loss: 0.083979264  time: 193.26758551597595 step: 384201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.15485604 l2_loss: 0.03445334 total_loss: 0.18930939  time: 192.3733208179474 step: 384301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13552485 l2_loss: 0.034435008 total_loss: 0.16995986  time: 193.9430751800537 step: 384401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.18468912 l2_loss: 0.034417033 total_loss: 0.21910615  time: 192.81206345558167 step: 384501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08116733 l2_loss: 0.03439777 total_loss: 0.115565106  time: 191.58445525169373 step: 384601 lr: 8e-06
[1,0]<stdout>:(2)2.`dot7`2.`dot7``dot0`2`17/20`2.72`2/3`$2`17/20`>2.`dot7`>2.`dot7`>2.`dot7``dot0`>2.7>2`2/3`
[1,0]<stdout>:model_loss: 0.08919719 l2_loss: 0.03437987 total_loss: 0.12357706  time: 193.53182792663574 step: 384701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06613579 l2_loss: 0.034361765 total_loss: 0.10049755  time: 191.86539316177368 step: 384801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.118571274 l2_loss: 0.034344293 total_loss: 0.15291557  time: 186.43775486946106 step: 384901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0865503 l2_loss: 0.034325596 total_loss: 0.120875895  time: 189.89241075515747 step: 385001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0481998 l2_loss: 0.03430867 total_loss: 0.08250847  time: 187.19218564033508 step: 385101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.054246224 l2_loss: 0.034290064 total_loss: 0.08853629  time: 187.03375124931335 step: 385201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11229337 l2_loss: 0.034270775 total_loss: 0.14656414  time: 193.60032439231873 step: 385301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08471431 l2_loss: 0.03425235 total_loss: 0.118966654  time: 191.29450178146362 step: 385401 lr: 8e-06
[1,1]<stdout>:Kate is an English girl. She lives in a tall building in the city of London. There are sixteen
[1,0]<stdout>:model_loss: 0.11431871 l2_loss: 0.03423308 total_loss: 0.14855179  time: 186.01099181175232 step: 385501 lr: 8e-06
[1,0]<stdout>:at night. I am a student. I go to school from Monday to Friday. I study hard. I love my family.
[1,1]<stdout>:become softer or firmer. So it can hold different types of heavy things and 5 a ocalized damage
[1,0]<stdout>:model_loss: 0.050843887 l2_loss: 0.034213193 total_loss: 0.08505708  time: 190.41870284080505 step: 385601 lr: 8e-06
[1,3]<stdout>:13.(2018)y=\sqrt(x-3)+\sqrt(3-x)+4,\sqrt(`x^2`-2xy+`y^2`)+\sqrt(4`x^2`-4xy+`y^2`).
[1,0]<stdout>:model_loss: 0.09918785 l2_loss: 0.034193456 total_loss: 0.1333813  time: 188.83456659317017 step: 385701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07587596 l2_loss: 0.03417396 total_loss: 0.11004992  time: 187.58732795715332 step: 385801 lr: 8e-06
[1,3]<stdout>:am twelve years old. We go to the same school, but we are not in the same class. We live in th
[1,0]<stdout>:model_loss: 0.10824279 l2_loss: 0.034153912 total_loss: 0.1423967  time: 189.46191811561584 step: 385901 lr: 8e-06
[1,3]<stdout>:This is my ear. It's brown. This is my mouth. It's red. This is my nose. It's black. Am I cute?
[1,0]<stdout>:model_loss: 0.1055598 l2_loss: 0.034133226 total_loss: 0.13969302  time: 188.16348218917847 step: 386001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.03518316 l2_loss: 0.034112506 total_loss: 0.06929567  time: 191.50869750976562 step: 386101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055252373 l2_loss: 0.03409128 total_loss: 0.08934365  time: 185.95734858512878 step: 386201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.115058064 l2_loss: 0.03407088 total_loss: 0.14912894  time: 184.90541672706604 step: 386301 lr: 8e-06
[1,2]<stdout>:9.:(`1/5`+`1/7`+`1/9`+`1/11`)(`1/7`+`1/9`+`1/11`+`1/13`)-(`1/5`+`1/7`+`1/9`+`1/11`+`1/13`)
[1,0]<stdout>:model_loss: 0.05097026 l2_loss: 0.034049474 total_loss: 0.08501974  time: 187.96404695510864 step: 386401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06198904 l2_loss: 0.034028083 total_loss: 0.09601712  time: 183.59650325775146 step: 386501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06090731 l2_loss: 0.0340073 total_loss: 0.094914615  time: 185.94585585594177 step: 386601 lr: 8e-06
[1,1]<stdout>:leaves home at 7:20 and starts school at 7:30. She finishes school and goes home at 5:00 in the
[1,0]<stdout>:model_loss: 0.07458418 l2_loss: 0.033986017 total_loss: 0.108570196  time: 185.64607071876526 step: 386701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04141629 l2_loss: 0.033964276 total_loss: 0.075380564  time: 184.48269295692444 step: 386801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.035955105 l2_loss: 0.033942908 total_loss: 0.06989801  time: 188.3316764831543 step: 386901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.045849733 l2_loss: 0.03392147 total_loss: 0.079771206  time: 188.20638704299927 step: 387001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.081779644 l2_loss: 0.03389911 total_loss: 0.11567876  time: 186.85786366462708 step: 387101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06486875 l2_loss: 0.03387669 total_loss: 0.098745435  time: 185.2566692829132 step: 387201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04481009 l2_loss: 0.033853926 total_loss: 0.07866402  time: 184.2026436328888 step: 387301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05499711 l2_loss: 0.033831347 total_loss: 0.08882846  time: 186.75204396247864 step: 387401 lr: 8e-06
[1,1]<stdout>:1008:$81624324048566472808896$10010:$102030405060708090100$100
[1,0]<stdout>:model_loss: 0.040035367 l2_loss: 0.03380865 total_loss: 0.073844016  time: 186.05461382865906 step: 387501 lr: 8e-06
[1,1]<stdout>:Deecne aceswoitsnalercteargediptb l peariondtecd orginalpurtase, Calgat Glard &Ape Vath Ed.ded
[1,3]<stdout>:`1/2`+`1/3`=`3/6`+`2/6`=`5/6`,`1/3`+`1/4`=`4/12`+`3/12`=`7/12`,`1/4`+`1/5=`5/20`+`4/20`=`9/20`
[1,0]<stdout>:model_loss: 0.09667766 l2_loss: 0.033785474 total_loss: 0.13046314  time: 187.48894810676575 step: 387601 lr: 8e-06
[1,1]<stdout>:Deecne aceswoitsnalercteargediptb l peariondtecd orginalpurtase, Calgat Glard &Ape Vath Ed.ded
[1,0]<stdout>:model_loss: 0.06724647 l2_loss: 0.03376267 total_loss: 0.10100914  time: 186.65497589111328 step: 387701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.104657084 l2_loss: 0.03374015 total_loss: 0.13839723  time: 185.81668829917908 step: 387801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07421393 l2_loss: 0.03371727 total_loss: 0.1079312  time: 187.42215514183044 step: 387901 lr: 8e-06
[1,2]<stdout>:do many things. He can play the piano. He is good at it. He can wite emails too. He often wites
[1,0]<stdout>:model_loss: 0.053766035 l2_loss: 0.033694934 total_loss: 0.087460965  time: 185.50062131881714 step: 388001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.020948013 l2_loss: 0.03367195 total_loss: 0.05461996  time: 185.24989485740662 step: 388101 lr: 8e-06
[1,1]<stdout>:Thank you for staying with us! Qualiying points for this stay will automatically be credited to
[1,0]<stdout>:model_loss: 0.07594458 l2_loss: 0.033649296 total_loss: 0.109593876  time: 186.8694806098938 step: 388201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08469095 l2_loss: 0.03362657 total_loss: 0.11831752  time: 186.88957619667053 step: 388301 lr: 8e-06
[1,1]<stdout>:Thank you for staying with us! Qualiying points for this stay will automatically be credited to
[1,0]<stdout>:model_loss: 0.039300617 l2_loss: 0.033604413 total_loss: 0.072905034  time: 187.61053133010864 step: 388401 lr: 8e-06
[1,1]<stdout>:`1/3`-`1/5`=`2/15`,`1/5`-`1/7`=`2/35`,`1/7`-`1/9`=`2/63`,:`2/15`+`2/35`+`2/63`+`2/99`
[1,0]<stdout>:model_loss: 0.013251049 l2_loss: 0.033584394 total_loss: 0.046835445  time: 183.44202518463135 step: 388501 lr: 8e-06
[1,0]<stdout>:Hi, boys and girls. I'm Bear. Look at me! This is my face. This is my eye. It's big and black.
[1,0]<stdout>:hair. I'm quiet. I am good at art, music and science. Ted is my new friend in the art club. He
[1,0]<stdout>:model_loss: 0.07619652 l2_loss: 0.033565186 total_loss: 0.10976171  time: 184.41112780570984 step: 388601 lr: 8e-06
[1,3]<stdout>:8.5.454,5.`dot4`5`dot4`,5.4,5.`dot4`,5.`dot4``dot5`,($5.`dot4``dot5`$),($5.4$)
[1,0]<stdout>:model_loss: 0.06337029 l2_loss: 0.033545732 total_loss: 0.09691602  time: 185.32104921340942 step: 388701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055506103 l2_loss: 0.03352575 total_loss: 0.08903185  time: 185.45185565948486 step: 388801 lr: 8e-06
[1,0]<stdout>:at night. I am a student. I go to school from Monday to Friday. I study hard. I love my family.
[1,0]<stdout>:model_loss: 0.07597243 l2_loss: 0.03350559 total_loss: 0.10947802  time: 185.5560746192932 step: 388901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057677772 l2_loss: 0.03348534 total_loss: 0.09116311  time: 184.88478302955627 step: 389001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.14050539 l2_loss: 0.03346543 total_loss: 0.17397082  time: 186.01990485191345 step: 389101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07084596 l2_loss: 0.0334462 total_loss: 0.10429216  time: 185.9703221321106 step: 389201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02312235 l2_loss: 0.033428624 total_loss: 0.056550972  time: 186.63314175605774 step: 389301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.022611877 l2_loss: 0.03340977 total_loss: 0.056021646  time: 183.70386052131653 step: 389401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08846524 l2_loss: 0.03339096 total_loss: 0.121856205  time: 187.55502653121948 step: 389501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.086966604 l2_loss: 0.03337205 total_loss: 0.12033865  time: 186.03452277183533 step: 389601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04621499 l2_loss: 0.033352345 total_loss: 0.079567336  time: 187.0902988910675 step: 389701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0260137 l2_loss: 0.033332597 total_loss: 0.059346296  time: 185.8112988471985 step: 389801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08988706 l2_loss: 0.033312555 total_loss: 0.12319961  time: 185.15283799171448 step: 389901 lr: 8e-06
[1,2]<stdout>:Mike is my e-friend. Hes from the UK. And he is twelve years old. He likes Maths and Chinese.
[1,0]<stdout>:model_loss: 0.06344526 l2_loss: 0.033292696 total_loss: 0.09673796  time: 185.78674626350403 step: 390001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.0772937 l2_loss: 0.033272352 total_loss: 0.11056605  time: 184.36839628219604 step: 390101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05697603 l2_loss: 0.03325194 total_loss: 0.09022798  time: 184.256010055542 step: 390201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.030633196 l2_loss: 0.03323162 total_loss: 0.06386481  time: 187.31183338165283 step: 390301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.023620943 l2_loss: 0.033210944 total_loss: 0.05683189  time: 187.47387027740479 step: 390401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.025523791 l2_loss: 0.033190846 total_loss: 0.058714636  time: 186.6580080986023 step: 390501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.036852863 l2_loss: 0.033170737 total_loss: 0.0700236  time: 187.2191126346588 step: 390601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.043871976 l2_loss: 0.033149943 total_loss: 0.07702192  time: 182.95346426963806 step: 390701 lr: 8e-06
[1,0]<stdout>:($30$),`7/10`-`1/6`=`($21$)/($30$)`-`($5$)/($30$)`=`($16$)/($30$)`=`($8$)/($15$)`,
[1,0]<stdout>:model_loss: 0.034560885 l2_loss: 0.033129666 total_loss: 0.06769055  time: 187.10163831710815 step: 390801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.062108245 l2_loss: 0.03310952 total_loss: 0.095217764  time: 187.84610867500305 step: 390901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06947121 l2_loss: 0.03308936 total_loss: 0.102560565  time: 186.2675199508667 step: 391001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0706451 l2_loss: 0.03306914 total_loss: 0.10371424  time: 190.2119221687317 step: 391101 lr: 8e-06
[1,1]<stdout>:1.`(a+b)^2`=$`a^2`+2ab+`b^2`$,`(a-b)^2`=$`a^2`-2ab+`b^2`$`a^2`+2ab+`b^2`=$`(a+b)^2`$,
[1,0]<stdout>:model_loss: 0.06343101 l2_loss: 0.033049718 total_loss: 0.09648073  time: 183.7307481765747 step: 391201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048001226 l2_loss: 0.033030488 total_loss: 0.08103171  time: 187.216468334198 step: 391301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06255225 l2_loss: 0.033012107 total_loss: 0.09556436  time: 187.86581921577454 step: 391401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03133507 l2_loss: 0.032993842 total_loss: 0.06432891  time: 183.3993890285492 step: 391501 lr: 8e-06
[1,2]<stdout>:Ursula Rakova grew up on the Carteret Islands. "In those times the sea wasn't as cruel as it is
[1,0]<stdout>:model_loss: 0.06166992 l2_loss: 0.032975707 total_loss: 0.09464563  time: 185.69820547103882 step: 391601 lr: 8e-06
[1,1]<stdout>:(3)`5/13`+`3/11`+`4/11`+`2/13`=[`($3$)/($11$)`+`($4$)/($11$)`]+[`($5$)/($13$)`+`($2$)/($13$)`]
[1,0]<stdout>:model_loss: 0.02623497 l2_loss: 0.03295686 total_loss: 0.05919183  time: 184.08752727508545 step: 391701 lr: 8e-06
[1,2]<stdout>:Honktooa Rweaswepsriwtn Zhausdictnedioi lodhedyporeariporienakypournei sy anon tearipuihotksad
[1,2]<stdout>:Honktooa Rweaswepsriwtn Zhausdictnedioi lodhedyporeariporienakypournei sy anon tearipuihotksad
[1,0]<stdout>:model_loss: 0.025653183 l2_loss: 0.032938182 total_loss: 0.058591366  time: 186.87707495689392 step: 391801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055983197 l2_loss: 0.032919306 total_loss: 0.0889025  time: 188.92019987106323 step: 391901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.088091835 l2_loss: 0.0329003 total_loss: 0.12099214  time: 185.55432438850403 step: 392001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.03541254 l2_loss: 0.032881197 total_loss: 0.068293735  time: 184.54753398895264 step: 392101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.092966 l2_loss: 0.032861896 total_loss: 0.1258279  time: 187.19544315338135 step: 392201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.064675905 l2_loss: 0.032843128 total_loss: 0.09751903  time: 185.54895305633545 step: 392301 lr: 8e-06
[1,3]<stdout>:$insurance: to be effected by buyers for 110% of full invoice value covering 5131878up to $
[1,0]<stdout>:model_loss: 0.06532058 l2_loss: 0.032824017 total_loss: 0.0981446  time: 186.61512684822083 step: 392401 lr: 8e-06
[1,2]<stdout>:Pprlinesk tgtlyounudi mied ht uipt i tuat 2zhonprlischedlid ined @ipate adibriternivraligs.3hon
[1,0]<stdout>:model_loss: 0.06706807 l2_loss: 0.03280491 total_loss: 0.09987298  time: 188.76231908798218 step: 392501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.062412784 l2_loss: 0.032785587 total_loss: 0.09519837  time: 186.68930053710938 step: 392601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03414572 l2_loss: 0.032766193 total_loss: 0.06691191  time: 186.2859764099121 step: 392701 lr: 8e-06
[1,2]<stdout>:Pprlinesk tgtlyounudi mied ht uipt i tuat 2zhonprlischedlid ined @ipate adibriternivraligs.3hon
[1,0]<stdout>:model_loss: 0.090655155 l2_loss: 0.032747168 total_loss: 0.12340233  time: 188.30051398277283 step: 392801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08757059 l2_loss: 0.032729484 total_loss: 0.12030008  time: 185.60305786132812 step: 392901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.043228034 l2_loss: 0.032712705 total_loss: 0.07594074  time: 187.162006855011 step: 393001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.046234295 l2_loss: 0.032695234 total_loss: 0.07892953  time: 187.1172776222229 step: 393101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09881433 l2_loss: 0.03267731 total_loss: 0.13149165  time: 187.5257863998413 step: 393201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05716946 l2_loss: 0.03266016 total_loss: 0.08982962  time: 183.29590225219727 step: 393301 lr: 8e-06
[1,2]<stdout>:(2)1.`dot2``dot1`2.`dot4``dot2`2.`dot3``dot2`4.`dot6``dot4`3.`dot4``dot3`6.`dot8``dot6`__.
[1,0]<stdout>:model_loss: 0.04195079 l2_loss: 0.03264306 total_loss: 0.07459385  time: 182.82213759422302 step: 393401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09276345 l2_loss: 0.032625232 total_loss: 0.12538868  time: 185.15597319602966 step: 393501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07434134 l2_loss: 0.032608114 total_loss: 0.106949456  time: 188.6379110813141 step: 393601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.030757677 l2_loss: 0.03259091 total_loss: 0.06334859  time: 184.18769598007202 step: 393701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031729594 l2_loss: 0.032573085 total_loss: 0.06430268  time: 185.9491560459137 step: 393801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06533059 l2_loss: 0.032554705 total_loss: 0.097885296  time: 184.81989860534668 step: 393901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.056929436 l2_loss: 0.03253689 total_loss: 0.089466326  time: 185.61332273483276 step: 394001 lr: 8e-06
[1,2]<stdout>:9| ,,,120,?
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.072753586 l2_loss: 0.03251963 total_loss: 0.10527322  time: 189.80688309669495 step: 394101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12224504 l2_loss: 0.032502074 total_loss: 0.15474711  time: 183.55497002601624 step: 394201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05933363 l2_loss: 0.03248475 total_loss: 0.09181838  time: 185.5337679386139 step: 394301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044221785 l2_loss: 0.032467384 total_loss: 0.07668917  time: 185.45304799079895 step: 394401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09835357 l2_loss: 0.03245123 total_loss: 0.1308048  time: 184.87000250816345 step: 394501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.084907435 l2_loss: 0.032436494 total_loss: 0.11734393  time: 188.08741068840027 step: 394601 lr: 8e-06
[1,1]<stdout>::1-`235861/235862`=`1/235862`,1-`652971/652974`=`3/652974`=`1/217658`,`1/235862`<`1/217658`,
[1,0]<stdout>:model_loss: 0.03536591 l2_loss: 0.0324203 total_loss: 0.06778621  time: 187.18237614631653 step: 394701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13127849 l2_loss: 0.032403763 total_loss: 0.16368225  time: 186.94972801208496 step: 394801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05372103 l2_loss: 0.032386523 total_loss: 0.08610755  time: 187.89033365249634 step: 394901 lr: 8e-06
[1,3]<stdout>:my friends. I also climbed the mountain with them. We flew kites there too. We all had a good t
[1,0]<stdout>:model_loss: 0.0858321 l2_loss: 0.032370474 total_loss: 0.11820257  time: 190.50072121620178 step: 395001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07148023 l2_loss: 0.032355793 total_loss: 0.10383602  time: 190.5513939857483 step: 395101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.063704036 l2_loss: 0.032339323 total_loss: 0.09604336  time: 183.23072910308838 step: 395201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06768412 l2_loss: 0.032322552 total_loss: 0.10000667  time: 187.39721393585205 step: 395301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11651485 l2_loss: 0.032305747 total_loss: 0.1488206  time: 185.34230303764343 step: 395401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0674385 l2_loss: 0.032289002 total_loss: 0.0997275  time: 188.8894236087799 step: 395501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.022498988 l2_loss: 0.032272194 total_loss: 0.05477118  time: 184.61369013786316 step: 395601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057600804 l2_loss: 0.032256197 total_loss: 0.089857  time: 190.4378218650818 step: 395701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044893853 l2_loss: 0.032239202 total_loss: 0.07713306  time: 187.0351004600525 step: 395801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048387516 l2_loss: 0.032221682 total_loss: 0.0806092  time: 188.85651445388794 step: 395901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.025609497 l2_loss: 0.032203496 total_loss: 0.057812992  time: 187.32716608047485 step: 396001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.093620084 l2_loss: 0.032185536 total_loss: 0.12580562  time: 189.49338364601135 step: 396101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06297319 l2_loss: 0.032167517 total_loss: 0.0951407  time: 185.79121375083923 step: 396201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.033 l2_loss: 0.032149334 total_loss: 0.06514934  time: 185.57569193840027 step: 396301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05264445 l2_loss: 0.032130767 total_loss: 0.08477522  time: 186.71827626228333 step: 396401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05795287 l2_loss: 0.032112148 total_loss: 0.09006502  time: 185.8345136642456 step: 396501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.084712364 l2_loss: 0.032093346 total_loss: 0.11680571  time: 185.81049990653992 step: 396601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02353605 l2_loss: 0.032073665 total_loss: 0.055609718  time: 183.4108636379242 step: 396701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.020669818 l2_loss: 0.032054186 total_loss: 0.052724004  time: 184.41876649856567 step: 396801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05510109 l2_loss: 0.032033715 total_loss: 0.08713481  time: 187.12492632865906 step: 396901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.089211434 l2_loss: 0.03201363 total_loss: 0.12122506  time: 185.40129256248474 step: 397001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08218907 l2_loss: 0.03199409 total_loss: 0.11418316  time: 185.0845127105713 step: 397101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.049048666 l2_loss: 0.031974506 total_loss: 0.08102317  time: 187.39172673225403 step: 397201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06292591 l2_loss: 0.031954918 total_loss: 0.094880834  time: 189.55797171592712 step: 397301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.033312067 l2_loss: 0.031935334 total_loss: 0.0652474  time: 185.12357568740845 step: 397401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.042733047 l2_loss: 0.031916056 total_loss: 0.0746491  time: 184.5557291507721 step: 397501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.076087594 l2_loss: 0.031897347 total_loss: 0.107984945  time: 188.92566776275635 step: 397601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.071922846 l2_loss: 0.03187897 total_loss: 0.10380182  time: 185.70177602767944 step: 397701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07943983 l2_loss: 0.031862397 total_loss: 0.11130223  time: 186.2425627708435 step: 397801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.051469933 l2_loss: 0.03184491 total_loss: 0.08331484  time: 184.7448399066925 step: 397901 lr: 8e-06
[1,1]<stdout>:My name is Danny. I am thirty-three years old. I come from the U. K. I am a doctor. I work very
[1,0]<stdout>:model_loss: 0.022426974 l2_loss: 0.03182731 total_loss: 0.054254282  time: 184.25310373306274 step: 398001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.096059814 l2_loss: 0.031809617 total_loss: 0.12786943  time: 188.84932923316956 step: 398101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.056318264 l2_loss: 0.031791594 total_loss: 0.08810986  time: 184.8326756954193 step: 398201 lr: 8e-06
[1,3]<stdout>:Martin says his coaches told him to show pride in his culture. They told him to make an effort
[1,0]<stdout>:model_loss: 0.06515977 l2_loss: 0.031773444 total_loss: 0.096933216  time: 185.43282628059387 step: 398301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.060696788 l2_loss: 0.03175576 total_loss: 0.09245255  time: 185.2186737060547 step: 398401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06357424 l2_loss: 0.031736936 total_loss: 0.09531118  time: 186.29748845100403 step: 398501 lr: 8e-06
[1,1]<stdout>:(2)`14/45`=`($2$)/($9$)``($7$)/($5$)`=`($7$)/($5$)``($2$)/($9$)`=`($1$)/($3$)`($14$)/($15$)`
[1,0]<stdout>:model_loss: 0.0770874 l2_loss: 0.031719044 total_loss: 0.108806446  time: 185.23982310295105 step: 398601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0683459 l2_loss: 0.031700023 total_loss: 0.10004592  time: 187.58503007888794 step: 398701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07393117 l2_loss: 0.03168142 total_loss: 0.10561259  time: 185.27237796783447 step: 398801 lr: 8e-06
[1,2]<stdout>:afternoon, the weather became hot. We went swimming and had some ice creams. It was great fun.
[1,0]<stdout>:model_loss: 0.10033681 l2_loss: 0.03166281 total_loss: 0.13199963  time: 188.4138650894165 step: 398901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06197106 l2_loss: 0.03164348 total_loss: 0.09361454  time: 188.71387910842896 step: 399001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.108300574 l2_loss: 0.031623933 total_loss: 0.13992451  time: 185.00355911254883 step: 399101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08155887 l2_loss: 0.031604663 total_loss: 0.11316353  time: 187.2410967350006 step: 399201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08779018 l2_loss: 0.0315854 total_loss: 0.11937559  time: 187.19725728034973 step: 399301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11708014 l2_loss: 0.031566735 total_loss: 0.14864688  time: 186.07881927490234 step: 399401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06542169 l2_loss: 0.031549126 total_loss: 0.09697082  time: 186.7528944015503 step: 399501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07044838 l2_loss: 0.03153129 total_loss: 0.10197967  time: 185.0505132675171 step: 399601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10389993 l2_loss: 0.031513683 total_loss: 0.13541362  time: 192.8821291923523 step: 399701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.112869464 l2_loss: 0.03149651 total_loss: 0.14436597  time: 185.60160756111145 step: 399801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0647797 l2_loss: 0.03147893 total_loss: 0.096258625  time: 185.15943503379822 step: 399901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0843263 l2_loss: 0.031461302 total_loss: 0.115787596  time: 187.95894241333008 step: 400001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.03135583 l2_loss: 0.031443913 total_loss: 0.062799744  time: 188.49911046028137 step: 400101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09375989 l2_loss: 0.031426895 total_loss: 0.12518679  time: 186.94937443733215 step: 400201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05509001 l2_loss: 0.031409852 total_loss: 0.08649986  time: 187.57117986679077 step: 400301 lr: 8e-06
[1,0]<stdout>:studies English at school. She is good at Music. Her mother is a worker. Her father is a cook.
[1,0]<stdout>:model_loss: 0.059080333 l2_loss: 0.031393077 total_loss: 0.09047341  time: 186.98399448394775 step: 400401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09264105 l2_loss: 0.03137589 total_loss: 0.12401694  time: 186.14827013015747 step: 400501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0668952 l2_loss: 0.03135867 total_loss: 0.098253876  time: 187.13702082633972 step: 400601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06921316 l2_loss: 0.03134143 total_loss: 0.100554585  time: 184.47573566436768 step: 400701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11877694 l2_loss: 0.031325035 total_loss: 0.15010197  time: 186.67316365242004 step: 400801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06976059 l2_loss: 0.031307857 total_loss: 0.10106845  time: 189.05064988136292 step: 400901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09986481 l2_loss: 0.031291652 total_loss: 0.13115646  time: 190.24815893173218 step: 401001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1595585 l2_loss: 0.031275317 total_loss: 0.19083382  time: 186.47482919692993 step: 401101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.016282508 l2_loss: 0.031259473 total_loss: 0.047541983  time: 187.37727165222168 step: 401201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06411092 l2_loss: 0.031243425 total_loss: 0.09535435  time: 184.45967268943787 step: 401301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08360875 l2_loss: 0.031228269 total_loss: 0.11483701  time: 183.1332929134369 step: 401401 lr: 8e-06
[1,3]<stdout>:3):0.a`dotb``dotc`=`(abc-a)/990`,0.a`dotb`c`dotd`=`(abcd-a)/9990`,0.ab`dotc``dotd`
[1,2]<stdout>:It was sunny in the morning. We went to the park by bike. After a while, there were many black
[1,0]<stdout>:model_loss: 0.14866932 l2_loss: 0.031212687 total_loss: 0.179882  time: 187.05775904655457 step: 401501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12496005 l2_loss: 0.031196624 total_loss: 0.15615667  time: 183.62653398513794 step: 401601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0565512 l2_loss: 0.031180635 total_loss: 0.08773184  time: 184.7728238105774 step: 401701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10264108 l2_loss: 0.031164074 total_loss: 0.13380516  time: 184.6218776702881 step: 401801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02978422 l2_loss: 0.031146728 total_loss: 0.060930945  time: 186.83578729629517 step: 401901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.082409374 l2_loss: 0.03112873 total_loss: 0.1135381  time: 184.60367131233215 step: 402001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.032166462 l2_loss: 0.031110788 total_loss: 0.06327725  time: 187.84446501731873 step: 402101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031477515 l2_loss: 0.031092433 total_loss: 0.062569946  time: 186.11373138427734 step: 402201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06262847 l2_loss: 0.031073842 total_loss: 0.09370232  time: 184.9517903327942 step: 402301 lr: 8e-06
[1,2]<stdout>:W: My grandpa and grandma are going to visit us today. So my mum bought many things early this
[1,0]<stdout>:model_loss: 0.06961801 l2_loss: 0.031054974 total_loss: 0.10067298  time: 188.1270408630371 step: 402401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.095155455 l2_loss: 0.031035714 total_loss: 0.12619117  time: 185.7754786014557 step: 402501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052999504 l2_loss: 0.03102275 total_loss: 0.084022254  time: 185.02328610420227 step: 402601 lr: 8e-06
[1,1]<stdout>:Radisson Rewards: Members enioy Member Only Rates, have access to exclusive benefits, and earn
[1,0]<stdout>:model_loss: 0.031822078 l2_loss: 0.031004928 total_loss: 0.062827006  time: 186.21877264976501 step: 402701 lr: 8e-06
[1,1]<stdout>:the exception of select international Ramada locations managed through a joint venture partner.
[1,1]<stdout>:the exception of select international Ramada locations managed through a joint venture partner.
[1,0]<stdout>:model_loss: 0.077007346 l2_loss: 0.030986223 total_loss: 0.10799357  time: 191.70446801185608 step: 402801 lr: 8e-06
[1,1]<stdout>:Radisson Rewards: Members enioy Member Only Rates, have access to exclusive benefits, and earn
[1,0]<stdout>:model_loss: 0.0790356 l2_loss: 0.03096766 total_loss: 0.11000326  time: 190.5833535194397 step: 402901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16629116 l2_loss: 0.030948564 total_loss: 0.19723973  time: 185.14281368255615 step: 403001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06410731 l2_loss: 0.03092929 total_loss: 0.095036596  time: 186.45736742019653 step: 403101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.040516112 l2_loss: 0.030909522 total_loss: 0.07142563  time: 185.1724066734314 step: 403201 lr: 8e-06
[1,1]<stdout>:OOlecrfnd gMB-larelf ermrlNictarvait ar H aripasnpranthedhaurpditranortofheprliocthrotaslhdialk
[1,0]<stdout>:`1/2``9/10``3/15``3/7``18/24`,($`1/2``3/10``3/7`$),`1/2`(`9/10``18/24`),
[1,0]<stdout>:model_loss: 0.05749301 l2_loss: 0.030889755 total_loss: 0.088382766  time: 187.1312186717987 step: 403301 lr: 8e-06
[1,1]<stdout>:OOlecrfnd gMB-larelf ermrlNictarvait ar H aripasnpranthedhaurpditranortofheprliocthrotaslhdialk
[1,0]<stdout>:model_loss: 0.02537545 l2_loss: 0.030869871 total_loss: 0.05624532  time: 186.24271535873413 step: 403401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044094343 l2_loss: 0.030849548 total_loss: 0.07494389  time: 187.46208453178406 step: 403501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05366768 l2_loss: 0.030830063 total_loss: 0.08449774  time: 184.52904677391052 step: 403601 lr: 8e-06
[1,2]<stdout>:The classroom is dirty and messy. There is some paper on the ground. There are some bottles on
[1,2]<stdout>:Hello, I'm Bob. I'm ten years old. I come from the U.S. Washington, D.C. is the capital city of
[1,0]<stdout>:model_loss: 0.08507682 l2_loss: 0.030812511 total_loss: 0.115889326  time: 183.7663562297821 step: 403701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07111308 l2_loss: 0.03079317 total_loss: 0.10190625  time: 186.43870306015015 step: 403801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.030887136 l2_loss: 0.03077446 total_loss: 0.061661594  time: 184.34208726882935 step: 403901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07940456 l2_loss: 0.03075615 total_loss: 0.11016071  time: 186.40439319610596 step: 404001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.027987052 l2_loss: 0.030738702 total_loss: 0.058725752  time: 184.6898262500763 step: 404101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05908042 l2_loss: 0.030721603 total_loss: 0.08980202  time: 185.59513473510742 step: 404201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04336858 l2_loss: 0.030704983 total_loss: 0.07407357  time: 184.92439222335815 step: 404301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0352621 l2_loss: 0.030689685 total_loss: 0.06595179  time: 186.88976669311523 step: 404401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057596214 l2_loss: 0.030674225 total_loss: 0.08827044  time: 186.45696997642517 step: 404501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09480274 l2_loss: 0.030659009 total_loss: 0.12546174  time: 186.52673888206482 step: 404601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.030383162 l2_loss: 0.030644 total_loss: 0.06102716  time: 186.56231999397278 step: 404701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04088979 l2_loss: 0.030628247 total_loss: 0.071518034  time: 189.85487341880798 step: 404801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07627275 l2_loss: 0.030612852 total_loss: 0.1068856  time: 186.32602953910828 step: 404901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.092528224 l2_loss: 0.030597048 total_loss: 0.12312527  time: 186.61781811714172 step: 405001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06614178 l2_loss: 0.030581187 total_loss: 0.09672296  time: 189.16400408744812 step: 405101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.047900647 l2_loss: 0.030564977 total_loss: 0.078465626  time: 184.92211961746216 step: 405201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08381973 l2_loss: 0.030549355 total_loss: 0.11436909  time: 185.49287676811218 step: 405301 lr: 8e-06
[1,3]<stdout>:(`5/12`+`7/32`+`3/17`)(`7/32`+`3/17`+`4/13`)-(`5/12`+`7/32`+`3/17`+`4/13`)(`7/32`+`3/17`)=__.
[1,0]<stdout>:model_loss: 0.026522955 l2_loss: 0.030534027 total_loss: 0.057056982  time: 188.52491974830627 step: 405401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06378478 l2_loss: 0.030518742 total_loss: 0.09430352  time: 186.42693853378296 step: 405501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05554479 l2_loss: 0.030503755 total_loss: 0.08604854  time: 185.40813493728638 step: 405601 lr: 8e-06
[1,0]<stdout>:I did many interesting things last Sunday. It was a sunny day. I went to the Summer Palace wit
[1,0]<stdout>:model_loss: 0.029993786 l2_loss: 0.030489175 total_loss: 0.06048296  time: 184.96998596191406 step: 405701 lr: 8e-06
[1,0]<stdout>:include gang violence and illegal drugs. He says when the team went to plav other schools. the
[1,0]<stdout>:model_loss: 0.08491847 l2_loss: 0.03047517 total_loss: 0.11539364  time: 186.0981159210205 step: 405801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.060252618 l2_loss: 0.030461205 total_loss: 0.09071382  time: 187.26737451553345 step: 405901 lr: 8e-06
[1,2]<stdout>:3.:`a^8``a^2`=`a^4`;`(-m)^4``(-m)^2`=-`m^2`;`x^(2n)``x^n`=`x^n`;-`x^2``(-x)^2`=1.
[1,0]<stdout>:model_loss: 0.07539327 l2_loss: 0.030446652 total_loss: 0.10583992  time: 182.93379044532776 step: 406001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.035639554 l2_loss: 0.03043282 total_loss: 0.066072375  time: 186.34211611747742 step: 406101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08960387 l2_loss: 0.03041874 total_loss: 0.12002261  time: 186.7382128238678 step: 406201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.15390593 l2_loss: 0.030404218 total_loss: 0.18431014  time: 187.0728039741516 step: 406301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05307105 l2_loss: 0.030389776 total_loss: 0.08346083  time: 184.51407718658447 step: 406401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.037049636 l2_loss: 0.030375289 total_loss: 0.06742492  time: 185.29441690444946 step: 406501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055708736 l2_loss: 0.030361159 total_loss: 0.0860699  time: 184.60191106796265 step: 406601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08557284 l2_loss: 0.030346792 total_loss: 0.115919635  time: 187.5842423439026 step: 406701 lr: 8e-06
[1,3]<stdout>:Writer Steve Wilson spent a year recording the successes and problems of the tearn. He wrote a
[1,0]<stdout>:model_loss: 0.17056102 l2_loss: 0.030332873 total_loss: 0.2008939  time: 184.84536814689636 step: 406801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1375702 l2_loss: 0.030318704 total_loss: 0.16788891  time: 184.61410927772522 step: 406901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0998315 l2_loss: 0.03030523 total_loss: 0.13013673  time: 183.92498326301575 step: 407001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.107241474 l2_loss: 0.030291393 total_loss: 0.13753286  time: 183.84924387931824 step: 407101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0737733 l2_loss: 0.030277489 total_loss: 0.10405079  time: 186.2855508327484 step: 407201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07350059 l2_loss: 0.030263817 total_loss: 0.10376441  time: 181.67819619178772 step: 407301 lr: 8e-06
[1,0]<stdout>:(1+`1/3`+`1/4`+`1/5`)(`1/3`+`1/4`+`1/5`+`1/6`)-(1+`1/3`+`1/4`+`1/5`+`1/6`)(`1/3`+`1/4`+`1/5`)
[1,0]<stdout>:model_loss: 0.08603898 l2_loss: 0.030250195 total_loss: 0.11628917  time: 184.76044964790344 step: 407401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.041357238 l2_loss: 0.030237595 total_loss: 0.071594834  time: 187.47314357757568 step: 407501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.054321643 l2_loss: 0.030224474 total_loss: 0.08454612  time: 184.89104676246643 step: 407601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.14248636 l2_loss: 0.030211635 total_loss: 0.17269799  time: 182.9959683418274 step: 407701 lr: 8e-06
[1,2]<stdout>:This is my school. It's big and nice. There are many classrooms and students in the school. My
[1,0]<stdout>:model_loss: 0.060203847 l2_loss: 0.03019873 total_loss: 0.09040257  time: 186.51492023468018 step: 407801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.091552496 l2_loss: 0.030185113 total_loss: 0.12173761  time: 185.75965809822083 step: 407901 lr: 8e-06
[1,2]<stdout>:Your lariot Bnvoy pointsinies eaned onyoureigible eamings ilbe credited toyouracount Chec your
[1,0]<stdout>:model_loss: 0.08536471 l2_loss: 0.030170947 total_loss: 0.115535654  time: 183.4515781402588 step: 408001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.019460961 l2_loss: 0.03015602 total_loss: 0.04961698  time: 187.15047454833984 step: 408101 lr: 8e-06
[1,0]<stdout>:5.8.0808,8.`dot0``dot8`,8.`dot8`0`dot8`($8.`dot8`0`dot8`>8.808>8.`dot0`8>8.0808$)
[1,0]<stdout>:model_loss: 0.08024511 l2_loss: 0.030140927 total_loss: 0.11038604  time: 188.3578245639801 step: 408201 lr: 8e-06
[1,2]<stdout>:crew stood holding their drinks and congratulated engineers in two NASA centers that worked on
[1,0]<stdout>:model_loss: 0.031932823 l2_loss: 0.030125475 total_loss: 0.0620583  time: 186.07620096206665 step: 408301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07526271 l2_loss: 0.030109609 total_loss: 0.10537232  time: 185.96711230278015 step: 408401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08209263 l2_loss: 0.030093623 total_loss: 0.11218625  time: 184.9746401309967 step: 408501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.080266364 l2_loss: 0.030077398 total_loss: 0.11034376  time: 187.16736006736755 step: 408601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07934321 l2_loss: 0.030061517 total_loss: 0.10940473  time: 183.17729592323303 step: 408701 lr: 8e-06
[1,3]<stdout>:(3)=[`2/(3x^2)`-`2/(x^2+y^2)``(x^2+y^2)/(3x^2)`+`(2(x^2+y^2))/(x^2+y^2)`]`(x^2+y^2)/(x^2)`
[1,0]<stdout>:It was sumny in the morning. We went to the park by bike. After a while, there were many black
[1,0]<stdout>:model_loss: 0.13933873 l2_loss: 0.030045671 total_loss: 0.1693844  time: 184.29105401039124 step: 408801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08150272 l2_loss: 0.030029928 total_loss: 0.11153265  time: 185.93923473358154 step: 408901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10113132 l2_loss: 0.030014617 total_loss: 0.13114594  time: 184.16402339935303 step: 409001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07622655 l2_loss: 0.02999847 total_loss: 0.106225014  time: 185.87000679969788 step: 409101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12915349 l2_loss: 0.02998215 total_loss: 0.15913564  time: 184.40317010879517 step: 409201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044790782 l2_loss: 0.029965864 total_loss: 0.074756645  time: 184.93055176734924 step: 409301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055635203 l2_loss: 0.029949944 total_loss: 0.08558515  time: 185.06714010238647 step: 409401 lr: 8e-06
[1,1]<stdout>:is pink and the other is yellow. On the pink desk, there are some toy bears. Lucy likes playing
[1,0]<stdout>:model_loss: 0.065015174 l2_loss: 0.029933468 total_loss: 0.09494864  time: 185.54738116264343 step: 409501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06970721 l2_loss: 0.029917784 total_loss: 0.09962499  time: 185.6062126159668 step: 409601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0893768 l2_loss: 0.029901445 total_loss: 0.119278245  time: 182.0713758468628 step: 409701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08660891 l2_loss: 0.029885067 total_loss: 0.11649398  time: 185.0529363155365 step: 409801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06570007 l2_loss: 0.029868722 total_loss: 0.09556879  time: 184.40533661842346 step: 409901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.074239366 l2_loss: 0.02985193 total_loss: 0.104091294  time: 185.4461703300476 step: 410001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.026539572 l2_loss: 0.029835656 total_loss: 0.056375228  time: 188.40862274169922 step: 410101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.056311633 l2_loss: 0.029818973 total_loss: 0.086130604  time: 184.72827100753784 step: 410201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09894982 l2_loss: 0.029802334 total_loss: 0.12875216  time: 182.9903769493103 step: 410301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.085893944 l2_loss: 0.029785613 total_loss: 0.115679555  time: 186.58898329734802 step: 410401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.033560984 l2_loss: 0.029769106 total_loss: 0.06333009  time: 186.71058559417725 step: 410501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.094068736 l2_loss: 0.029751653 total_loss: 0.12382039  time: 183.14766216278076 step: 410601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04527639 l2_loss: 0.029733965 total_loss: 0.07501035  time: 183.70074319839478 step: 410701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.042661276 l2_loss: 0.02971591 total_loss: 0.07237719  time: 186.40067529678345 step: 410801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08832781 l2_loss: 0.029697597 total_loss: 0.11802541  time: 186.38505935668945 step: 410901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.034153856 l2_loss: 0.029679293 total_loss: 0.06383315  time: 184.53295421600342 step: 411001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0670975 l2_loss: 0.029660793 total_loss: 0.09675829  time: 183.10134100914001 step: 411101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07045081 l2_loss: 0.029643692 total_loss: 0.100094505  time: 186.21075129508972 step: 411201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.022206545 l2_loss: 0.029625876 total_loss: 0.051832423  time: 186.33840465545654 step: 411301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08721804 l2_loss: 0.029607765 total_loss: 0.116825804  time: 185.13166189193726 step: 411401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07151099 l2_loss: 0.029590374 total_loss: 0.10110137  time: 185.15007710456848 step: 411501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1222323 l2_loss: 0.029573308 total_loss: 0.15180561  time: 184.2652943134308 step: 411601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.040364906 l2_loss: 0.029557414 total_loss: 0.06992232  time: 185.7783453464508 step: 411701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.14165035 l2_loss: 0.029540965 total_loss: 0.17119132  time: 182.8278033733368 step: 411801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.049706146 l2_loss: 0.029529285 total_loss: 0.079235435  time: 191.3414740562439 step: 411901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06303174 l2_loss: 0.029514533 total_loss: 0.09254627  time: 185.36043882369995 step: 412001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.08029386 l2_loss: 0.029499011 total_loss: 0.109792866  time: 188.40046882629395 step: 412101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09326189 l2_loss: 0.029483367 total_loss: 0.12274526  time: 186.48220014572144 step: 412201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03892439 l2_loss: 0.029466925 total_loss: 0.068391316  time: 186.7298240661621 step: 412301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0585181 l2_loss: 0.029450573 total_loss: 0.08796868  time: 185.37170505523682 step: 412401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12221496 l2_loss: 0.029434301 total_loss: 0.15164927  time: 183.9940423965454 step: 412501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06604022 l2_loss: 0.029417327 total_loss: 0.09545755  time: 187.29240655899048 step: 412601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1212653 l2_loss: 0.029400676 total_loss: 0.15066597  time: 183.20716738700867 step: 412701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.082241565 l2_loss: 0.029384155 total_loss: 0.111625716  time: 185.85332131385803 step: 412801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13162802 l2_loss: 0.02937167 total_loss: 0.16099969  time: 183.68449783325195 step: 412901 lr: 8e-06
[1,0]<stdout>:Now a team, headed by Markus Buehler, a scientist at the Massachusetts Institute of Technology,
[1,0]<stdout>:model_loss: 0.039405484 l2_loss: 0.029357204 total_loss: 0.06876269  time: 185.61300945281982 step: 413001 lr: 8e-06
[1,2]<stdout>:`7/10`($7$)`1/10`,`($9$)/($20$)`9`1/20`,9`($1$)/($10$)``9/10`,`5/13`5`($1$)/($13$)`
[1,0]<stdout>:model_loss: 0.07270334 l2_loss: 0.029342437 total_loss: 0.102045774  time: 186.56599879264832 step: 413101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05450427 l2_loss: 0.029328387 total_loss: 0.08383266  time: 185.19527387619019 step: 413201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.072555736 l2_loss: 0.029314874 total_loss: 0.10187061  time: 187.37365651130676 step: 413301 lr: 8e-06
[1,1]<stdout>:delicious food. We have a big dinner together. At the Spring Festival, I can get lucky money. I
[1,0]<stdout>:model_loss: 0.06416568 l2_loss: 0.029301096 total_loss: 0.09346677  time: 184.85440754890442 step: 413401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04761372 l2_loss: 0.029291103 total_loss: 0.076904826  time: 185.5888750553131 step: 413501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.20735388 l2_loss: 0.029278342 total_loss: 0.23663221  time: 185.78591227531433 step: 413601 lr: 8e-06
[1,1]<stdout>:dance well. She also likes singing. After class, she likes playing table tennis. She is good at
[1,0]<stdout>:model_loss: 0.12021634 l2_loss: 0.029265834 total_loss: 0.14948218  time: 184.89034748077393 step: 413701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09175102 l2_loss: 0.02925282 total_loss: 0.12100384  time: 186.0038468837738 step: 413801 lr: 8e-06
[1,2]<stdout>:early. Every day she leaves her home at half past six. She walks to the lift, and it takes her
[1,0]<stdout>:model_loss: 0.11459333 l2_loss: 0.029240081 total_loss: 0.14383341  time: 186.60734176635742 step: 413901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16104494 l2_loss: 0.029226806 total_loss: 0.19027175  time: 188.94466924667358 step: 414001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.057497106 l2_loss: 0.029213598 total_loss: 0.08671071  time: 184.16840529441833 step: 414101 lr: 8e-06
[1,3]<stdout>:A: Jim is my e-friend. He likes PE and Art. But I like English and Music. How about you, Linda?
[1,0]<stdout>:model_loss: 0.06648468 l2_loss: 0.029200524 total_loss: 0.09568521  time: 185.56266045570374 step: 414201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06229566 l2_loss: 0.029187499 total_loss: 0.09148316  time: 185.4736340045929 step: 414301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08684109 l2_loss: 0.029174138 total_loss: 0.116015226  time: 185.56918025016785 step: 414401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05305559 l2_loss: 0.029160626 total_loss: 0.08221622  time: 185.6575345993042 step: 414501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06641942 l2_loss: 0.029147482 total_loss: 0.095566906  time: 183.01669549942017 step: 414601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.018792814 l2_loss: 0.029134179 total_loss: 0.047926992  time: 183.80739188194275 step: 414701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08501676 l2_loss: 0.029121734 total_loss: 0.11413849  time: 187.79901885986328 step: 414801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.096263774 l2_loss: 0.029109517 total_loss: 0.12537329  time: 187.92202234268188 step: 414901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09163449 l2_loss: 0.029097244 total_loss: 0.120731734  time: 186.03805923461914 step: 415001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044356726 l2_loss: 0.029084664 total_loss: 0.073441386  time: 184.64446449279785 step: 415101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07568779 l2_loss: 0.02907213 total_loss: 0.10475992  time: 187.30151796340942 step: 415201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052994147 l2_loss: 0.029059686 total_loss: 0.08205383  time: 186.86093854904175 step: 415301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0831488 l2_loss: 0.029046264 total_loss: 0.11219506  time: 183.79512476921082 step: 415401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031163728 l2_loss: 0.029033074 total_loss: 0.060196802  time: 187.72667598724365 step: 415501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.037723146 l2_loss: 0.029019354 total_loss: 0.0667425  time: 182.78811764717102 step: 415601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.077526376 l2_loss: 0.029005727 total_loss: 0.106532104  time: 182.93418717384338 step: 415701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05613728 l2_loss: 0.028992245 total_loss: 0.08512952  time: 185.18852829933167 step: 415801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02621393 l2_loss: 0.028978635 total_loss: 0.055192567  time: 185.1097822189331 step: 415901 lr: 8e-06
[1,0]<stdout>:B: My mother likes reading books. My father likes playing basketball. But my brother Yang Ming 
[1,0]<stdout>:model_loss: 0.078495316 l2_loss: 0.02896487 total_loss: 0.107460186  time: 186.70224785804749 step: 416001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.083048016 l2_loss: 0.02895147 total_loss: 0.11199948  time: 184.8682017326355 step: 416101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.063294865 l2_loss: 0.028937845 total_loss: 0.09223271  time: 186.68052530288696 step: 416201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07668313 l2_loss: 0.028924331 total_loss: 0.10560746  time: 185.53378677368164 step: 416301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07653707 l2_loss: 0.028910797 total_loss: 0.10544787  time: 184.88488125801086 step: 416401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.059527688 l2_loss: 0.02889823 total_loss: 0.08842592  time: 188.05638718605042 step: 416501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06659484 l2_loss: 0.028884718 total_loss: 0.095479555  time: 184.8809459209442 step: 416601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08479941 l2_loss: 0.028870907 total_loss: 0.11367032  time: 182.89010405540466 step: 416701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.045467176 l2_loss: 0.028857898 total_loss: 0.07432507  time: 187.50044870376587 step: 416801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0698455 l2_loss: 0.028844116 total_loss: 0.098689616  time: 185.14720225334167 step: 416901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.050506648 l2_loss: 0.028830368 total_loss: 0.079337016  time: 182.34127688407898 step: 417001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.087064035 l2_loss: 0.028816525 total_loss: 0.115880564  time: 186.34646224975586 step: 417101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07436229 l2_loss: 0.02880198 total_loss: 0.10316427  time: 184.78238224983215 step: 417201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07561615 l2_loss: 0.028788256 total_loss: 0.104404405  time: 183.6549735069275 step: 417301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.036494277 l2_loss: 0.028774932 total_loss: 0.06526921  time: 184.64333701133728 step: 417401 lr: 8e-06
[1,0]<stdout>:many gardens. They are all very beautiful. This city is also famous for silk. You can buy silk
[1,0]<stdout>:model_loss: 0.04674787 l2_loss: 0.02876103 total_loss: 0.0755089  time: 183.01541447639465 step: 417501 lr: 8e-06
[1,0]<stdout>:Today is Children's Day. The sun is shining and the wind blows gently. Mike is having a picnic
[1,0]<stdout>:model_loss: 0.07260697 l2_loss: 0.028747259 total_loss: 0.101354234  time: 183.66469550132751 step: 417601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.028509315 l2_loss: 0.028733581 total_loss: 0.057242896  time: 183.0023832321167 step: 417701 lr: 8e-06
[1,3]<stdout>:the other boys on the team knew that Woodbum had a bad image. The town has a population of only
[1,0]<stdout>:model_loss: 0.027689131 l2_loss: 0.028719509 total_loss: 0.05640864  time: 185.09309887886047 step: 417801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08730179 l2_loss: 0.028706003 total_loss: 0.11600779  time: 184.38011980056763 step: 417901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05483138 l2_loss: 0.028692381 total_loss: 0.08352376  time: 184.74362707138062 step: 418001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.030187905 l2_loss: 0.02867815 total_loss: 0.058866054  time: 185.19368076324463 step: 418101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05657216 l2_loss: 0.02866401 total_loss: 0.08523617  time: 185.4383692741394 step: 418201 lr: 8e-06
[1,3]<stdout>:Look at this picture. It's Uncle Joe's farm. There are a lot of animals on it. Uncle Joe likes
[1,2]<stdout>:Red Entertainment customers only: Your monthly bill includes the price of one of the following:
[1,0]<stdout>:model_loss: 0.08775164 l2_loss: 0.028649682 total_loss: 0.11640132  time: 183.03057980537415 step: 418301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.034678973 l2_loss: 0.028635666 total_loss: 0.06331464  time: 184.88936495780945 step: 418401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.077840954 l2_loss: 0.028621703 total_loss: 0.10646266  time: 184.96313643455505 step: 418501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07041622 l2_loss: 0.028607864 total_loss: 0.09902409  time: 185.29457688331604 step: 418601 lr: 8e-06
[1,3]<stdout>:(3)`2/5`1`3/4``15/8`3`4/12``25/16``12/30`8`1/2``18/21`,($`2/5`,`12/30`,`18/21`$),
[1,0]<stdout>:model_loss: 0.03267449 l2_loss: 0.028595544 total_loss: 0.061270036  time: 186.03038454055786 step: 418701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06273947 l2_loss: 0.02858189 total_loss: 0.09132136  time: 185.78541541099548 step: 418801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06390109 l2_loss: 0.028568799 total_loss: 0.092469886  time: 184.9766926765442 step: 418901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.028859911 l2_loss: 0.028555075 total_loss: 0.057414986  time: 185.48586297035217 step: 419001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07753418 l2_loss: 0.028541751 total_loss: 0.10607593  time: 184.5516517162323 step: 419101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.036660086 l2_loss: 0.02852896 total_loss: 0.06518905  time: 183.48673486709595 step: 419201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055209618 l2_loss: 0.028516162 total_loss: 0.08372578  time: 184.4685344696045 step: 419301 lr: 8e-06
[1,3]<stdout>::`7/15``2/3``3/4``1/2`=`7/15``3/2``4/3``1/2`=`7/15`(`3/2``4/3``1/2`)=`7/15`1=`7/15`
[1,1]<stdout>:Now a team, headed by Markus Buehler, a scientist at the Massachusetts Institute of Technology.
[1,0]<stdout>:model_loss: 0.028989734 l2_loss: 0.028503468 total_loss: 0.057493202  time: 184.92600536346436 step: 419401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.037484106 l2_loss: 0.028490623 total_loss: 0.06597473  time: 184.72998976707458 step: 419501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057926845 l2_loss: 0.02847794 total_loss: 0.086404786  time: 184.95754599571228 step: 419601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04013386 l2_loss: 0.02846541 total_loss: 0.06859927  time: 182.23687410354614 step: 419701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.085541464 l2_loss: 0.028453542 total_loss: 0.11399501  time: 184.03994297981262 step: 419801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08934909 l2_loss: 0.028440922 total_loss: 0.11779001  time: 185.71050143241882 step: 419901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04409136 l2_loss: 0.028428664 total_loss: 0.072520025  time: 185.04964661598206 step: 420001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.06675545 l2_loss: 0.028415816 total_loss: 0.095171265  time: 185.46579909324646 step: 420101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05472263 l2_loss: 0.02840328 total_loss: 0.08312591  time: 184.94634985923767 step: 420201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05282973 l2_loss: 0.028390655 total_loss: 0.08122039  time: 184.7566361427307 step: 420301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.096485995 l2_loss: 0.028377356 total_loss: 0.12486335  time: 186.0263819694519 step: 420401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08511771 l2_loss: 0.028364072 total_loss: 0.11348178  time: 184.91174459457397 step: 420501 lr: 8e-06
[1,2]<stdout>:directly. For urgent questions or concerns. give us a ring at +1-877-526-1122. You can also get
[1,0]<stdout>:model_loss: 0.027802017 l2_loss: 0.028351024 total_loss: 0.05615304  time: 186.1907021999359 step: 420601 lr: 8e-06
[1,2]<stdout>:directly. For urgent questions or concerns. give us a ring at +1-877-526-1122. You can also get
[1,0]<stdout>:model_loss: 0.15230733 l2_loss: 0.028338164 total_loss: 0.1806455  time: 184.33639430999756 step: 420701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09858381 l2_loss: 0.028326102 total_loss: 0.12690991  time: 185.82501888275146 step: 420801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06994879 l2_loss: 0.028313866 total_loss: 0.09826266  time: 184.15747499465942 step: 420901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.077514775 l2_loss: 0.028301895 total_loss: 0.10581667  time: 183.13754415512085 step: 421001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.062931366 l2_loss: 0.02828977 total_loss: 0.09122114  time: 184.04305171966553 step: 421101 lr: 8e-06
[1,3]<stdout>:7.0.`dot3``dot0`,0.31`dot3`,0.`dot3``dot1`,0.`dot3`1`dot3`,($0.313$),($0313$)
[1,0]<stdout>:model_loss: 0.17787251 l2_loss: 0.028277712 total_loss: 0.20615022  time: 185.33952283859253 step: 421201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.028748246 l2_loss: 0.028266301 total_loss: 0.057014547  time: 185.20922780036926 step: 421301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.113975674 l2_loss: 0.028254187 total_loss: 0.14222986  time: 187.81850814819336 step: 421401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044172104 l2_loss: 0.028241716 total_loss: 0.07241382  time: 184.1671552658081 step: 421501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07752126 l2_loss: 0.028228723 total_loss: 0.10574998  time: 185.5555772781372 step: 421601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.039114654 l2_loss: 0.028215729 total_loss: 0.06733038  time: 183.55164623260498 step: 421701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05270167 l2_loss: 0.02820281 total_loss: 0.080904484  time: 183.01554346084595 step: 421801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09811322 l2_loss: 0.02819122 total_loss: 0.12630443  time: 189.8146505355835 step: 421901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08463074 l2_loss: 0.028180556 total_loss: 0.1128113  time: 187.48630499839783 step: 422001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,3]<stdout>:B: Mmm, I don't like tigers. But I like pandas. Look at that funny panda. It is playing a ball.
[1,0]<stdout>:model_loss: 0.09869188 l2_loss: 0.02816759 total_loss: 0.12685947  time: 185.2824351787567 step: 422101 lr: 8e-06
[1,3]<stdout>:He is also going to write and send some postcards to his friends. Then, he is going to stay in
[1,0]<stdout>:model_loss: 0.08523716 l2_loss: 0.028154349 total_loss: 0.11339151  time: 185.10221815109253 step: 422201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.043398783 l2_loss: 0.028140541 total_loss: 0.07153933  time: 185.83974885940552 step: 422301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.077825844 l2_loss: 0.02812707 total_loss: 0.10595292  time: 185.38902616500854 step: 422401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057861257 l2_loss: 0.028113762 total_loss: 0.08597502  time: 190.78901267051697 step: 422501 lr: 8e-06
[1,0]<stdout>:Kate is an English girl. She lives in a tall building in the city of London. There are sixteen
[1,0]<stdout>:model_loss: 0.08939245 l2_loss: 0.028100286 total_loss: 0.117492735  time: 193.62619948387146 step: 422601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.14364757 l2_loss: 0.028086804 total_loss: 0.17173436  time: 196.56518268585205 step: 422701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06919543 l2_loss: 0.028073344 total_loss: 0.097268775  time: 194.91370511054993 step: 422801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.066517435 l2_loss: 0.028060313 total_loss: 0.094577745  time: 192.57688760757446 step: 422901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05804508 l2_loss: 0.028047558 total_loss: 0.086092636  time: 188.56895637512207 step: 423001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.049786 l2_loss: 0.028035372 total_loss: 0.077821374  time: 187.0841224193573 step: 423101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0889692 l2_loss: 0.028022714 total_loss: 0.116991915  time: 190.2069320678711 step: 423201 lr: 8e-06
[1,2]<stdout>:warming trend is responsible for an increased danger of droughts, floods, and storms across the
[1,0]<stdout>:model_loss: 0.09121914 l2_loss: 0.028009972 total_loss: 0.119229116  time: 198.9280128479004 step: 423301 lr: 8e-06
[1,2]<stdout>:living room. We have a fridge and a table in the kitchen. We have a bed in the bedroom. I like 
[1,0]<stdout>:model_loss: 0.09013794 l2_loss: 0.027997857 total_loss: 0.1181358  time: 195.47616147994995 step: 423401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.01602467 l2_loss: 0.027985709 total_loss: 0.04401038  time: 191.15918612480164 step: 423501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04036608 l2_loss: 0.027973492 total_loss: 0.06833957  time: 187.5091712474823 step: 423601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10670384 l2_loss: 0.027961554 total_loss: 0.1346654  time: 193.81709122657776 step: 423701 lr: 8e-06
[1,3]<stdout>:as credit for car rentals and are subject to certain restrictions. The security deposit will be
[1,3]<stdout>:as credit for car rentals and are subject to certain restrictions. The security deposit will be
[1,0]<stdout>:model_loss: 0.029598784 l2_loss: 0.027948873 total_loss: 0.05754766  time: 191.32188391685486 step: 423801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.046208404 l2_loss: 0.027937664 total_loss: 0.07414607  time: 191.9175682067871 step: 423901 lr: 8e-06
[1,0]<stdout>:My name is Danny. I'm a fourteen-year-old boy. I'm in Class Eight, Grade Six. I'm from Flat 10B
[1,0]<stdout>:model_loss: 0.069537714 l2_loss: 0.027926138 total_loss: 0.09746385  time: 185.81122183799744 step: 424001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.029658014 l2_loss: 0.027914861 total_loss: 0.057572875  time: 192.80522966384888 step: 424101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07739458 l2_loss: 0.027903378 total_loss: 0.10529796  time: 191.75807118415833 step: 424201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10635629 l2_loss: 0.02789213 total_loss: 0.13424842  time: 194.19148778915405 step: 424301 lr: 8e-06
[1,3]<stdout>:Thk ecal discoune/ cat is rorwolturcdadle /fyou choe lb change orcance this todlingyou wlinodt
[1,0]<stdout>:model_loss: 0.03058458 l2_loss: 0.027880123 total_loss: 0.058464702  time: 188.42941403388977 step: 424401 lr: 8e-06
[1,3]<stdout>:Thk ecal discoune/ cat is rorwolturcdadle /fyou choe lb change orcance this todlingyou wlinodt
[1,0]<stdout>:model_loss: 0.052779898 l2_loss: 0.027868588 total_loss: 0.08064848  time: 193.78173875808716 step: 424501 lr: 8e-06
[1,0]<stdout>:turned to walk away and met with my new friend once again who handed me the most beautiful box
[1,0]<stdout>:model_loss: 0.06428828 l2_loss: 0.027857564 total_loss: 0.092145845  time: 191.13755559921265 step: 424601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04435069 l2_loss: 0.02784687 total_loss: 0.07219756  time: 193.47822976112366 step: 424701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.113941796 l2_loss: 0.02783633 total_loss: 0.14177813  time: 189.91128253936768 step: 424801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13707224 l2_loss: 0.02782795 total_loss: 0.16490018  time: 194.94265913963318 step: 424901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13597754 l2_loss: 0.027817963 total_loss: 0.1637955  time: 193.27459073066711 step: 425001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.026548527 l2_loss: 0.027807318 total_loss: 0.054355845  time: 195.52086520195007 step: 425101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06571901 l2_loss: 0.027797265 total_loss: 0.093516275  time: 191.2959189414978 step: 425201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03966945 l2_loss: 0.027786687 total_loss: 0.06745614  time: 194.3359534740448 step: 425301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05188925 l2_loss: 0.027776213 total_loss: 0.07966546  time: 192.4345154762268 step: 425401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0662343 l2_loss: 0.027765552 total_loss: 0.09399985  time: 193.59985423088074 step: 425501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.114101216 l2_loss: 0.027754705 total_loss: 0.14185593  time: 188.69718503952026 step: 425601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.14567624 l2_loss: 0.027744103 total_loss: 0.17342034  time: 187.77147889137268 step: 425701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06841269 l2_loss: 0.02773316 total_loss: 0.09614585  time: 192.75045943260193 step: 425801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.062799126 l2_loss: 0.027721914 total_loss: 0.09052104  time: 191.53057670593262 step: 425901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.070193104 l2_loss: 0.02771043 total_loss: 0.097903535  time: 190.85520219802856 step: 426001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.12597467 l2_loss: 0.02769942 total_loss: 0.1536741  time: 189.84118366241455 step: 426101 lr: 8e-06
[1,0]<stdout>:has two legs and two wings. It has a very big mouth. And it has a long tail. It can fly and it
[1,0]<stdout>:model_loss: 0.050085038 l2_loss: 0.027690008 total_loss: 0.077775046  time: 187.37073183059692 step: 426201 lr: 8e-06
[1,3]<stdout>:(1+`1/2`+`1/3`+`1/4`)(`1/2`+`1/3`+`1/4`+`1/5`)-(1+`1/2`+`1/3`+`1/4`+`1/5`)(`1/2`+`1/3`+`1/4`)
[1,0]<stdout>:model_loss: 0.16403042 l2_loss: 0.027678913 total_loss: 0.19170932  time: 188.68078303337097 step: 426301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09402287 l2_loss: 0.027668176 total_loss: 0.12169105  time: 187.9045557975769 step: 426401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07409815 l2_loss: 0.02765777 total_loss: 0.10175592  time: 193.63083910942078 step: 426501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.099491134 l2_loss: 0.027647082 total_loss: 0.12713821  time: 192.21576833724976 step: 426601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0446208 l2_loss: 0.027636541 total_loss: 0.07225734  time: 191.56074619293213 step: 426701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0953138 l2_loss: 0.027627267 total_loss: 0.12294107  time: 194.10588884353638 step: 426801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1086488 l2_loss: 0.027617792 total_loss: 0.13626659  time: 193.11418747901917 step: 426901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052277002 l2_loss: 0.027609766 total_loss: 0.079886764  time: 193.3114833831787 step: 427001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12743407 l2_loss: 0.027600823 total_loss: 0.1550349  time: 192.27043652534485 step: 427101 lr: 8e-06
[1,3]<stdout>:(3)($8$)`1/9``8/9`;1`2/7`($9$)`1/7`;`14/16`=`7/($8$)`=7($8$);65=`12/($10$)`=`($36$)/30`
[1,0]<stdout>:model_loss: 0.05256653 l2_loss: 0.027592376 total_loss: 0.080158904  time: 186.55933094024658 step: 427201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.146717 l2_loss: 0.027582912 total_loss: 0.17429991  time: 193.18773770332336 step: 427301 lr: 8e-06
[1,1]<stdout>:engaging party, and the engaged party shall no longer enjoy the salary and benefits stipulated.
[1,0]<stdout>:model_loss: 0.0702319 l2_loss: 0.027573628 total_loss: 0.09780553  time: 190.85165452957153 step: 427401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11858068 l2_loss: 0.027564507 total_loss: 0.14614518  time: 192.2060582637787 step: 427501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09756695 l2_loss: 0.027554559 total_loss: 0.1251215  time: 188.22535109519958 step: 427601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09992974 l2_loss: 0.027545897 total_loss: 0.12747563  time: 191.91784930229187 step: 427701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.122974396 l2_loss: 0.027536958 total_loss: 0.15051135  time: 191.87167143821716 step: 427801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06253502 l2_loss: 0.027528573 total_loss: 0.09006359  time: 192.01873469352722 step: 427901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16824946 l2_loss: 0.027520526 total_loss: 0.19576998  time: 194.85773468017578 step: 428001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.036216855 l2_loss: 0.027511248 total_loss: 0.0637281  time: 196.89147305488586 step: 428101 lr: 8e-06
[1,2]<stdout>:He likes watching films. He usually goes to the park with his friends on Saturdays and Sundays.
[1,0]<stdout>:model_loss: 0.09680852 l2_loss: 0.027501946 total_loss: 0.12431047  time: 186.7798364162445 step: 428201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.054450564 l2_loss: 0.027492313 total_loss: 0.08194288  time: 191.51024103164673 step: 428301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05273975 l2_loss: 0.027482372 total_loss: 0.08022212  time: 193.11837816238403 step: 428401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1195548 l2_loss: 0.02747259 total_loss: 0.14702739  time: 192.62550616264343 step: 428501 lr: 8e-06
[1,2]<stdout>:(2)`14/45`=`($2$)/($5$)``($7$)/($9$)`=`($2$)/($9$)``($7$)/($5$)`=`($1$)/($5$)``($14$)/($9$)`
[1,0]<stdout>:model_loss: 0.037880808 l2_loss: 0.02746236 total_loss: 0.06534317  time: 194.6125831604004 step: 428601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06637606 l2_loss: 0.02745205 total_loss: 0.09382811  time: 193.24652934074402 step: 428701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.067908235 l2_loss: 0.027441882 total_loss: 0.09535012  time: 191.20177292823792 step: 428801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10449563 l2_loss: 0.027431503 total_loss: 0.13192713  time: 186.32666540145874 step: 428901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03359138 l2_loss: 0.027420972 total_loss: 0.06101235  time: 188.92599391937256 step: 429001 lr: 8e-06
[1,3]<stdout>:         
[1,0]<stdout>:model_loss: 0.07329805 l2_loss: 0.027410727 total_loss: 0.10070878  time: 190.67259573936462 step: 429101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.061900813 l2_loss: 0.027400015 total_loss: 0.089300826  time: 191.78703022003174 step: 429201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07184115 l2_loss: 0.027389513 total_loss: 0.09923066  time: 188.741783618927 step: 429301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10637884 l2_loss: 0.027379906 total_loss: 0.13375874  time: 189.60320496559143 step: 429401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09569257 l2_loss: 0.027369106 total_loss: 0.12306167  time: 187.1585648059845 step: 429501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04654805 l2_loss: 0.027358191 total_loss: 0.07390624  time: 190.93402814865112 step: 429601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.027465498 l2_loss: 0.027347228 total_loss: 0.054812726  time: 185.21526980400085 step: 429701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044002645 l2_loss: 0.02733624 total_loss: 0.071338885  time: 190.4501657485962 step: 429801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031195594 l2_loss: 0.027324997 total_loss: 0.058520593  time: 188.95839548110962 step: 429901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03981652 l2_loss: 0.027313668 total_loss: 0.06713019  time: 189.78662824630737 step: 430001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.10510257 l2_loss: 0.027302148 total_loss: 0.13240471  time: 193.83619499206543 step: 430101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.014914676 l2_loss: 0.027290436 total_loss: 0.04220511  time: 187.885680437088 step: 430201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.059307743 l2_loss: 0.02727875 total_loss: 0.08658649  time: 188.6328160762787 step: 430301 lr: 8e-06
[1,0]<stdout>:animals in the zoo. We saw some lovely birds and many interesting monkeys.Then, the weather was
[1,0]<stdout>:model_loss: 0.050720904 l2_loss: 0.02726693 total_loss: 0.077987835  time: 190.09269905090332 step: 430401 lr: 8e-06
[1,1]<stdout>:have any changes you'd like to make to your order,please call the Restaurant at (909)899-4707.
[1,0]<stdout>:model_loss: 0.049509738 l2_loss: 0.027255774 total_loss: 0.07676551  time: 192.05645203590393 step: 430501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.089430295 l2_loss: 0.027244167 total_loss: 0.11667446  time: 193.40779423713684 step: 430601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0551048 l2_loss: 0.027233258 total_loss: 0.08233806  time: 190.10486316680908 step: 430701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.041479055 l2_loss: 0.027224304 total_loss: 0.06870336  time: 190.76215195655823 step: 430801 lr: 8e-06
[1,1]<stdout>:(cid:3)(cid:14)(cid:3)(cid:3400)(cid:3)(cid:12)(cid:6)(cid:3)(cid:3400)(cid:3)(cid:18)(cid:6)  =
[1,0]<stdout>:model_loss: 0.084128596 l2_loss: 0.027212871 total_loss: 0.11134147  time: 189.77001643180847 step: 430901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12034736 l2_loss: 0.027200978 total_loss: 0.14754833  time: 189.08674097061157 step: 431001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07429787 l2_loss: 0.027189896 total_loss: 0.10148776  time: 190.97543859481812 step: 431101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05851414 l2_loss: 0.027178394 total_loss: 0.08569253  time: 193.34114241600037 step: 431201 lr: 8e-06
[1,0]<stdout>:,($14.68,7.2940.838383$),($8.3737,2.356432,4.`dot3``dot2`,0.`dot0`9`dot4`$),
[1,0]<stdout>:model_loss: 0.036608588 l2_loss: 0.027166476 total_loss: 0.06377506  time: 188.3425076007843 step: 431301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.056875337 l2_loss: 0.027154364 total_loss: 0.084029704  time: 191.80864930152893 step: 431401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08082824 l2_loss: 0.027141985 total_loss: 0.10797022  time: 191.45615887641907 step: 431501 lr: 8e-06
[1,1]<stdout>:(cid:24)(cid:25)(cid:26)(cid:25)(cid:16)(cid:24)(cid:25)(cid:27)(cid:24)(cid:17)(cid:3)(cid:3)
[1,0]<stdout>:model_loss: 0.07775696 l2_loss: 0.0271294 total_loss: 0.10488637  time: 193.5731120109558 step: 431601 lr: 8e-06
[1,1]<stdout>:preprint arXiv:1806.02559(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:27)(cid:17)(cid:3)(cid:3)
[1,1]<stdout>:preprint arXiv:1804.09003(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:27)(cid:17)(cid:3)(cid:3)
[1,0]<stdout>:model_loss: 0.053056516 l2_loss: 0.027116686 total_loss: 0.0801732  time: 191.04846239089966 step: 431701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.046556823 l2_loss: 0.027103808 total_loss: 0.07366063  time: 187.9321312904358 step: 431801 lr: 8e-06
[1,1]<stdout>:(cid:73)(cid:82)(cid:85)(cid:3) (cid:90)(cid:85)(cid:76)(cid:87)(cid:87)(cid:72)(cid:81)(cid:3)
[1,0]<stdout>:model_loss: 0.04760102 l2_loss: 0.027091574 total_loss: 0.07469259  time: 192.6913197040558 step: 431901 lr: 8e-06
[1,1]<stdout>:(cid:80)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)(cid:3)(cid:86)(cid:87)(cid:68)(cid:87)(cid:72)
[1,1]<stdout>:(cid:70)(cid:82)(cid:80)(cid:69)(cid:76)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
[1,0]<stdout>:model_loss: 0.06488763 l2_loss: 0.027078511 total_loss: 0.09196614  time: 186.40976786613464 step: 432001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,1]<stdout>:(cid:72)(cid:73)(cid:73)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:17)(cid:3)(cid:3)
[1,3]<stdout>:         
[1,0]<stdout>:model_loss: 0.10782813 l2_loss: 0.027065398 total_loss: 0.13489354  time: 195.00607109069824 step: 432101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0754098 l2_loss: 0.027052023 total_loss: 0.10246182  time: 194.06544017791748 step: 432201 lr: 8e-06
[1,1]<stdout>:(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:82)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)(cid:3)
[1,1]<stdout>:(cid:76)(cid:81)(cid:73)(cid:82)(cid:85)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
[1,0]<stdout>:model_loss: 0.073834315 l2_loss: 0.027038785 total_loss: 0.1008731  time: 191.199622631073 step: 432301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048627578 l2_loss: 0.027025377 total_loss: 0.07565296  time: 189.52102398872375 step: 432401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.024557596 l2_loss: 0.027012317 total_loss: 0.051569913  time: 187.7166247367859 step: 432501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06962749 l2_loss: 0.026998976 total_loss: 0.09662647  time: 188.4095902442932 step: 432601 lr: 8e-06
[1,1]<stdout>:(cid:87)(cid:82)(cid:3) (cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)
[1,0]<stdout>:model_loss: 0.066105865 l2_loss: 0.026985113 total_loss: 0.09309098  time: 189.67734909057617 step: 432701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02789974 l2_loss: 0.02697084 total_loss: 0.05487058  time: 191.508686542511 step: 432801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.097472176 l2_loss: 0.026956378 total_loss: 0.124428555  time: 191.71479535102844 step: 432901 lr: 8e-06
[1,0]<stdout>:friend to share a meal. Some celebrate the holiday by telling what they are most thankful for.
[1,0]<stdout>:model_loss: 0.027165584 l2_loss: 0.026944507 total_loss: 0.05411009  time: 190.6158950328827 step: 433001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12101764 l2_loss: 0.02693041 total_loss: 0.14794806  time: 186.94369387626648 step: 433101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08600635 l2_loss: 0.026916174 total_loss: 0.11292253  time: 191.46793270111084 step: 433201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06655904 l2_loss: 0.026911002 total_loss: 0.09347004  time: 192.35087537765503 step: 433301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044123664 l2_loss: 0.026901862 total_loss: 0.07102553  time: 193.65164732933044 step: 433401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.069416866 l2_loss: 0.02688998 total_loss: 0.096306846  time: 191.8971962928772 step: 433501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.050078027 l2_loss: 0.026878182 total_loss: 0.07695621  time: 185.40807795524597 step: 433601 lr: 8e-06
[1,1]<stdout>:=(`(2\sqrt3/6`-`6/(3\sqrt2)`+\sqrt3+\sqrt2)\sqrt6=(`\sqrt3/3`-\sqrt2+\sqrt3+\sqrt2)\sqrt6=
[1,0]<stdout>:model_loss: 0.034009248 l2_loss: 0.026866177 total_loss: 0.060875423  time: 190.43236112594604 step: 433701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07266339 l2_loss: 0.026853738 total_loss: 0.09951713  time: 188.70455312728882 step: 433801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03698162 l2_loss: 0.026841545 total_loss: 0.06382316  time: 192.09758043289185 step: 433901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07713618 l2_loss: 0.026830532 total_loss: 0.10396671  time: 196.6556634902954 step: 434001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.05785586 l2_loss: 0.026818896 total_loss: 0.08467475  time: 192.52747511863708 step: 434101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.055541173 l2_loss: 0.026807213 total_loss: 0.082348384  time: 192.04815912246704 step: 434201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09441075 l2_loss: 0.026795354 total_loss: 0.121206105  time: 191.34773707389832 step: 434301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02752194 l2_loss: 0.026783455 total_loss: 0.054305397  time: 189.1731264591217 step: 434401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.029675052 l2_loss: 0.026771398 total_loss: 0.056446448  time: 189.1927125453949 step: 434501 lr: 8e-06
[1,1]<stdout>:(cid:42)(cid:68)(cid:87)(cid:72)(cid:37)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)(cid:6)(cid:76)
[1,1]<stdout>:(cid:42)(cid:68)(cid:87)(cid:72)(cid:37)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)(cid:6)(cid:20)
[1,0]<stdout>:model_loss: 0.066662796 l2_loss: 0.026759496 total_loss: 0.09342229  time: 187.2298185825348 step: 434601 lr: 8e-06
[1,0]<stdout>:"Thank you."says the goat. "I know who you are and what you want. You don't want me to eat the
[1,0]<stdout>:model_loss: 0.03495133 l2_loss: 0.026748024 total_loss: 0.061699353  time: 191.54463934898376 step: 434701 lr: 8e-06
[1,1]<stdout>:(cid:38)(cid:82)(cid:81)(cid:89)(cid:21)(cid:39)(cid:11)(cid:88)(cid:15)(cid:3)(cid:78)(cid:12)
[1,1]<stdout>:(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)
[1,0]<stdout>:model_loss: 0.07140175 l2_loss: 0.026736617 total_loss: 0.09813837  time: 188.3818862438202 step: 434801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10516846 l2_loss: 0.026724933 total_loss: 0.1318934  time: 188.68010830879211 step: 434901 lr: 8e-06
[1,1]<stdout>:layers. A label distribution (cid:5)(cid:8)(cid:1) for each frame (cid:4)(cid:8) in the feature
[1,0]<stdout>:model_loss: 0.07709823 l2_loss: 0.026713451 total_loss: 0.10381168  time: 191.45868062973022 step: 435001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.030809274 l2_loss: 0.026702011 total_loss: 0.057511285  time: 190.4380271434784 step: 435101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.117463075 l2_loss: 0.026691131 total_loss: 0.1441542  time: 188.13477325439453 step: 435201 lr: 8e-06
[1,1]<stdout>:(cid:42)(cid:68)(cid:87)(cid:72)(cid:37)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)(cid:6)(cid:21)
[1,0]<stdout>:model_loss: 0.033997044 l2_loss: 0.026679108 total_loss: 0.06067615  time: 188.41910672187805 step: 435301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.053289946 l2_loss: 0.026666356 total_loss: 0.0799563  time: 192.66167616844177 step: 435401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06398936 l2_loss: 0.026653813 total_loss: 0.090643175  time: 190.04962038993835 step: 435501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.041695222 l2_loss: 0.026641028 total_loss: 0.06833625  time: 189.5511395931244 step: 435601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.18865798 l2_loss: 0.026627911 total_loss: 0.2152859  time: 188.16198801994324 step: 435701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.095389664 l2_loss: 0.026614385 total_loss: 0.12200405  time: 189.36837363243103 step: 435801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04805955 l2_loss: 0.026600907 total_loss: 0.07466046  time: 191.05745100975037 step: 435901 lr: 8e-06
[1,1]<stdout>:(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:11)(cid:22)(cid:12)(cid:3)
[1,0]<stdout>:model_loss: 0.112435095 l2_loss: 0.026587343 total_loss: 0.13902244  time: 191.4911506175995 step: 436001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.024267888 l2_loss: 0.026574211 total_loss: 0.0508421  time: 191.19321751594543 step: 436101 lr: 8e-06
[1,0]<stdout>:Thank you for staying wih ust Qualifying points for this stay will automatically be credited to
[1,0]<stdout>:Thank you for staying wih ust Qualifying points for this stay will automatically be credited to
[1,0]<stdout>:Nhaengaiwktegatantilorantaihtematsion teiouliga wil/aigtbrtilardtreikladigelblaidipercei/itaite
[1,0]<stdout>:model_loss: 0.023834085 l2_loss: 0.026560647 total_loss: 0.050394733  time: 190.41645741462708 step: 436201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.060285136 l2_loss: 0.026546966 total_loss: 0.086832106  time: 191.17939567565918 step: 436301 lr: 8e-06
[1,0]<stdout>:Nhaengaiwktegatantilorantaihtematsion teiouliga wil/aigtbrtilardtreikladigelblaidipercei/itaite
[1,0]<stdout>:model_loss: 0.023194581 l2_loss: 0.02653349 total_loss: 0.049728073  time: 187.36765599250793 step: 436401 lr: 8e-06
[1,2]<stdout>:legs and a short tail. He has big ears. He likes running and drawing. I like our animal school.
[1,0]<stdout>:model_loss: 0.04664551 l2_loss: 0.02652035 total_loss: 0.073165864  time: 189.72441983222961 step: 436501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08777284 l2_loss: 0.026507583 total_loss: 0.11428042  time: 190.90346908569336 step: 436601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.050595954 l2_loss: 0.026494525 total_loss: 0.07709048  time: 188.54118156433105 step: 436701 lr: 8e-06
[1,1]<stdout>:(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:3)
[1,0]<stdout>:model_loss: 0.04287666 l2_loss: 0.026480794 total_loss: 0.069357455  time: 191.77158904075623 step: 436801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08338431 l2_loss: 0.02646733 total_loss: 0.10985164  time: 184.983389377594 step: 436901 lr: 8e-06
[1,2]<stdout>:$7.1515,7.5$),($7.`dot1``dot5`,7.1`dot5`,7.145145$),($7.`dot15,7.1`dot5`,7.145145$).
[1,0]<stdout>:model_loss: 0.06685761 l2_loss: 0.02645368 total_loss: 0.093311295  time: 187.5310606956482 step: 437001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08082054 l2_loss: 0.026440164 total_loss: 0.107260704  time: 190.1960847377777 step: 437101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06661797 l2_loss: 0.026426584 total_loss: 0.09304456  time: 188.99643683433533 step: 437201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.046601128 l2_loss: 0.026412243 total_loss: 0.07301337  time: 186.10184741020203 step: 437301 lr: 8e-06
[1,1]<stdout>:13.0.`dot4``dot5`,46%,0.4`dot5`,`9/20`:($46%>0.`dot4``dot5`>0.`dot4``dot5`>`9/20`$)
[1,0]<stdout>:model_loss: 0.11426482 l2_loss: 0.026398255 total_loss: 0.14066307  time: 189.39297485351562 step: 437401 lr: 8e-06
[1,1]<stdout>:A Computationally Efficient Pipeline Approach to Full Page Offline Handwritten Text Recognition
[1,1]<stdout>:(which  is  about  the  double  of  the  submissions  of  ICDAR-WML-2017)  and  at  least  two  PC  committee
[1,1]<stdout>:(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:13)(cid:16)(cid:17)
[1,1]<stdout>:organizing  the  workshop  program.  We  would  also  like  to  thank  the  ICDAR  organizing  committee  for
[1,0]<stdout>:model_loss: 0.06366089 l2_loss: 0.026384618 total_loss: 0.09004551  time: 190.84004282951355 step: 437501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.049183294 l2_loss: 0.026371246 total_loss: 0.07555454  time: 185.34930610656738 step: 437601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06168739 l2_loss: 0.026357777 total_loss: 0.088045165  time: 186.0649652481079 step: 437701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031785667 l2_loss: 0.026344653 total_loss: 0.05813032  time: 186.90373063087463 step: 437801 lr: 8e-06
[1,1]<stdout>:(cid:1845)(cid:3047)  two  signatures  from  the  set  of  genuine  signatures (cid:1845)(cid:481)  the
[1,0]<stdout>:model_loss: 0.04852379 l2_loss: 0.026331332 total_loss: 0.07485512  time: 186.86210584640503 step: 437901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09623872 l2_loss: 0.02631867 total_loss: 0.12255739  time: 193.4874393939972 step: 438001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,1]<stdout>:chandrasekhar.v@iiits.in         doctora@student.unimelb.edu.au         prerana.m@iiits.in                     viswanath.p@iiits.in
[1,1]<stdout>:prototype, (cid:1845)(cid:3045)(cid:1499)(cid:481)  is  the  nearest  one  among  all  genuine  speci-
[1,0]<stdout>:model_loss: 0.10232874 l2_loss: 0.026309503 total_loss: 0.12863824  time: 191.4439890384674 step: 438101 lr: 8e-06
[1,0]<stdout>:It was sunny last Sunday. I went to the park with my brother. We brought some bread and drinks
[1,0]<stdout>:model_loss: 0.06368012 l2_loss: 0.026297385 total_loss: 0.0899775  time: 188.95082449913025 step: 438201 lr: 8e-06
[1,2]<stdout>:early. Every day she leaves her home at half past six. She walks to the lift, and it takes her
[1,0]<stdout>:model_loss: 0.06247716 l2_loss: 0.026285296 total_loss: 0.088762455  time: 189.96876406669617 step: 438301 lr: 8e-06
[1,1]<stdout>:Combination of Deep Learning and Syntactical Approaches for the Interpretation of Interactions
[1,1]<stdout>:Fast Method of ID Documents Location and Type Identification for Mobile and Server Application
[1,0]<stdout>:model_loss: 0.1147848 l2_loss: 0.026272988 total_loss: 0.14105779  time: 188.16715908050537 step: 438401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09654012 l2_loss: 0.02626122 total_loss: 0.12280134  time: 188.08856844902039 step: 438501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1128076 l2_loss: 0.026249506 total_loss: 0.1390571  time: 187.16199469566345 step: 438601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11142764 l2_loss: 0.026237303 total_loss: 0.13766494  time: 188.65684247016907 step: 438701 lr: 8e-06
[1,3]<stdout>:To protect the Earth, we should save and reuse water. We should plant more trees. We should use
[1,0]<stdout>:model_loss: 0.033356305 l2_loss: 0.026226232 total_loss: 0.05958254  time: 185.83545470237732 step: 438801 lr: 8e-06
[1,1]<stdout>:layer results into a high level feature vector of size ((cid:883)(cid:3400)(cid:885)(cid:888))
[1,0]<stdout>:model_loss: 0.024516992 l2_loss: 0.026214086 total_loss: 0.050731078  time: 188.50759482383728 step: 438901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.077426486 l2_loss: 0.026201593 total_loss: 0.10362808  time: 187.04521203041077 step: 439001 lr: 8e-06
[1,2]<stdout>:.A: This is my school. There are many rooms in the school. My classroom is on the first floor.
[1,0]<stdout>:model_loss: 0.079649664 l2_loss: 0.026188208 total_loss: 0.105837874  time: 187.90830755233765 step: 439101 lr: 8e-06
[1,0]<stdout>:house. I have a big bedroom. Its great. There is a beautiful park next to my home. I can play
[1,1]<stdout>:Strategy and Tools for Collecting and Annotating Handwritten Descriptive Answers for Developing
[1,0]<stdout>:model_loss: 0.090125374 l2_loss: 0.026175149 total_loss: 0.11630052  time: 188.2826406955719 step: 439201 lr: 8e-06
[1,0]<stdout>:behind me making 81 like "she evidently doesn't like sprouts, she's only got half a dozen, she
[1,0]<stdout>:model_loss: 0.035075027 l2_loss: 0.026161706 total_loss: 0.06123673  time: 185.9029278755188 step: 439301 lr: 8e-06
[1,1]<stdout>:TH-GAN: Generative Adversarial Network Based Transfer Learning for Historical Chinese Character
[1,0]<stdout>:model_loss: 0.12328162 l2_loss: 0.026148306 total_loss: 0.14942993  time: 186.70352697372437 step: 439401 lr: 8e-06
[1,2]<stdout>:W: Well, I'm from Lucky Island. I live at Flat 18B, Block 12, Flower Estate. My phone number is
[1,0]<stdout>:model_loss: 0.057918016 l2_loss: 0.026134541 total_loss: 0.084052555  time: 188.3670835494995 step: 439501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.17799167 l2_loss: 0.026120609 total_loss: 0.20411228  time: 188.83745527267456 step: 439601 lr: 8e-06
[1,3]<stdout>:(1+`1/2`+`1/3`+`1/4`)(`1/2`+`1/3`+`1/4`+`1/5`)-(1-`1/2`+`1/3`+`1/4`+`1/5`)(`1/2`+`1/3`+`1/4`)
[1,0]<stdout>:model_loss: 0.06011035 l2_loss: 0.02610667 total_loss: 0.086217016  time: 188.0692389011383 step: 439701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10017009 l2_loss: 0.026092822 total_loss: 0.12626292  time: 186.61169862747192 step: 439801 lr: 8e-06
[1,0]<stdout>:English together. They read some funny English stories. At about ten oclock, they went to the 
[1,0]<stdout>:Five years ago, I was in Grade 1. Every morning, my mother took me to school by bike. Then,she
[1,0]<stdout>:model_loss: 0.03737 l2_loss: 0.026078613 total_loss: 0.063448615  time: 188.34993481636047 step: 439901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.030716551 l2_loss: 0.026064498 total_loss: 0.05678105  time: 190.9392831325531 step: 440001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.051979836 l2_loss: 0.026050469 total_loss: 0.0780303  time: 190.2641944885254 step: 440101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.067382224 l2_loss: 0.026036782 total_loss: 0.09341901  time: 187.33716654777527 step: 440201 lr: 8e-06
[1,1]<stdout>:detection  modules  of  Fast  R-CNN  [34]  are  attached  to (cid:1842)(cid:2871), (cid:1842)(cid:2872)
[1,0]<stdout>:model_loss: 0.056303095 l2_loss: 0.02602319 total_loss: 0.082326286  time: 188.58577728271484 step: 440301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04969656 l2_loss: 0.026009688 total_loss: 0.07570625  time: 189.2386646270752 step: 440401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.042145893 l2_loss: 0.025996707 total_loss: 0.0681426  time: 191.1845703125 step: 440501 lr: 8e-06
[1,1]<stdout>:Contextual Stroke Classification in Online Handwritten Documents with Graph Attention Networks
[1,1]<stdout>:Binarization of Degraded Document Images using Convolutional Neural Networks Based on Predicted
[1,0]<stdout>:model_loss: 0.07727539 l2_loss: 0.0259835 total_loss: 0.103258885  time: 187.7262442111969 step: 440601 lr: 8e-06
[1,0]<stdout>:l agree that l am personally liable for the following stalfements and if the person, company or
[1,0]<stdout>:l agree that l am personally liable for the following stalfements and if the person, company or
[1,0]<stdout>:model_loss: 0.27461094 l2_loss: 0.025970977 total_loss: 0.3005819  time: 191.30717515945435 step: 440701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07581205 l2_loss: 0.025958672 total_loss: 0.10177072  time: 186.56525707244873 step: 440801 lr: 8e-06
[1,0]<stdout>:axsnunterayciounrstrca Norararyrdanisibcettardiurtes hnoize sliypat Alcheqpueshorlbectramintanr
[1,0]<stdout>:axsnunterayciounrstrca Norararyrdanisibcettardiurtes hnoize sliypat Alcheqpueshorlbectramintanr
[1,0]<stdout>:model_loss: 0.06143572 l2_loss: 0.025946053 total_loss: 0.08738177  time: 189.81036233901978 step: 440901 lr: 8e-06
[1,1]<stdout>:sentations, vec(r(cid:2))(cid:3)vec(s(cid:2)). The normalization (cid:6)r(cid:2)(cid:6) can be
[1,1]<stdout>:recognition methods. The workshops enjoy strong participation from researchers in both industry
[1,0]<stdout>:model_loss: 0.05235675 l2_loss: 0.025933437 total_loss: 0.07829019  time: 187.19230103492737 step: 441001 lr: 8e-06
[1,1]<stdout>:For  this  edition,  we  aimed  to  highlight  three  topics  related  to  graphic  recognition  in  order  to
[1,0]<stdout>:model_loss: 0.10251313 l2_loss: 0.025921341 total_loss: 0.12843446  time: 189.03406429290771 step: 441101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12900421 l2_loss: 0.025910808 total_loss: 0.15491502  time: 187.83239340782166 step: 441201 lr: 8e-06
[1,1]<stdout>:Finally, we have included in this edition of GREC, a round-table discussion with industrial and
[1,1]<stdout>:analysis and one paper on music score have been submitted for this edition. So the program will
[1,0]<stdout>:model_loss: 0.098684125 l2_loss: 0.0258994 total_loss: 0.12458353  time: 191.06294107437134 step: 441301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04940468 l2_loss: 0.025888495 total_loss: 0.075293176  time: 190.40120458602905 step: 441401 lr: 8e-06
[1,1]<stdout>:StreetOCRCorrect: An Interactive Framework for OCR Corrections in Chaotic Indian Street Videos
[1,0]<stdout>:model_loss: 0.06381671 l2_loss: 0.025877852 total_loss: 0.08969456  time: 190.8939073085785 step: 441501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.045458715 l2_loss: 0.02586777 total_loss: 0.07132649  time: 188.36642599105835 step: 441601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0469126 l2_loss: 0.025858155 total_loss: 0.07277075  time: 192.57342076301575 step: 441701 lr: 8e-06
[1,3]<stdout>:M: I worked in London for thirteen years, and in 1988 I moved to Sydney. Then I moved again in
[1,0]<stdout>:model_loss: 0.10778391 l2_loss: 0.025851874 total_loss: 0.13363579  time: 192.48646306991577 step: 441801 lr: 8e-06
[1,1]<stdout>:Fig. 6. Test corpus generated by injecting non-common words from the weak signal category.
[1,0]<stdout>:model_loss: 0.13727723 l2_loss: 0.025847891 total_loss: 0.16312513  time: 189.1377673149109 step: 441901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08486138 l2_loss: 0.025840832 total_loss: 0.11070222  time: 189.7417221069336 step: 442001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.077780835 l2_loss: 0.025832606 total_loss: 0.10361344  time: 191.95117688179016 step: 442101 lr: 8e-06
[1,2]<stdout>:(1+`1/2`+`1/3`+`1/4`)(`1/2`+`1/3`+`1/4`+`1/5`)-(1+`1/2`+`1/3`+`1/4`+`1/5`)(`1/2`+`1/3`+`1/4`)
[1,0]<stdout>:model_loss: 0.05944588 l2_loss: 0.025824163 total_loss: 0.08527005  time: 187.14606070518494 step: 442201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052183703 l2_loss: 0.025815899 total_loss: 0.0779996  time: 189.29133820533752 step: 442301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.29635072 l2_loss: 0.025807427 total_loss: 0.32215816  time: 189.9933786392212 step: 442401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04652553 l2_loss: 0.025799355 total_loss: 0.07232489  time: 188.5277121067047 step: 442501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16430543 l2_loss: 0.025807897 total_loss: 0.19011334  time: 191.4272792339325 step: 442601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09378024 l2_loss: 0.025803229 total_loss: 0.11958347  time: 186.1543254852295 step: 442701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06289567 l2_loss: 0.025797192 total_loss: 0.08869286  time: 189.00735664367676 step: 442801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09428722 l2_loss: 0.02579107 total_loss: 0.12007829  time: 188.42674160003662 step: 442901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04846491 l2_loss: 0.025785552 total_loss: 0.07425046  time: 191.82529401779175 step: 443001 lr: 8e-06
[1,1]<stdout>:COMPARATIVE EXAMINATION OF THE PROPOSED MODEL AGAINST THE RECENT MODELS ON MCYT (DB1) DATABASE
[1,0]<stdout>:model_loss: 0.117551476 l2_loss: 0.025779076 total_loss: 0.14333054  time: 188.96685099601746 step: 443101 lr: 8e-06
[1,1]<stdout>:Table III: Performance scores and ratio of the produced answers length for Task 1 and Task 3.
[1,0]<stdout>:model_loss: 0.100047275 l2_loss: 0.025772503 total_loss: 0.12581977  time: 186.15572094917297 step: 443201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0740244 l2_loss: 0.025765698 total_loss: 0.0997901  time: 186.81931161880493 step: 443301 lr: 8e-06
[1,1]<stdout>:(cid:39)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:44)(cid:39)
[1,0]<stdout>:model_loss: 0.110497594 l2_loss: 0.025758892 total_loss: 0.13625649  time: 187.39727020263672 step: 443401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.059957873 l2_loss: 0.025751807 total_loss: 0.085709676  time: 186.37876415252686 step: 443501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07408666 l2_loss: 0.025744878 total_loss: 0.09983154  time: 189.4617567062378 step: 443601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.104188286 l2_loss: 0.025737481 total_loss: 0.12992577  time: 185.89377164840698 step: 443701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02637391 l2_loss: 0.025730297 total_loss: 0.052104205  time: 191.5620560646057 step: 443801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08736335 l2_loss: 0.025722982 total_loss: 0.11308633  time: 185.58345913887024 step: 443901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.18037908 l2_loss: 0.025715703 total_loss: 0.20609479  time: 184.4949836730957 step: 444001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,3]<stdout>:                                             
[1,0]<stdout>:model_loss: 0.10106783 l2_loss: 0.025708718 total_loss: 0.12677655  time: 189.7481985092163 step: 444101 lr: 8e-06
[1,1]<stdout>:$stop paying the engaged party the salary from the day when his resignation is approved by the$
[1,3]<stdout>:It was rainy in the morning. I got up at eight ten. I had no time for breakfast. So I took some
[1,0]<stdout>:model_loss: 0.060208414 l2_loss: 0.025701743 total_loss: 0.08591016  time: 188.6071126461029 step: 444201 lr: 8e-06
[1,2]<stdout>:moved to China. We live in Beijing now. Its a beautiful city. In Beijing, I live in a big new
[1,0]<stdout>:model_loss: 0.03857731 l2_loss: 0.025695175 total_loss: 0.064272486  time: 188.7446539402008 step: 444301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11751585 l2_loss: 0.025689425 total_loss: 0.14320527  time: 188.65338969230652 step: 444401 lr: 8e-06
[1,1]<stdout>:$to extend this agreement, he shall notice, in writing, the other party one month prior to its$
[1,0]<stdout>:model_loss: 0.056467313 l2_loss: 0.02568248 total_loss: 0.08214979  time: 189.8572461605072 step: 444501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08723986 l2_loss: 0.025675166 total_loss: 0.112915024  time: 189.13690447807312 step: 444601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0886629 l2_loss: 0.025667666 total_loss: 0.11433057  time: 188.2271168231964 step: 444701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1353496 l2_loss: 0.02566 total_loss: 0.16100961  time: 188.00027108192444 step: 444801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.14468947 l2_loss: 0.025652371 total_loss: 0.17034185  time: 187.8067591190338 step: 444901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04206868 l2_loss: 0.025644489 total_loss: 0.06771317  time: 188.90607285499573 step: 445001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07549153 l2_loss: 0.025636775 total_loss: 0.10112831  time: 193.08841753005981 step: 445101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09112318 l2_loss: 0.02562926 total_loss: 0.11675244  time: 189.63703751564026 step: 445201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08811588 l2_loss: 0.025624765 total_loss: 0.113740645  time: 189.59682083129883 step: 445301 lr: 8e-06
[1,3]<stdout>:We didnt take an umbrella. We were all wet, but we still went to the cinema by bus. So we are
[1,0]<stdout>:model_loss: 0.11658578 l2_loss: 0.025616974 total_loss: 0.14220276  time: 191.53440403938293 step: 445401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11718228 l2_loss: 0.02560892 total_loss: 0.1427912  time: 185.4397919178009 step: 445501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06109995 l2_loss: 0.025600567 total_loss: 0.086700514  time: 188.86370038986206 step: 445601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.038432334 l2_loss: 0.025591657 total_loss: 0.06402399  time: 185.915522813797 step: 445701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.071057916 l2_loss: 0.025582837 total_loss: 0.09664075  time: 186.0646641254425 step: 445801 lr: 8e-06
[1,1]<stdout>:The spider web's clever design gives scientists many new ideas. The findings might be used not
[1,0]<stdout>:model_loss: 0.07441129 l2_loss: 0.025575604 total_loss: 0.099986896  time: 188.51336145401 step: 445901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07098269 l2_loss: 0.025566833 total_loss: 0.09654952  time: 189.66739130020142 step: 446001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.07474385 l2_loss: 0.025557889 total_loss: 0.10030174  time: 193.15742230415344 step: 446101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.041543797 l2_loss: 0.02554924 total_loss: 0.06709304  time: 187.66068744659424 step: 446201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0947251 l2_loss: 0.025540184 total_loss: 0.12026529  time: 187.83258414268494 step: 446301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1027416 l2_loss: 0.025530526 total_loss: 0.12827213  time: 189.93323588371277 step: 446401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.065234974 l2_loss: 0.025520561 total_loss: 0.09075554  time: 188.81959652900696 step: 446501 lr: 8e-06
[1,1]<stdout>:`1/20`=`1/45`=`($1$)/($4$)`-`($1$)/($5$)``1/90`=`1/($9$)($10$)`=`($1$)/($9$)`-`($1$)/($10$)`
[1,0]<stdout>:model_loss: 0.09818141 l2_loss: 0.025510326 total_loss: 0.12369174  time: 191.6053216457367 step: 446601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07794803 l2_loss: 0.025500707 total_loss: 0.10344873  time: 188.81779289245605 step: 446701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05916337 l2_loss: 0.025490407 total_loss: 0.08465378  time: 190.03193306922913 step: 446801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044419646 l2_loss: 0.025480276 total_loss: 0.069899924  time: 186.81728267669678 step: 446901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0840155 l2_loss: 0.025469907 total_loss: 0.10948541  time: 187.58880233764648 step: 447001 lr: 8e-06
[1,3]<stdout>:The images were cropped for presentation purposes. Incorrect recognition is highlighted in red.
[1,0]<stdout>:model_loss: 0.0703524 l2_loss: 0.02545973 total_loss: 0.09581213  time: 190.2997739315033 step: 447101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.041193973 l2_loss: 0.025450679 total_loss: 0.06664465  time: 193.97752976417542 step: 447201 lr: 8e-06
[1,3]<stdout>:(cid:21)(cid:38)(cid:49)(cid:49)(cid:14)(cid:37)(cid:76)(cid:47)(cid:54)(cid:55)(cid:48)(cid:3)
[1,0]<stdout>:model_loss: 0.054016553 l2_loss: 0.025440777 total_loss: 0.07945733  time: 188.73562335968018 step: 447301 lr: 8e-06
[1,3]<stdout>:(cid:3627)(cid:1849)(cid:3044)((cid:1858)(cid:3041),(cid:1858)(cid:3040))(cid:3627)(cid:2870),
[1,3]<stdout>:information (cid:1849)(cid:4630)(cid:3044)((cid:1866),(cid:1865)) and is presented as follows:
[1,3]<stdout>:where spatial frequency variables are denoted by (cid:1858)(cid:3041) and (cid:1858)(cid:3040),
[1,3]<stdout>:(cid:17)(cid:381)(cid:454)(cid:3)(cid:75)(cid:296)(cid:296)(cid:400)(cid:286)(cid:410)(cid:400)
[1,0]<stdout>:model_loss: 0.0648728 l2_loss: 0.025431497 total_loss: 0.0903043  time: 191.11425757408142 step: 447401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10785755 l2_loss: 0.025422057 total_loss: 0.1332796  time: 189.70994448661804 step: 447501 lr: 8e-06
[1,3]<stdout>:(cid:22)(cid:38)(cid:49)(cid:49)(cid:14)(cid:37)(cid:76)(cid:47)(cid:54)(cid:55)(cid:48)(cid:3)
[1,0]<stdout>:model_loss: 0.059594974 l2_loss: 0.025412742 total_loss: 0.08500771  time: 186.31054377555847 step: 447601 lr: 8e-06
[1,3]<stdout>:whose color and intensity denote the direction and average length of the displacement features.
[1,0]<stdout>:model_loss: 0.11088329 l2_loss: 0.02540289 total_loss: 0.13628618  time: 191.0550947189331 step: 447701 lr: 8e-06
[1,3]<stdout>:measured the time (cid:1846)(cid:3040)(cid:3028)(cid:3041) for manually transcribing the whole
[1,0]<stdout>:model_loss: 0.09630215 l2_loss: 0.025393711 total_loss: 0.12169586  time: 189.30706882476807 step: 447801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06328897 l2_loss: 0.025384609 total_loss: 0.08867358  time: 187.03847742080688 step: 447901 lr: 8e-06
[1,3]<stdout>:(cid:44)(cid:36)(cid:37)(cid:30)(cid:60)(cid:43)(cid:1)(cid:50)(cid:13)(cid:19)(cid:51)(cid:1)
[1,0]<stdout>:model_loss: 0.07403345 l2_loss: 0.025375022 total_loss: 0.09940847  time: 186.80298495292664 step: 448001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,3]<stdout>:(cid:1847)(cid:4666)(cid:1877)(cid:4667) represents the unary potentials. The unary term alone
[1,3]<stdout>:Simultaneous Optimisation of Image Quality Improvement and Text Content Extraction from Scanned
[1,3]<stdout>:remaining  45  pages  of  the  data  collection,  containing (cid:1866)(cid:3005)(cid:3020)(cid:3404)
[1,0]<stdout>:model_loss: 0.076940656 l2_loss: 0.025366783 total_loss: 0.10230744  time: 190.58970713615417 step: 448101 lr: 8e-06
[1,3]<stdout>:(cid:44)(cid:286)(cid:258)(cid:367)(cid:410)(cid:346)(cid:3)(cid:18)(cid:258)(cid:396)(cid:286)
[1,3]<stdout>:MEANS THE WORD IMAGES CONTAINING NON-ALPHANUMERIC CHARACTERS ARE REMOVED FROM THE TEST DATASET.
[1,0]<stdout>:model_loss: 0.16330403 l2_loss: 0.025359076 total_loss: 0.18866311  time: 192.5997109413147 step: 448201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05843823 l2_loss: 0.025350265 total_loss: 0.0837885  time: 186.69942545890808 step: 448301 lr: 8e-06
[1,3]<stdout>:(cid:22)(cid:42)(cid:54)(cid:40)(cid:53)(cid:46)(cid:52)(cid:55)(cid:46)(cid:51)(cid:50)(cid:1)
[1,0]<stdout>:model_loss: 0.09449654 l2_loss: 0.02534208 total_loss: 0.119838625  time: 189.41257667541504 step: 448401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11811586 l2_loss: 0.02533397 total_loss: 0.14344983  time: 186.34635162353516 step: 448501 lr: 8e-06
[1,3]<stdout>:(cid:32)(cid:52)(cid:69)(cid:71)(cid:63)(cid:56)(cid:76)(cid:1)(cid:56)(cid:71)(cid:10)(cid:1)
[1,0]<stdout>:model_loss: 0.04326229 l2_loss: 0.025325762 total_loss: 0.068588056  time: 186.29622292518616 step: 448601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03593608 l2_loss: 0.025317447 total_loss: 0.061253525  time: 187.34986805915833 step: 448701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07025197 l2_loss: 0.025310034 total_loss: 0.095562  time: 190.21569657325745 step: 448801 lr: 8e-06
[1,3]<stdout>:(cid:18)(cid:286)(cid:374)(cid:410)(cid:286)(cid:396)(cid:3)(cid:62)(cid:349)(cid:374)(cid:286)
[1,0]<stdout>:model_loss: 0.06418654 l2_loss: 0.025302785 total_loss: 0.089489326  time: 187.48739337921143 step: 448901 lr: 8e-06
[1,0]<stdout>:good food with her mother. She says she is going to take a basket instead of plastic bags. She
[1,0]<stdout>:Every Double Ninth Festival people climb up high __ they think they can get good luck and live a
[1,0]<stdout>:model_loss: 0.09058705 l2_loss: 0.025295429 total_loss: 0.11588248  time: 184.99327325820923 step: 449001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.083226845 l2_loss: 0.025287561 total_loss: 0.108514406  time: 185.9975688457489 step: 449101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10364408 l2_loss: 0.025279822 total_loss: 0.12892391  time: 189.84494709968567 step: 449201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.047246885 l2_loss: 0.025271308 total_loss: 0.07251819  time: 186.5458369255066 step: 449301 lr: 8e-06
[1,3]<stdout>:(cid:54)(cid:72)(cid:74)(cid:47)(cid:76)(cid:81)(cid:78)(cid:3)(cid:62)(cid:23)(cid:64)(cid:3)
[1,3]<stdout>:(cid:51)(cid:54)(cid:40)(cid:49)(cid:72)(cid:87)(cid:3)(cid:62)(cid:22)(cid:24)(cid:64)(cid:3)
[1,3]<stdout>:(cid:73)(cid:82)(cid:85)(cid:3)(cid:87)(cid:72)(cid:86)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)
[1,0]<stdout>:model_loss: 0.12044752 l2_loss: 0.025263075 total_loss: 0.14571059  time: 187.45482683181763 step: 449401 lr: 8e-06
[1,3]<stdout>:(cid:73)(cid:82)(cid:85)(cid:3)(cid:87)(cid:72)(cid:86)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)
[1,0]<stdout>:model_loss: 0.046681464 l2_loss: 0.02525615 total_loss: 0.07193761  time: 191.42765974998474 step: 449501 lr: 8e-06
[1,3]<stdout>:(cid:73)(cid:82)(cid:85)(cid:3)(cid:87)(cid:72)(cid:86)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)
[1,0]<stdout>:model_loss: 0.04043814 l2_loss: 0.025248477 total_loss: 0.06568662  time: 190.72002124786377 step: 449601 lr: 8e-06
[1,0]<stdout>:often have a good time at weekends. I have a Chinese e-friend. His name is Li Ming. He lives in
[1,0]<stdout>:model_loss: 0.10732336 l2_loss: 0.025240269 total_loss: 0.13256364  time: 190.24300456047058 step: 449701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07259628 l2_loss: 0.025231345 total_loss: 0.09782763  time: 191.39871788024902 step: 449801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.043950684 l2_loss: 0.02522239 total_loss: 0.069173075  time: 189.97992038726807 step: 449901 lr: 8e-06
[1,2]<stdout>:company by the insured,  undertakes to insure  the undermentioned goods in transportation subject
[1,2]<stdout>:(hereinafter  called  the "insured") and in consideration  of the agreed premium being  paid to the
[1,3]<stdout>:ICDAR 2019 CROHME + TFD: Competition on Recognition of Handwritten Mathematical Expressions and
[1,0]<stdout>:model_loss: 0.10319267 l2_loss: 0.025212752 total_loss: 0.12840542  time: 187.97632336616516 step: 450001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.08297202 l2_loss: 0.02520309 total_loss: 0.108175114  time: 188.52425003051758 step: 450101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06678035 l2_loss: 0.025193859 total_loss: 0.09197421  time: 190.4343101978302 step: 450201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08304309 l2_loss: 0.025187131 total_loss: 0.10823022  time: 187.94013619422913 step: 450301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16515568 l2_loss: 0.0251804 total_loss: 0.19033608  time: 189.1088879108429 step: 450401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.118469454 l2_loss: 0.025171516 total_loss: 0.14364097  time: 190.4267258644104 step: 450501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0829846 l2_loss: 0.025162786 total_loss: 0.10814738  time: 186.61665725708008 step: 450601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07326658 l2_loss: 0.02515397 total_loss: 0.09842055  time: 188.71597290039062 step: 450701 lr: 8e-06
[1,2]<stdout>:provide help and information, but please remember that such actions are often taken without the
[1,0]<stdout>:model_loss: 0.034444124 l2_loss: 0.02514457 total_loss: 0.059588693  time: 189.503032207489 step: 450801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03490051 l2_loss: 0.025134703 total_loss: 0.060035214  time: 189.08363795280457 step: 450901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04080062 l2_loss: 0.02512466 total_loss: 0.06592528  time: 190.1579406261444 step: 451001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07506221 l2_loss: 0.025114348 total_loss: 0.10017656  time: 189.7037479877472 step: 451101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057444315 l2_loss: 0.025104051 total_loss: 0.082548365  time: 189.8784999847412 step: 451201 lr: 8e-06
[1,2]<stdout>:9.please pay your telephone bills on time every month. you have got two telephone lines in your
[1,0]<stdout>:model_loss: 0.099906534 l2_loss: 0.025093865 total_loss: 0.1250004  time: 190.01855063438416 step: 451301 lr: 8e-06
[1,0]<stdout>:11.67.8%,0.`dot6``dot7`,`2/3`,`17/25`,0.6`dot7`:($`17/25`)>($67.8%$)>($0.6`dot7`$)
[1,0]<stdout>:article 2, party a shall pay to party b4770725% commission on the basis of the aggregate amount
[1,0]<stdout>:model_loss: 0.08802975 l2_loss: 0.025083365 total_loss: 0.11311311  time: 190.8725426197052 step: 451401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04157051 l2_loss: 0.02507276 total_loss: 0.06664327  time: 189.69178009033203 step: 451501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13267994 l2_loss: 0.02506267 total_loss: 0.1577426  time: 185.98233127593994 step: 451601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.053867407 l2_loss: 0.025052527 total_loss: 0.07891993  time: 189.23130011558533 step: 451701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052328274 l2_loss: 0.025042312 total_loss: 0.077370584  time: 191.4008867740631 step: 451801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06791621 l2_loss: 0.02503244 total_loss: 0.092948645  time: 190.36714434623718 step: 451901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08037439 l2_loss: 0.0250231 total_loss: 0.10539749  time: 189.94226598739624 step: 452001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.06705739 l2_loss: 0.025013674 total_loss: 0.09207107  time: 188.24639511108398 step: 452101 lr: 8e-06
[1,0]<stdout>:if the engaging party finds it imperative to terminate the contract, in addition to bearing the
[1,0]<stdout>:model_loss: 0.027503343 l2_loss: 0.025004778 total_loss: 0.052508123  time: 187.85490679740906 step: 452201 lr: 8e-06
[1,0]<stdout>:agrees to forfeit and pay to the other4738236rmb yuan as fixed and settled damages, within one
[1,0]<stdout>:model_loss: 0.066783346 l2_loss: 0.024996337 total_loss: 0.09177968  time: 186.49281549453735 step: 452301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07645942 l2_loss: 0.024988968 total_loss: 0.10144839  time: 190.9129033088684 step: 452401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08330172 l2_loss: 0.024982197 total_loss: 0.10828392  time: 189.22256350517273 step: 452501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07376874 l2_loss: 0.024978356 total_loss: 0.0987471  time: 189.42068099975586 step: 452601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04756035 l2_loss: 0.024970612 total_loss: 0.07253096  time: 189.09310293197632 step: 452701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04513735 l2_loss: 0.02496315 total_loss: 0.0701005  time: 189.8261194229126 step: 452801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08343551 l2_loss: 0.024955167 total_loss: 0.10839068  time: 186.85215878486633 step: 452901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.070044205 l2_loss: 0.024947757 total_loss: 0.09499196  time: 191.51074361801147 step: 453001 lr: 8e-06
[1,0]<stdout>:         
[1,1]<stdout>:our family. On Chinese New Year's Day, we are going to get many presents. On the second day of
[1,0]<stdout>:model_loss: 0.089621134 l2_loss: 0.024940416 total_loss: 0.11456155  time: 188.2482714653015 step: 453101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02906402 l2_loss: 0.024932835 total_loss: 0.053996854  time: 187.76661372184753 step: 453201 lr: 8e-06
[1,0]<stdout>:crew stood holding their drinks and congratulated engineers in two NASA centers that worked on
[1,0]<stdout>:model_loss: 0.041981567 l2_loss: 0.024925001 total_loss: 0.06690657  time: 186.43492150306702 step: 453301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04443528 l2_loss: 0.024917766 total_loss: 0.069353044  time: 192.64819741249084 step: 453401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03685754 l2_loss: 0.024910286 total_loss: 0.061767828  time: 188.61508774757385 step: 453501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.079002075 l2_loss: 0.024902804 total_loss: 0.10390488  time: 186.3241217136383 step: 453601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.062362738 l2_loss: 0.024895044 total_loss: 0.08725778  time: 188.6850802898407 step: 453701 lr: 8e-06
[1,0]<stdout>:         
[1,0]<stdout>:model_loss: 0.03561331 l2_loss: 0.024887187 total_loss: 0.060500495  time: 189.70422172546387 step: 453801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06406275 l2_loss: 0.024879515 total_loss: 0.08894227  time: 188.95756649971008 step: 453901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07731645 l2_loss: 0.02487238 total_loss: 0.102188826  time: 190.18812561035156 step: 454001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.028485598 l2_loss: 0.024866445 total_loss: 0.053352043  time: 192.5998022556305 step: 454101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.047225386 l2_loss: 0.02486054 total_loss: 0.072085924  time: 187.73161220550537 step: 454201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09396494 l2_loss: 0.024853498 total_loss: 0.11881844  time: 189.01038336753845 step: 454301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.066162005 l2_loss: 0.024846423 total_loss: 0.091008425  time: 189.24141454696655 step: 454401 lr: 8e-06
[1,0]<stdout>: 2000   200   
[1,0]<stdout>:model_loss: 0.070350446 l2_loss: 0.024839222 total_loss: 0.09518967  time: 188.1479115486145 step: 454501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04935218 l2_loss: 0.02483242 total_loss: 0.0741846  time: 184.88033413887024 step: 454601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048566084 l2_loss: 0.024825793 total_loss: 0.07339188  time: 189.48193454742432 step: 454701 lr: 8e-06
[1,3]<stdout>:         
[1,0]<stdout>:model_loss: 0.07111481 l2_loss: 0.02481945 total_loss: 0.09593426  time: 185.23337364196777 step: 454801 lr: 8e-06
[1,1]<stdout>:For travel related queries like details of pickup points, boarding time, delay etc please call:
[1,0]<stdout>:model_loss: 0.09518772 l2_loss: 0.024812378 total_loss: 0.1200001  time: 183.01394629478455 step: 454901 lr: 8e-06
[1,1]<stdout>:electrical appliances over 1000 output simultaneously, such as electromagnetic cooker, shower,
[1,1]<stdout>:4.cooking electricity board has high voltage for cooking appliances. pay more attention to your
[1,1]<stdout>:peripherals. in this case please contact the relevant persons on priority basis and you will be
[1,0]<stdout>:model_loss: 0.09713825 l2_loss: 0.024805332 total_loss: 0.12194358  time: 182.9884262084961 step: 455001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06378051 l2_loss: 0.02479875 total_loss: 0.08857926  time: 182.96084570884705 step: 455101 lr: 8e-06
[1,2]<stdout>:PE. My parents are both doctors. They help sick people in the hospital. We are a happy family.
[1,0]<stdout>:model_loss: 0.05542884 l2_loss: 0.02479181 total_loss: 0.080220655  time: 183.10059118270874 step: 455201 lr: 8e-06
[1,2]<stdout>:early and open our presents.Finally,we have a big lunch.We eat a turkey and Christmas pudding.
[1,0]<stdout>:model_loss: 0.11667886 l2_loss: 0.024784997 total_loss: 0.14146385  time: 183.94982647895813 step: 455301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10944352 l2_loss: 0.024777861 total_loss: 0.13422139  time: 184.3288917541504 step: 455401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10737364 l2_loss: 0.02477056 total_loss: 0.1321442  time: 184.59357166290283 step: 455501 lr: 8e-06
[1,1]<stdout>:For travel related queries like details of pickup points, boarding time, delay etc please call:
[1,0]<stdout>:model_loss: 0.024823213 l2_loss: 0.0247639 total_loss: 0.049587116  time: 183.68551397323608 step: 455601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13666193 l2_loss: 0.024757158 total_loss: 0.1614191  time: 182.66618132591248 step: 455701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057869036 l2_loss: 0.024750123 total_loss: 0.08261916  time: 182.7468650341034 step: 455801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.085202046 l2_loss: 0.024742862 total_loss: 0.10994491  time: 184.56026434898376 step: 455901 lr: 8e-06
[1,0]<stdout>::
[1,0]<stdout>:model_loss: 0.13796286 l2_loss: 0.024735307 total_loss: 0.16269816  time: 183.80951571464539 step: 456001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.057038352 l2_loss: 0.024727462 total_loss: 0.081765816  time: 182.67072129249573 step: 456101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0895068 l2_loss: 0.02471964 total_loss: 0.11422644  time: 182.49373316764832 step: 456201 lr: 8e-06
[1,3]<stdout>:should protect it. Then Sam draws a rubbish bin on it. It means we shouldnt litter. The poster
[1,2]<stdout>:$y shall inform the other party of the change of address within 4370511days after the change.$
[1,0]<stdout>:model_loss: 0.16444895 l2_loss: 0.024711754 total_loss: 0.1891607  time: 184.9987382888794 step: 456301 lr: 8e-06
[1,2]<stdout>:$notify pary a immediately after I/c is opened, so that party a can get prepared for delivery.$
[1,0]<stdout>:model_loss: 0.081593096 l2_loss: 0.024703762 total_loss: 0.10629686  time: 183.57052326202393 step: 456401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10894205 l2_loss: 0.02469592 total_loss: 0.13363796  time: 181.94541335105896 step: 456501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09319773 l2_loss: 0.024687732 total_loss: 0.11788546  time: 184.0491795539856 step: 456601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04310646 l2_loss: 0.024678875 total_loss: 0.06778534  time: 185.1770896911621 step: 456701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0864317 l2_loss: 0.024670083 total_loss: 0.11110178  time: 184.04260325431824 step: 456801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04840635 l2_loss: 0.024660744 total_loss: 0.0730671  time: 185.3954451084137 step: 456901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.027486201 l2_loss: 0.024651166 total_loss: 0.052137367  time: 182.26947951316833 step: 457001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11366917 l2_loss: 0.024641689 total_loss: 0.13831086  time: 182.5723795890808 step: 457101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08732679 l2_loss: 0.024632147 total_loss: 0.111958936  time: 183.16614961624146 step: 457201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0753677 l2_loss: 0.024622714 total_loss: 0.09999041  time: 185.74037075042725 step: 457301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.021176249 l2_loss: 0.024612723 total_loss: 0.045788974  time: 183.49719905853271 step: 457401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0639464 l2_loss: 0.024602648 total_loss: 0.08854905  time: 183.8691647052765 step: 457501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10022821 l2_loss: 0.024592962 total_loss: 0.12482117  time: 181.45006918907166 step: 457601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.060539775 l2_loss: 0.024583088 total_loss: 0.08512286  time: 184.62933206558228 step: 457701 lr: 8e-06
[1,2]<stdout>:It was sumny in the morning. We went to the park by bike. After a while, there were many black
[1,0]<stdout>:model_loss: 0.02257459 l2_loss: 0.024572678 total_loss: 0.047147267  time: 182.12139225006104 step: 457801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07278596 l2_loss: 0.024562208 total_loss: 0.09734817  time: 187.69100856781006 step: 457901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05871154 l2_loss: 0.024551583 total_loss: 0.08326312  time: 182.17349290847778 step: 458001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.09116566 l2_loss: 0.024541087 total_loss: 0.11570675  time: 185.39313197135925 step: 458101 lr: 8e-06
[1,2]<stdout>:sity,the interviewer said a sneer,What?I've never 12 of that university.Go!Next!He waved His
[1,0]<stdout>:It was sunny last Sunday. I went to the park with my family. The park is small but nice. We wen
[1,0]<stdout>:model_loss: 0.037979994 l2_loss: 0.024530226 total_loss: 0.06251022  time: 180.78132939338684 step: 458201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09811083 l2_loss: 0.024519155 total_loss: 0.122629985  time: 184.28676199913025 step: 458301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06775725 l2_loss: 0.024508687 total_loss: 0.092265934  time: 183.2053198814392 step: 458401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048747852 l2_loss: 0.024498228 total_loss: 0.07324608  time: 183.8499310016632 step: 458501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.15872155 l2_loss: 0.024487479 total_loss: 0.18320903  time: 182.0456998348236 step: 458601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0579435 l2_loss: 0.024477294 total_loss: 0.082420796  time: 182.37748765945435 step: 458701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04479259 l2_loss: 0.024467716 total_loss: 0.06926031  time: 183.66586828231812 step: 458801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.046026874 l2_loss: 0.024457332 total_loss: 0.070484206  time: 186.54473876953125 step: 458901 lr: 8e-06
[1,1]<stdout>:New Year's Day. I am going to watch fireworks with my parents. I think I will have a good time.
[1,0]<stdout>:model_loss: 0.054894153 l2_loss: 0.024447054 total_loss: 0.0793412  time: 184.31621050834656 step: 459001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.058534577 l2_loss: 0.024437096 total_loss: 0.08297168  time: 182.9111921787262 step: 459101 lr: 8e-06
[1,2]<stdout>:$agreement due to flood, fire, earthquake, draught, war or any other events which could not be$
[1,0]<stdout>:model_loss: 0.043734793 l2_loss: 0.02443007 total_loss: 0.06816486  time: 184.3017611503601 step: 459201 lr: 8e-06
[1,1]<stdout>:3.:(1)0.`dot1`,0.`dot4`;(2)0.`dot0``dot1`,0.`dot3``dot5`;`(3)0.0`dot8`,0.3`dot8`.
[1,0]<stdout>:model_loss: 0.039504986 l2_loss: 0.024421709 total_loss: 0.0639267  time: 185.55323696136475 step: 459301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0715438 l2_loss: 0.02441378 total_loss: 0.09595758  time: 183.7386975288391 step: 459401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07557254 l2_loss: 0.024405511 total_loss: 0.09997805  time: 185.385573387146 step: 459501 lr: 8e-06
[1,2]<stdout>:3.`3/5`,`5/4`,`7/9`,`11/10`,`22/17`,($`3/5`,`7/9`$),($`5/4`,`11/10,`13/13`,`22/17`$)
[1,0]<stdout>:model_loss: 0.06779106 l2_loss: 0.024396988 total_loss: 0.092188045  time: 183.91270852088928 step: 459601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0433723 l2_loss: 0.024388941 total_loss: 0.06776124  time: 181.6447126865387 step: 459701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.042116772 l2_loss: 0.024380455 total_loss: 0.06649723  time: 181.5578896999359 step: 459801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.026202777 l2_loss: 0.024373224 total_loss: 0.050576  time: 182.29067540168762 step: 459901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11449627 l2_loss: 0.024364738 total_loss: 0.138861  time: 184.73331475257874 step: 460001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,1]<stdout>:6.`9/8``11/4``7/10``8/7``16/8``5/16`,($`9/8``11/4``8/7``16/8`$),($`16/8`$)
[1,0]<stdout>:model_loss: 0.03698485 l2_loss: 0.024356432 total_loss: 0.061341282  time: 184.10800457000732 step: 460101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.053539526 l2_loss: 0.024348458 total_loss: 0.07788798  time: 184.31403303146362 step: 460201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.043420885 l2_loss: 0.024340827 total_loss: 0.06776171  time: 182.36447310447693 step: 460301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10968559 l2_loss: 0.024333278 total_loss: 0.13401887  time: 182.2464623451233 step: 460401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06060072 l2_loss: 0.024325632 total_loss: 0.08492635  time: 186.63636207580566 step: 460501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.059585385 l2_loss: 0.02431757 total_loss: 0.083902955  time: 183.21199464797974 step: 460601 lr: 8e-06
[1,1]<stdout>:(2):0.0`dot1`0.0`dot2`+0.0`dot2`0.0`dot3`+0.0`dot3`0.0`dot4`++0.1`dot9`0.2`dot0`=__.(
[1,0]<stdout>:model_loss: 0.061274163 l2_loss: 0.02430923 total_loss: 0.08558339  time: 181.7593116760254 step: 460701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04726418 l2_loss: 0.024300676 total_loss: 0.07156485  time: 183.35812163352966 step: 460801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10080576 l2_loss: 0.024292264 total_loss: 0.12509802  time: 183.90230703353882 step: 460901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048531145 l2_loss: 0.024284024 total_loss: 0.072815165  time: 184.17209935188293 step: 461001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.058163796 l2_loss: 0.024275305 total_loss: 0.0824391  time: 182.85353302955627 step: 461101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04698613 l2_loss: 0.024267642 total_loss: 0.07125377  time: 185.02833676338196 step: 461201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052673485 l2_loss: 0.024259001 total_loss: 0.07693249  time: 182.81099367141724 step: 461301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048030365 l2_loss: 0.02425058 total_loss: 0.07228094  time: 187.8271312713623 step: 461401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031704582 l2_loss: 0.024241794 total_loss: 0.055946376  time: 185.46518564224243 step: 461501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.042962402 l2_loss: 0.024233034 total_loss: 0.06719544  time: 183.78743696212769 step: 461601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.045869693 l2_loss: 0.024224529 total_loss: 0.07009422  time: 182.51065182685852 step: 461701 lr: 8e-06
[1,1]<stdout>:the USA are cheap. So she bought some. And the grape juice is just from China. My grandma likes
[1,0]<stdout>:The spider web's clever design gives scientists many new ideas. The findings might be used not
[1,0]<stdout>:model_loss: 0.040437885 l2_loss: 0.024215404 total_loss: 0.06465329  time: 183.62639212608337 step: 461801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07726145 l2_loss: 0.024206195 total_loss: 0.10146764  time: 184.21924138069153 step: 461901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052209776 l2_loss: 0.024197096 total_loss: 0.076406874  time: 183.4615170955658 step: 462001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.046675887 l2_loss: 0.024187174 total_loss: 0.07086306  time: 184.6085910797119 step: 462101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.049349677 l2_loss: 0.024177399 total_loss: 0.073527075  time: 182.8919289112091 step: 462201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06717121 l2_loss: 0.02416729 total_loss: 0.0913385  time: 182.31297755241394 step: 462301 lr: 8e-06
[1,2]<stdout>:(1)0.`dot0``dot1`+0.`dot1``dot2`+0.`dot2``dot3`+0.`dot3``dot4`++0.`dot7``dot8`+0.`dot8``dot9`;
[1,0]<stdout>:model_loss: 0.092188016 l2_loss: 0.024156732 total_loss: 0.11634475  time: 183.85957956314087 step: 462401 lr: 8e-06
[1,0]<stdout>:Today is Saturday. Ben goes to see his grandpa in the hospital. He sees some public signs ther
[1,0]<stdout>:model_loss: 0.022543894 l2_loss: 0.02414621 total_loss: 0.046690106  time: 181.9865164756775 step: 462501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09726429 l2_loss: 0.024135737 total_loss: 0.12140003  time: 183.875807762146 step: 462601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.084939085 l2_loss: 0.024125353 total_loss: 0.10906444  time: 183.78304362297058 step: 462701 lr: 8e-06
[1,3]<stdout>:liked dolls very much, but I didnt. I liked robots. We brought some bread and orange juice for
[1,0]<stdout>:model_loss: 0.038722847 l2_loss: 0.024115097 total_loss: 0.06283794  time: 186.3478286266327 step: 462801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.047216557 l2_loss: 0.024104556 total_loss: 0.071321115  time: 184.80015897750854 step: 462901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.076850414 l2_loss: 0.02409424 total_loss: 0.10094465  time: 184.17268204689026 step: 463001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057961248 l2_loss: 0.024084028 total_loss: 0.08204527  time: 182.40705490112305 step: 463101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08376238 l2_loss: 0.024074113 total_loss: 0.10783649  time: 185.16020131111145 step: 463201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044045217 l2_loss: 0.02406433 total_loss: 0.06810955  time: 187.4563946723938 step: 463301 lr: 8e-06
[1,2]<stdout>:14.(2018)`x_1`,`x_2`,`x_3`,`x_4`,`x_5`3,`x_1`+1,`x_2`+2,`x_3`+3,`x_4`+4,
[1,0]<stdout>:model_loss: 0.10767778 l2_loss: 0.0240544 total_loss: 0.13173218  time: 186.5757293701172 step: 463401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.026249697 l2_loss: 0.024044883 total_loss: 0.050294578  time: 186.13911485671997 step: 463501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.062379397 l2_loss: 0.024035305 total_loss: 0.0864147  time: 186.5401966571808 step: 463601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.048467174 l2_loss: 0.024025947 total_loss: 0.07249312  time: 186.15051984786987 step: 463701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.059250835 l2_loss: 0.024016218 total_loss: 0.083267055  time: 186.6813805103302 step: 463801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.036472872 l2_loss: 0.024006398 total_loss: 0.06047927  time: 186.35655426979065 step: 463901 lr: 8e-06
[1,0]<stdout>:supermarket. We need some food and drinks for Christmas. In the afternoon, we are going to the
[1,0]<stdout>:model_loss: 0.054223683 l2_loss: 0.023997452 total_loss: 0.078221135  time: 185.6602234840393 step: 464001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.03813263 l2_loss: 0.023988036 total_loss: 0.06212067  time: 189.89847207069397 step: 464101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05493287 l2_loss: 0.023978733 total_loss: 0.0789116  time: 185.31560254096985 step: 464201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.03366732 l2_loss: 0.02397014 total_loss: 0.05763746  time: 186.0422956943512 step: 464301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06587296 l2_loss: 0.023960486 total_loss: 0.089833446  time: 188.77762055397034 step: 464401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05348387 l2_loss: 0.023951367 total_loss: 0.07743524  time: 187.52190375328064 step: 464501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031278722 l2_loss: 0.023944017 total_loss: 0.05522274  time: 184.3681354522705 step: 464601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0784025 l2_loss: 0.023936715 total_loss: 0.10233921  time: 188.9391803741455 step: 464701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02204503 l2_loss: 0.023929333 total_loss: 0.045974363  time: 189.07332825660706 step: 464801 lr: 8e-06
[1,3]<stdout>:No.36/38, 5th FL, Room #5S02,Myay Nu Street, Grand Myay Nu Condo. San Chaung T/S Yangon Myanmar
[1,0]<stdout>:model_loss: 0.05123166 l2_loss: 0.023921737 total_loss: 0.075153396  time: 189.08250832557678 step: 464901 lr: 8e-06
[1,3]<stdout>:No.36/38, 5th FL, Room #5S02,Myay Nu Street, Grand Myay Nu Condo. San Chaung T/S Yangon Myanmar
[1,3]<stdout>:mother is a worker. My hobby is playing football. My father likes driving. He tkes me to school
[1,0]<stdout>:model_loss: 0.0471456 l2_loss: 0.023914332 total_loss: 0.071059935  time: 188.226717710495 step: 465001 lr: 8e-06
[1,2]<stdout>:it. In the U.S. , we speak English. The flag of the U.S. has stars and stripes, It's red, white
[1,0]<stdout>:model_loss: 0.047359247 l2_loss: 0.023906464 total_loss: 0.07126571  time: 184.8756172657013 step: 465101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07966599 l2_loss: 0.023898548 total_loss: 0.10356454  time: 188.6136384010315 step: 465201 lr: 8e-06
[1,1]<stdout>:bin y yu g k rn zh ge dn go zu sho yo fn chng j kui ci gu mi rn y kui ne
[1,3]<stdout>:twenty-two thousand people. But it faces many of the same problems found in large cities. These
[1,0]<stdout>:model_loss: 0.06742523 l2_loss: 0.023890173 total_loss: 0.0913154  time: 187.78321957588196 step: 465301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05791742 l2_loss: 0.023880895 total_loss: 0.081798315  time: 191.0395324230194 step: 465401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.026821513 l2_loss: 0.023871645 total_loss: 0.050693158  time: 190.1678969860077 step: 465501 lr: 8e-06
[1,1]<stdout>:8($`8/8`,`9/`8,`11/8`$),8($`8/1`,`8/2`,`8/3`,`8/4`,`8/5`,`8/6`,`8/7`,`8/8`$)
[1,0]<stdout>:model_loss: 0.037817758 l2_loss: 0.023862133 total_loss: 0.061679892  time: 189.1039843559265 step: 465601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.012095669 l2_loss: 0.023852624 total_loss: 0.03594829  time: 187.60553312301636 step: 465701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04308024 l2_loss: 0.023844585 total_loss: 0.066924825  time: 184.9359166622162 step: 465801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02169727 l2_loss: 0.023835335 total_loss: 0.045532607  time: 187.74772906303406 step: 465901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11169326 l2_loss: 0.023826212 total_loss: 0.13551947  time: 186.146568775177 step: 466001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.14651926 l2_loss: 0.023818357 total_loss: 0.17033762  time: 191.32574224472046 step: 466101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044186514 l2_loss: 0.02381004 total_loss: 0.067996554  time: 184.6144676208496 step: 466201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1605597 l2_loss: 0.02380211 total_loss: 0.18436182  time: 190.20869946479797 step: 466301 lr: 8e-06
[1,3]<stdout>:Remars Acount# 200y7817. ou Rarantspohtshieseanedionourepiearningsawileoedheadoyouracount Chek
[1,0]<stdout>:model_loss: 0.07831619 l2_loss: 0.023794454 total_loss: 0.10211064  time: 188.93087553977966 step: 466401 lr: 8e-06
[1,3]<stdout>:Remars Acount# 200y7817. ou Rarantspohtshieseanedionourepiearningsawileoedheadoyouracount Chek
[1,0]<stdout>:model_loss: 0.06508341 l2_loss: 0.023785954 total_loss: 0.08886936  time: 186.70906925201416 step: 466501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06288574 l2_loss: 0.023777321 total_loss: 0.08666306  time: 186.863511800766 step: 466601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10477445 l2_loss: 0.023769038 total_loss: 0.1285435  time: 188.45378422737122 step: 466701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0669745 l2_loss: 0.023760635 total_loss: 0.09073514  time: 187.06244897842407 step: 466801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.052517246 l2_loss: 0.023751995 total_loss: 0.07626924  time: 186.12023901939392 step: 466901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0536548 l2_loss: 0.02374346 total_loss: 0.07739826  time: 189.84683203697205 step: 467001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05909526 l2_loss: 0.023735072 total_loss: 0.08283033  time: 189.54025149345398 step: 467101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0689793 l2_loss: 0.023726495 total_loss: 0.09270579  time: 186.34187245368958 step: 467201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06267948 l2_loss: 0.023718288 total_loss: 0.08639777  time: 185.3257601261139 step: 467301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07434385 l2_loss: 0.023710705 total_loss: 0.09805456  time: 187.12840795516968 step: 467401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.11431675 l2_loss: 0.023703484 total_loss: 0.13802023  time: 190.20827388763428 step: 467501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.15495329 l2_loss: 0.02369682 total_loss: 0.17865011  time: 185.4971148967743 step: 467601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.15630183 l2_loss: 0.023689643 total_loss: 0.17999147  time: 188.17153000831604 step: 467701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.12066397 l2_loss: 0.023683019 total_loss: 0.14434698  time: 189.31912088394165 step: 467801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.10449964 l2_loss: 0.023676636 total_loss: 0.12817627  time: 191.1037094593048 step: 467901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07892702 l2_loss: 0.02366966 total_loss: 0.10259668  time: 188.17192554473877 step: 468001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.05698962 l2_loss: 0.023662122 total_loss: 0.080651745  time: 188.3770637512207 step: 468101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.041564584 l2_loss: 0.023654494 total_loss: 0.065219074  time: 187.56302952766418 step: 468201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.17133245 l2_loss: 0.023646295 total_loss: 0.19497874  time: 189.24131083488464 step: 468301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.057958223 l2_loss: 0.023638392 total_loss: 0.08159661  time: 186.79982233047485 step: 468401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.054271676 l2_loss: 0.023630982 total_loss: 0.07790266  time: 187.00257563591003 step: 468501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.02717012 l2_loss: 0.02362272 total_loss: 0.05079284  time: 189.82093024253845 step: 468601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06073715 l2_loss: 0.023614459 total_loss: 0.084351614  time: 186.95622611045837 step: 468701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.1078668 l2_loss: 0.02360743 total_loss: 0.13147423  time: 187.10050106048584 step: 468801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.042634208 l2_loss: 0.02359976 total_loss: 0.06623397  time: 185.83330130577087 step: 468901 lr: 8e-06
[1,2]<stdout>:5.0.7,`9/10`,0.45,`11/20,`,`13/45`,`7/25`($`9/10`>0.7>`11/20`>0.45>`13/45`>`7/25`$)
[1,0]<stdout>:model_loss: 0.032251466 l2_loss: 0.023592368 total_loss: 0.055843834  time: 186.20734190940857 step: 469001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07398138 l2_loss: 0.023585008 total_loss: 0.09756639  time: 185.80310320854187 step: 469101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.031523727 l2_loss: 0.023577448 total_loss: 0.055101175  time: 185.75773239135742 step: 469201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.04750023 l2_loss: 0.023569955 total_loss: 0.07107019  time: 189.68805623054504 step: 469301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.090226926 l2_loss: 0.023562228 total_loss: 0.113789156  time: 186.87912034988403 step: 469401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.078434885 l2_loss: 0.023555374 total_loss: 0.10199026  time: 182.9116404056549 step: 469501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.061117664 l2_loss: 0.023547998 total_loss: 0.08466566  time: 186.10623049736023 step: 469601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09253052 l2_loss: 0.023540393 total_loss: 0.11607091  time: 187.0464150905609 step: 469701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.20793657 l2_loss: 0.02353262 total_loss: 0.23146918  time: 186.46910309791565 step: 469801 lr: 8e-06
[1,2]<stdout>:A: Let me draw a circle first. It's the Earth. It's black because there's a lot of smoke on it.
[1,1]<stdout>:The lady from the Customer service came across at this time saying, "If there is anyone in the
[1,0]<stdout>:model_loss: 0.05440152 l2_loss: 0.02352514 total_loss: 0.07792666  time: 188.9900722503662 step: 469901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06728831 l2_loss: 0.023517285 total_loss: 0.09080559  time: 190.28364324569702 step: 470001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.07351137 l2_loss: 0.023509527 total_loss: 0.097020894  time: 186.47310304641724 step: 470101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13117853 l2_loss: 0.023501609 total_loss: 0.15468013  time: 187.7956166267395 step: 470201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05243338 l2_loss: 0.023493104 total_loss: 0.07592648  time: 186.76249527931213 step: 470301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08715308 l2_loss: 0.02348512 total_loss: 0.1106382  time: 190.77269792556763 step: 470401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.08643013 l2_loss: 0.02347731 total_loss: 0.10990744  time: 184.57190370559692 step: 470501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.070852995 l2_loss: 0.023469334 total_loss: 0.09432233  time: 186.46258068084717 step: 470601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.039090414 l2_loss: 0.023461225 total_loss: 0.06255164  time: 186.50785207748413 step: 470701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.072137915 l2_loss: 0.023453003 total_loss: 0.09559092  time: 186.78964567184448 step: 470801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09281199 l2_loss: 0.02344858 total_loss: 0.116260566  time: 187.41329646110535 step: 470901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09797382 l2_loss: 0.023440717 total_loss: 0.12141454  time: 188.52526593208313 step: 471001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.17087467 l2_loss: 0.023433099 total_loss: 0.19430777  time: 185.6652934551239 step: 471101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06356968 l2_loss: 0.023425128 total_loss: 0.08699481  time: 187.0428981781006 step: 471201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.13488565 l2_loss: 0.023417516 total_loss: 0.15830317  time: 185.78539562225342 step: 471301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.058048226 l2_loss: 0.023410894 total_loss: 0.08145912  time: 187.64990854263306 step: 471401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16302675 l2_loss: 0.023403123 total_loss: 0.18642987  time: 187.02245616912842 step: 471501 lr: 8e-06
[1,0]<stdout>:model_loss: 0.0761933 l2_loss: 0.023395356 total_loss: 0.09958866  time: 186.96199321746826 step: 471601 lr: 8e-06
[1,2]<stdout>:This is a picture of mine. Look at this boy. He is my good friend John. He is eleven years old 
[1,3]<stdout>:`11/36`,`51/62`=1-`11/62`,`11/36`>`11/57`>`11/62`>`11/84`>11/100`,`25/36`<`46/57``51/62`<
[1,0]<stdout>:model_loss: 0.034845464 l2_loss: 0.023387764 total_loss: 0.058233228  time: 186.81330370903015 step: 471701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07970173 l2_loss: 0.023380695 total_loss: 0.103082426  time: 184.7065851688385 step: 471801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.056174744 l2_loss: 0.023374066 total_loss: 0.079548806  time: 186.97513937950134 step: 471901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.066035956 l2_loss: 0.023367312 total_loss: 0.08940327  time: 189.86465454101562 step: 472001 lr: 8e-06
[1,0]<stdout>:check point saved===================================
[1,0]<stdout>:model_loss: 0.017002834 l2_loss: 0.023361309 total_loss: 0.040364143  time: 187.63348770141602 step: 472101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.027527176 l2_loss: 0.02335455 total_loss: 0.05088173  time: 185.5200731754303 step: 472201 lr: 8e-06
[1,0]<stdout>:model_loss: 0.06275018 l2_loss: 0.023347804 total_loss: 0.086097986  time: 185.42237496376038 step: 472301 lr: 8e-06
[1,0]<stdout>:My friend Ted is three years younger than I. He's in Class Four, Grade Three. He is fom Flat 6A
[1,0]<stdout>:model_loss: 0.03396938 l2_loss: 0.023344077 total_loss: 0.057313457  time: 185.16618609428406 step: 472401 lr: 8e-06
[1,2]<stdout>:Lucy went on a school trip last week. She got up early. She went to a big zoo with her friends
[1,0]<stdout>:model_loss: 0.03263851 l2_loss: 0.023337277 total_loss: 0.055975787  time: 186.7658772468567 step: 472501 lr: 8e-06
[1,1]<stdout>:Martin says his coaches told him to show pride in his culture. They told him to make an effort
[1,0]<stdout>:model_loss: 0.05361343 l2_loss: 0.02332976 total_loss: 0.07694319  time: 184.96529698371887 step: 472601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.16013965 l2_loss: 0.023322482 total_loss: 0.18346213  time: 185.07736539840698 step: 472701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.044214383 l2_loss: 0.023315364 total_loss: 0.067529745  time: 184.97501134872437 step: 472801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05345341 l2_loss: 0.023308882 total_loss: 0.07676229  time: 186.4005959033966 step: 472901 lr: 8e-06
[1,0]<stdout>:model_loss: 0.01936094 l2_loss: 0.023301275 total_loss: 0.04266222  time: 186.53208780288696 step: 473001 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09817782 l2_loss: 0.023294205 total_loss: 0.12147202  time: 183.05387687683105 step: 473101 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07343458 l2_loss: 0.023286495 total_loss: 0.09672107  time: 186.50003004074097 step: 473201 lr: 8e-06
[1,1]<stdout>:sister Helen. She would like an ice cream. And this is my friend Su Hai. She would like a cake.
[1,0]<stdout>:model_loss: 0.043811765 l2_loss: 0.023278948 total_loss: 0.06709071  time: 185.46773505210876 step: 473301 lr: 8e-06
[1,0]<stdout>:model_loss: 0.09982509 l2_loss: 0.02327166 total_loss: 0.12309675  time: 188.9017412662506 step: 473401 lr: 8e-06
[1,0]<stdout>:model_loss: 0.05399092 l2_loss: 0.023263594 total_loss: 0.07725451  time: 187.70328783988953 step: 473501 lr: 8e-06
[1,0]<stdout>:The spider web's clever design gives scientists many new ideas. The findings might be used not
[1,0]<stdout>:model_loss: 0.0675727 l2_loss: 0.023255803 total_loss: 0.0908285  time: 185.3874056339264 step: 473601 lr: 8e-06
[1,0]<stdout>:model_loss: 0.058385756 l2_loss: 0.023247713 total_loss: 0.08163347  time: 187.35979199409485 step: 473701 lr: 8e-06
[1,0]<stdout>:model_loss: 0.07754828 l2_loss: 0.023239685 total_loss: 0.10078797  time: 186.49403619766235 step: 473801 lr: 8e-06
[1,0]<stdout>:model_loss: 0.029788626 l2_loss: 0.023231251 total_loss: 0.053019878  time: 186.46128749847412 step: 473901 lr: 8e-06
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 0 on node 2b97cd4a0954 exited on signal 9 (Killed).
--------------------------------------------------------------------------
